---
title: Unity Custom SRP 基础（六）
date: 2025-03-13 20:44:31
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2025/03/13/LozNxAmnlEJSOyV.gif
mathjax: true
description: 本笔记的主要内容包含支持 HDR 渲染以及 HDR Bloom (约束闪烁问题和 Scattering Bloom)；Tone Mapping 算法的介绍；Color Grading 相关工具和 LUT Baking；Multiple Camera 和 Rendering Layers 的简单说明。
---

> 本笔记是关于 Unity 的**自定义可编程渲染管线**的入门基础，即 **SRP (Scriptable Rendering Pipeline)**，主要参考了著名的教程 https://catlikecoding.com/ 的 Custom SRP Tutorial，以及知乎上各位图形学大神们的文章。  
>    
> 笔者使用的 Unity 版本是 6000.0.27f1，Core RP Library 的版本是 17.0.3。

# HDR
到目前为止，我们输出到相机的帧缓冲仍然是 LDR 格式，即 B8G8R8A8_SRGB 格式。这意味着输出的颜色都会被限制在 0.0 到 1.0 之间，任何大于 1.0 的颜色值都会被 clamp 到 1.0，相当于在片元着色器函数的结尾做了一次 `saturate()`。但是在 Shader 里计算时，部分像素的颜色值很可能会高于 1.0，特别是在多光源的情况下，这就会造成部分区域非常白，导致场景细节的丢失。

为了解决这一问题，我们就需要使用 **HDR (High Dynamic Range，高动态范围)** 的帧缓冲了，一般 HDR 会使用 R16G16B16A16_FLOAT 的格式（最大值为 65504），也有为了减少带宽的开销，而压缩成 R11G11B10_FLOAT 格式的。这样子大于 1.0 的颜色值就可以存储在纹理当中了，从而获取更大范围的黑暗或明亮值。但是大部分显示器仍然只能显示 sRGB 色彩空间的颜色（这里暂时不讨论支持 HDR 输出的显示器），即 [0, 255] 或 [0.0, 1.0] 范围的值，此时就需要将 HDR 重新映射回 LDR 值，而这个操作就叫做**色调映射 Tone Mapping**，该操作的主要目的就是尽可能地保留场景的黑暗与明亮细节，所以它本质上就是增强明暗的对比度。

> 注意区分 Tone Mapping 和 **Gamma 校正**，这两个操作是相互独立的，Tonemapping 将 HDR 颜色转换到 LDR 颜色，但两者仍然都在线性空间，之后仍然需要做 Gamma 校正。  

## 支持 HDR rendering
我们可以在 RenderPipelineAsset 里面增加一个 bool 字段，用于控制 HDR 的开启。然后在创建 FrameBuffer 的地方根据该字段生成 HDR 格式：  

``` C#
data.buffer.GetTemporaryRT(RenderTargetIDs.k_FrameBufferId, data.camera.pixelWidth, data.camera.pixelHeight, 32, FilterMode.Bilinear, asset.enableHDR ? RenderTextureFormat.DefaultHDR : RenderTextureFormat.Default);
```

default HDR 格式就是 R16G16B16A16_FLOAT。这样子，我们的场景就会使用 HDR 渲染，打开 Frame Debugger，可以看到在最后输出之前渲染出来的图片颜色变深了，这就是因为渲染到了 HDR 的线性空间当中，所以看起来变暗了。但是我们不用关心最后输出时的 gamma 校正，因为在线性空间的设置下，Unity 会自动处理 gamma 校正问题。

## HDR Bloom
之前 LDR Bloom 的预过滤和采样过程中，都是将中间结果输出到 LDR 贴图当中的。那么我们首先应该将它们改为 HDR 贴图：  

``` C#
public override void Render(YRenderPipelineAsset asset, ref PipelinePerFrameData data)
{
    // do bloom at half or quarter resolution
    ...
    // Determine the iteration count
    ...
    // Shader property and keyword setup
    ...

    // HDR
    RenderTextureFormat format = asset.enableHDRFrameBufferFormat ? RenderTextureFormat.DefaultHDR : RenderTextureFormat.Default;

    // Prefilter
    data.buffer.GetTemporaryRT(k_BloomPrefilterId, width, height, 0, FilterMode.Bilinear, format);
    ...

    // Downsample - gaussian pyramid
    int sourceId = k_BloomPrefilterId;
    for (int i = 0; i < iterationCount; i++)
    {
        data.buffer.GetTemporaryRT(m_BloomPyramidUpIds[i], width, height, 0, FilterMode.Bilinear, format);
        data.buffer.GetTemporaryRT(m_BloomPyramidDownIds[i], width, height, 0, FilterMode.Bilinear, format);
        ...
    }

    // Upsample - bilinear or bicubic
    ...
    // Final Blit
    ...
    // Release RT
    ...
}
```

这样子 Addictive Bloom 就可以在 HDR 下正常工作了。HDR Bloom 和 LDR Bloom 的重要区别在于，HDR Bloom 中一个特别大的值会影响到一个非常大的区域，即使是一个像素，也可能会产生非常高亮的 Bloom 区域。产生这个现象的原因就是平均值的极端值效应，本来 4 个像素平均，LDR 下无论怎么平均都是小于 1 的平均数，现在 HDR 下，4 个像素中一个像素值为 10，其他像素值再小，平均值也会被拉得很大。于是乎，就产生了 HDR Bloom 的一个最大弊端，也是必须要克服的弊端，即闪烁现象。

### Bloom 闪烁问题
HDR Bloom 的闪烁问题在镜头移动的情况下会格外严重，我这里 gif 就不放出来了，URP 下该现象就挺严重的（看了下 URP 的 Bloom Shader，确实没解决闪烁问题）。而解决方案是在 Prefilter 阶段做一次模糊处理，并使用以下权重做平均来降低动态范围，这个方法叫做 **Karis Average**：  

$$ weight = \cfrac {1} {1 + luminance} $$

在该权重下，亮度越高，权重越低。这个方法虽然损失一部分的 bloom 范围和亮度，但是能在一定程度减少 Bloom 闪烁问题，虽然做不到完全解决，完全解决可以说是不可能的。该方法的实现如下：①将 Prefilter 的 Bilinear 2 × 2 采样扩展为 6 × 6 的 box 核，在采样时（模糊平均前）应用 bloom threshold 提取较亮区域。②使用 Karis Average 在模糊时做加权平均，替代 box 核的算术平均，对颜色进行修正。

    #include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl"

    float4 BloomPrefilterFrag(Varyings IN) : SV_TARGET
    {
        float3 color = float3(0.0, 0.0, 0.0);
        float2 offsets[9] = { float2(0.0, 0.0), float2(-1.0, -1.0), float2(-1.0, 1.0), float2(1.0, -1.0), float2(1.0, 1.0), float2(-1.0, 0.0), float2(1.0, 0.0), float2(0.0, -1.0), float2(0.0, 1.0)};

        UNITY_UNROLL
        for (int i = 0; i < 9; i++)
        {
            float2 offset = offsets[i] * _BlitTexture_TexelSize.xy * 2.0;
            float3 c = ApplyBloomThreshold(SAMPLE_TEXTURE2D_LOD(_BlitTexture, sampler_LinearClamp, IN.uv + offset, 0).rgb);
            float w = 1.0 / (Luminance(c) + 1.0);
            color += c * w;
            weight += w;
        }
        color /= weight;

        color = max(color, 0.0);
        return float4(color, 1.0);
    }

亮度的计算之前《Unity Shader入门精要》读书笔记（四）中也讲过，Core RP Library 的 Color.hlsl 也提供了相关的函数。上面之所以在 offset 中乘以 2.0 是因为我们使用的是 bilinear 采样器，采样一次可以覆盖 2 × 2 个像素，这样我们使用一个 3 × 3 的像素之间距离为 2 的 box 核，就可以覆盖到 6 × 6 个像素。最后一定要将颜色 clamp 到 0 以上，否则画面有可能会出现黑块。

这样子就可以解决大部分的闪烁问题了。但上面代码还可以再优化一步，因为下采样阶段使用了高斯模糊，我们可以将上述代码再减少 4 次采样，即与中心点相邻的 4 个采样点：  

    float4 BloomPrefilterFrag(Varyings IN) : SV_TARGET
    {
        ...
        float2 offsets[5] = { float2(0.0, 0.0), float2(-1.0, -1.0), float2(-1.0, 1.0), float2(1.0, -1.0), float2(1.0, 1.0)};

        for (int i = 0; i < 5; i++)
        {
            ...
        }
        ...
    }

这将会使 Prefilter 阶段产生的结果中一个较亮像素变为 x 形状，可以点开 Frame Debugger 查看。但是经过一次下采样，这种图案样式就会消失。


# Scattering Bloom
Scattering Bloom 和 Additive Bloom 在实现方法上是非常类似的，但是对于各自所想要呈现的效果却有所不同，这两种方法的出发点是不同的，故它们之间没有优劣之分。**Addictive Bloom** 更偏向艺术化的表达，相对 Scattering Bloom 更不遵守能量守恒，因为我们在上采样阶段不断叠加会导致 bloom 区域亮度超过了原本图片中的亮度。而 **Scattering Bloom** 考虑了能量守恒，它只模糊但不添加额外的亮度，这使得特别是在灯光或者发光物体使用物理光照单位时，能够更正确地表现出其原本拥有的亮度值。

Scattering Bloom 的实现步骤也比较简单，就是在上采样阶段使用 Scatter 属性在 high-resolution 和 low-resolution 的贴图之间做 lerp，而不是直接相加。这样我们上采样阶段最后得到的贴图的亮度值就不会发生变化，最后在叠加到 frame buffer 时（final blit 阶段）先将 Prefilter 阶段提取的较亮区域抠出来，再将上采样阶段得到的贴图贴上去。

## Scatter
我们先在 Bloom 的 VolumeComponent 里增加一个 enum 用于切换 Addictive Bloom 和 Scattering Bloom，同时添加一个新的属性 scatter，其范围在 0 - 1：  

``` C#
public enum BloomMode
{
    Additive,
    Scattering
}

[System.Serializable]
public sealed class BloomDownscaleParameter : VolumeParameter<BloomDownscaleMode>
{
    public BloomDownscaleParameter(BloomDownscaleMode value, bool overrideState = false) : base(value, overrideState) { }
}

[System.Serializable, VolumeComponentMenu("YPipeline Post Processing/Bloom")]
[SupportedOnRenderPipeline(typeof(YRenderPipelineAsset))]
public class Bloom : VolumeComponent, IPostProcessComponent
{
    public BloomModeParameter mode = new BloomModeParameter(BloomMode.Scattering, true);
    ...
    public ClampedFloatParameter scatter = new ClampedFloatParameter(0.5f, 0.0f, 1.0f);
    ...
}
```

我们要为上采样阶段添加一个新的 Pass，我命名为了 BloomScatteringUpsampleFrag，我们要根据选择的模式选择对应的 Pass，同时 scatter 也通过之前说的 `_BloomIntensity` 传递：  

``` C#
public override void Render(YRenderPipelineAsset asset, ref PipelinePerFrameData data)
{
    // do bloom at half or quarter resolution
    ...
    // Determine the iteration count
    ...
    // Shader property and keyword setup
    float bloomIntensity = settings.mode.value == BloomMode.Additive ? 1.0f : settings.scatter.value;
    data.buffer.SetGlobalFloat(k_BloomIntensityId, bloomIntensity);
    ...
    // HDR
    ...
    // Prefilter
    ...
    // Downsample - gaussian pyramid
    ...
    // Upsample - bilinear or bicubic
    int upsamplePass = settings.mode.value == BloomMode.Additive ? 3 : 4;
    ...
    for (int i = iterationCount - 2; i >= 0; i--)
    {
        ...
        BlitUtility.BlitTexture(data.buffer, m_BloomPyramidDownIds[i], m_BloomPyramidUpIds[i], BloomMaterial, upsamplePass);
        ...
    }
    // Final Blit
    ...
    // Release RT
    ...
}
```

BloomScatteringUpsampleFrag 也只需要对原本的上采样函数做一个更改即可，即将返回值的叠加改为 lerp：  

    float4 BloomScatteringUpsampleFrag(Varyings IN) : SV_TARGET
    {
        #if _BLOOM_BICUBIC_UPSAMPLING
        float3 lowerTex = SampleTexture2DBicubic(_BloomLowerTexture, sampler_LinearClamp, IN.uv, _BloomLowerTexture_TexelSize.zwxy, (1.0).xx, 0.0).rgb;
        #else
        float3 lowerTex = SAMPLE_TEXTURE2D_LOD(_BloomLowerTexture, sampler_LinearClamp, IN.uv, 0).rgb;
        #endif
        
        float3 higherTex = SAMPLE_TEXTURE2D_LOD(_BlitTexture, sampler_LinearClamp, IN.uv, 0).rgb;
        return float4(lerp(higherTex, lowerTex, _BloomIntensity), 1.0);
    }

可以看出 scatter 为 0 就意味着只使用金字塔最高层级的贴图，为 1 就只使用金字塔最低层级的贴图。为了确保中间层级都被使用，可以选择限制 scatter 的范围为 0.05 – 0.95。

## Final Blit
上面的步骤本质就是将较亮区域进行模糊，并在叠加的同时不额外增加亮度。为了保证能量守恒，我们叠加到原图时，也需要先将较亮区域从原图抠出来，再叠加上模糊后的较亮区域，从而呈现一种 Bloom 效果。创建一个新的 Pass，如下：  

    float4 BloomScatteringFinalBlitFrag(Varyings IN) : SV_TARGET
    {
        #if _BLOOM_BICUBIC_UPSAMPLING
        float3 lowerTex = SampleTexture2DBicubic(_BloomLowerTexture, sampler_LinearClamp, IN.uv, _BloomLowerTexture_TexelSize.zwxy, (1.0).xx, 0.0).rgb;
        #else
        float3 lowerTex = SAMPLE_TEXTURE2D_LOD(_BloomLowerTexture, sampler_LinearClamp, IN.uv, 0).rgb;
        #endif
        
        float3 higherTex = SAMPLE_TEXTURE2D_LOD(_BlitTexture, sampler_LinearClamp, IN.uv, 0).rgb;

        lowerTex += higherTex - ApplyBloomThreshold(higherTex);
        return float4(lerp(higherTex, lowerTex, _BloomIntensity), 1.0);
    }

我们在原图片和有 bloom 效果的新图片使用 _BloomIntensity 做插值方便我们控制整体的 Bloom 强度。传递参数的工作如下，最好将 Scattering Bloom 和 Addictive Bloom 的 Intensity 属性分开，因为 Scattering Bloom 的 Intensity 的范围是 0 - 1 之间，而 Addictive Bloom 可以无限大：  

``` C#
public override void Render(YRenderPipelineAsset asset, ref PipelinePerFrameData data)
{
    ...
    // Final Blit
    bloomParams = settings.mode.value == BloomMode.Additive ? new Vector4(settings.intensity.value, 0.0f) : new Vector4(settings.finalIntensity.value, 0.0f);
    int finalPass = settings.mode.value == BloomMode.Additive ? 3 : 5;
    data.buffer.SetGlobalVector(k_BloomParamsId, bloomParams);
    data.buffer.SetGlobalTexture(k_BloomLowerTextureID, new RenderTargetIdentifier(lastDst));
    BlitUtility.BlitTexture(data.buffer, RenderTargetIDs.k_FrameBufferId, BuiltinRenderTextureType.CameraTarget, BloomMaterial, finalPass);
    ...
}
```

这样子就实现了 Scattering Bloom，和 Addictive Bloom 的效果对比如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/19/FtsGQbrKnMxoh5q.png" width = "90%" height = "90%" alt="图94 - Scattering Bloom 和 Addictive Bloom 的效果对比"/>
</div>

从上图可以看出，Scattering Bloom 的效果相对比较不易察觉的，它是一个相对更加真实的效果，没有改变场景的整体亮度，并且只有在亮度很大的时候才会展现得比较明显，它的这种特性只能在亲自体验后才能感觉到，只看上图可能感受不是很明显。Addictive Bloom 则改变了场景中的亮度，所以它看起来比 Scattering Bloom 的效果更明显，也更亮。


# Tone Mapping
Tone Mapping 经过了一个很长时间的发展，在这个过程中出现了很多的 tone mapping 算法。常见的有 **Reinhard Tone Mapping**、**Uncharted 2 Filmic Tone Mapping**、**Khronos PBR Neutral Tone Mapping**、**ACES Tone Mapping**、**AgX Tone Mapping**。

在实现上述 Tone Mapping 算法之前，先大致说一下 Tone Mapping 相关类在 CPU 端的准备工作。因为我们之前 Bloom 后处理完是直接绘制到 CameraTarget 的，我们要对 Bloom 完的结果进行 Tone Mapping，所以我们要先将 Bloom 的结果输出到一个纹理中，我命名为了 _BloomTexture。创建 RT，释放 RT 以及 BloomRenderer 中绘制代码的更改我这里就不做记录了，也比较简单。整个后处理管线的顺序问题，下面讲到 Color Grading 时再讨论。

Tone Mapping 的 VolumeComponent 和 PostProcessingRenderer 类，参考前面 Bloom 的写法就行，也没什么好说的，在 VolumeComponent 里创建对应的 Tone Mapping Mode，再根据选择的 mode 对应的 Shader Pass 去绘制 _BloomTexture 即可，最后输出到 CameraTarget。下面直接讲 Shader 里的处理，以及对应的效果了。为了相对更好地展示不同 Tone Mapping 算法的效果以及方便对比，一个基本的渲染图如下（这张参考图的场景颜色比较少，可能不是很能展现不同 Tone Mapping 之间的不同），场景中有一个范围较大的点光源，和台灯位置的聚光灯，聚光灯亮度较高，可以看到台灯照亮位置已经有点过曝了，并且使用了 Scattering Bloom：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/21/eXAwZqBa1fY5GxO.jpg" width = "80%" height = "80%" alt="图95 - 未做 Tone Mapping 前的基准图"/>
</div>

## Reinhard
Reinhard Tone Mapping 来自于这篇论文：https://www-old.cs.utah.edu/docs/techreports/2002/pdf/UUCS-02-001.pdf 。从该篇论文中，可以提炼出 3 种应用：  

**①**第一个是最基础的 Reinhard tone mapping，可以称为 "Simple" Reinhard：  

$$ L_d = \cfrac {L} {1 + L} $$

    float3 Reinhard_Simple(float3 color)
    {
        return color / (color + 1.0);
    }

在 Unity 中实现后的基准参考图效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/21/fpNIHz4c6UOKJ1j.jpg" width = "80%" height = "80%" alt="图96 - Reinhard Simple"/>
</div>

可以感受到效果像是蒙了一层灰色。

**②**第二个是 "Simple" Reinhard 的扩展，可以称为 "Extended" Reinhard：  

$$ L_d = \cfrac {L(1 + \cfrac {L} {L_{white}^2})} {1 + L}$$

    float3 Reinhard_Extended(float3 color, float minWhite)
    {
        float minWhite2 = minWhite * minWhite;
        float3 numerator = color * (1.0 + color / minWhite2);
        return numerator / (1.0 + color);
    }

上述公式中的 $\,L_{white}\,$ 是用户控制的变量，代表着在场景中纯白色的最低亮度值。当 $\,L_{white}\,$ 趋向于无穷大时，"Extended" Reinhard 就会跟 "Simple" Reinhard 一样。$\,L_{white}\,$ 设置为 2 时的基准参考图效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/21/sU2So8fgZdOL1Bb.jpg" width = "80%" height = "80%" alt="图97 - Reinhard Extended（minWhite = 2）"/>
</div>

效果比 Reinhard Simple 要鲜艳一点，但整体仍然灰蒙蒙的。

**③**从上面公式可以看出，理论上我们不应该直接用颜色进行计算，而应该用亮度值，因为 L 代表着 luminance。但是因为我们无法将 RGB 转化为 luminance 后，再从 luminance 转换回 RGB。所以下面代码中，使用 Tone Mapping 前的 luminance 和后的 luminance 对最后的输出颜色做了一次缩放：  

    float3 Reinhard_ExtendedLuminance(float3 color, float minWhite)
    {
        float l_in = Luminance(color);
        float minWhite2 = minWhite * minWhite;
        float numerator = l_in * (1.0 + l_in / minWhite2);
        float l_out = numerator / (1.0 + l_in);
        return color * l_out / l_in;
    }

$\,L_{white}\,$ 还是设置为 2，效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/21/dv6UhEnNG5uM7I4.jpg" width = "80%" height = "80%" alt="图98 - Reinhard Luminance"/>
</div>

仍然没能解决灰蒙蒙的问题。

## Uncharted 2 Filmic
Uncharted 2 Filmic Tone Mapping 最早来自于 GDC 2010 的一个演讲 Uncharted 2: HDR Lighting，演讲者为顽皮狗 Naughty Dog 公司的 John Hable。后来 John Hable 在自己的博客中做了总结和修改：http://filmicworlds.com/blog/filmic-tonemapping-operators/ 。之后他又提出了对 Filmic 曲线进行更多控制的手段：http://filmicworlds.com/blog/filmic-tonemapping-with-piecewise-power-curves/ 。本篇文章只涉及他第一篇文章内的 Uncharted 2 Filmic Tone Mapping，不涉及对 Filmic Tonemapping curves 进行自定义。代码如下：  

    float3 Uncharted2(float3 x)
    {
        const float A = 0.15;
        const float B = 0.50;
        const float C = 0.10;
        const float D = 0.20;
        const float E = 0.02;
        const float F = 0.30;
        return ((x * (A * x + C * B) + D * E) / (x * (A * x + B) + D * F)) - E / F;
    }

    float3 Uncharted2Filmic(float3 color, float exposureBias)
    {
        float3 curr = Uncharted2(color * exposureBias);
        const float W = 11.2;
        float3 whiteScale = 1.0 / Uncharted2(W);
        return whiteScale * curr;
    }

exposureBias 在博客文章中，默认值为 2，下面实现后的基准参考图的设置也是 2：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/21/lFcvCnE7x3HZz9g.jpg" width = "80%" height = "80%" alt="图99 - Uncharted 2 Filmic"/>
</div>

## Khronos PBR Neutral
Khronos PBR Neutral Tone Mapping 是由 Khronos Group 组织提出的，该组织由很多国际知名多媒体行业领导者创立，致力于发展开放标准的应用程序接口 API。Vulkan API 就是由该组织研发并发布的。该 Tone Mapping 的研发者还写了一篇文章，说明了该算法所要解决的问题：https://modelviewer.dev/examples/tone-mapping 。顾名思义，该算法想要更好地体现 PBR 所展现出来的最真实最还原的状态，保留其色调和饱和度，代码如下：  

    float3 KhronosPBRNeutral(float3 color)
    {
        const float startCompression = 0.76;
        const float desaturation = 0.15;

        float x = min(color.r, min(color.g, color.b));
        float offset = x < 0.08 ? x - 6.25 * x * x : 0.04;
        color -= offset;

        float peak = max(color.r, max(color.g, color.b));
        if (peak < startCompression) return color;

        const float d = 0.24;
        float newPeak = 1.0 - d * d / (peak + d - startCompression);
        color *= newPeak / peak;

        float g = 1.0 - 1.0 / (desaturation * (peak - newPeak) + 1.);
        return lerp(color, newPeak * float3(1.0, 1.0, 1.0), g);
    }

实现后，基准参考图效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/21/qSl2zf1cO7V8xvG.jpg" width = "80%" height = "80%" alt="图100 - Khronos PBR Neutral"/>
</div>

## ACES
**Academy Color Encoding System（ACES）** 是由美国电影艺术与科学学院（AMPAS）和行业合作伙伴开发的开放式色彩管理和互换系统，它规范了所有不同类型的项目中的色彩科学，其创立的目的是为行业提供标准化的色彩管理系统。ACES 是开源的，具体详见 https://github.com/ampas/aces 。整套色彩转换工作流程极其复杂，下面简单梳理一下（可能描述得不是很准确）：  

首先 ACES 流程有如下几个组成部分：  
**①Input Device Transform (IDT)**：这一步骤也称为 Input Transform，主要目的是将输入设备颜色数据转换至 ACES 色彩空间。ACES 色彩空间又包括：  
&emsp;&emsp; - **ACES2065-1**：这是一个非常广域的线性色彩空间，比人眼还广，主要是为了存储归档以及部门之间传递素材使用；  
&emsp;&emsp; - **ACEScc**、**ACEScct**：log 色彩空间，主要用于色彩校正或调色；  
&emsp;&emsp; - **ACEScg**：线性色彩空间，主要用于 CG 或特效制作。  

下面三个组成部分又合称为 ACES viewing pipeline，其中：  
**②Look Modification Transform (LMT)**：这一步骤不是必需的，主要是给艺术家更方便地调色，LMT 始终在 ACES 色彩空间中工作；  

③ 和 ④ 又合称为 Output Transform，要将 ACES 色彩空间转换为非 ACES 色彩空间都需要有 RRT 的参与：  
**③Reference Rendering Transform (RRT)**：这步就是核心的一步，将 IDT 转换获得的标准的、高精度的、高动态范围的场景参考线性图像数据，使用一个 S 形曲线，映射到输出颜色编码空间以适用于参考显示设备的观看。  
**④Output Device Transform (ODT)** 这是 ACES 色彩管理流程中图像信号最终输出转换，就是转换成最终播出设备所需要的色彩空间，例如 Rec.709、Rec.2020、DCI-P3 等。

可以看到一个完整的 ACES 流程非常复杂，所幸的是 Unity 提供了相关的函数，在 Core RP Library 的 ACES.hlsl 和 Color.hlsl 文件中，调用代码如下（顺便提一下，Unity 的 `AcesTonemap()` 函数默认情况下也是使用的较为复杂的拟合函数，除非我们设置 `TONEMAPPING_USE_FULL_ACES` 为 1），先将颜色转换至 ACES 色彩空间，再映射：  

    float4 ToneMappingACESFullFrag(Varyings IN) : SV_TARGET
    {
        float3 color = SAMPLE_TEXTURE2D_LOD(_BlitTexture, sampler_LinearClamp, IN.uv, 0).rgb;
        return float4(AcesTonemap(unity_to_ACES(color)), 1.0);
    }

由于完整的 ACES 流程计算量较大，所以有大佬提出了提出了拟合 ACES 曲线的代码，比较出名的有 Stephen Hill Fit 和 Krzysztof Narkowicz Fit，代码分别来源于：https://github.com/TheRealMJP/BakingLab/blob/master/BakingLab/ACES.hlsl ，https://knarkowicz.wordpress.com/2016/01/06/aces-filmic-tone-mapping-curve/ 。

**Stephen Hill Fit**：  

    static const float3x3 ACESInputMat =
    {
        {0.59719, 0.35458, 0.04823},
        {0.07600, 0.90834, 0.01566},
        {0.02840, 0.13383, 0.83777}
    };

    static const float3x3 ACESOutputMat =
    {
        { 1.60475, -0.53108, -0.07367},
        {-0.10208,  1.10813, -0.00605},
        {-0.00327, -0.07276,  1.07602}
    };

    float3 RRTAndODTFit(float3 v)
    {
        float3 a = v * (v + 0.0245786f) - 0.000090537f;
        float3 b = v * (0.983729f * v + 0.4329510f) + 0.238081f;
        return a / b;
    }

    float3 ACESStephenHillFit(float3 color)
    {
        color = mul(ACESInputMat, color);

        // Apply RRT and ODT
        color = RRTAndODTFit(color);

        color = mul(ACESOutputMat, color);

        // Clamp to [0, 1]
        color = saturate(color);
        return color;
    }

**Krzysztof Narkowicz Fit**：

    float3 ACESApproxFit(float3 color)
    {
        color *= 0.6;
        float a = 2.51;
        float b = 0.03;
        float c = 2.43;
        float d = 0.59;
        float e = 0.14;
        return saturate(color * (a * color + b)/(color * (c * color + d) + e));
    }

Krzysztof Narkowicz Fit 相对来说拟合误差更大，但是计算量小很多。上述 3 种实现方式的效果就不一一展示了，效果大差不差，下面展示的是 Stephen Hill Fit 的基准参考图效果：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/21/l3PMc9TmE4oVK27.jpg" width = "80%" height = "80%" alt="图101 - ACES"/>
</div>

## AgX
AgX 因为 Blender 在使用开始逐渐出名，是由 Troy Sobotka 开发的，详见 https://github.com/sobotka ，最初为 OpenColorIO 编写。**OpenColorIO (OCIO)** 是一个开源的色彩管理框架，专为电影、视觉效果、动画和游戏等视觉创作行业设计。OCIO 包含一个配置文件：config.ocio，该配置文件包含了 LUT 文件的索引，支持多种 LUT 格式，如 .cube、.3dl、.spi1d 等。LUT 的相关知识，后面讲 Color Grading 时会详细讲解。

AgX 的 LUT 生成代码可以看 https://github.com/sobotka/SB2383-Configuration-Generation/blob/main/AgX.py 或 https://github.com/EaryChow/AgX_LUT_Gen 。有人将其代码转换为了 Shader 代码，见 https://github.com/MrLixm/AgXc 或 https://www.shadertoy.com/view/dtSGD1 。但实现起来仍然过于复杂，我又在网上找到了近似的方案：详见 https://iolite-engine.com/blog_posts/minimal_agx_implementation 或 https://www.shadertoy.com/view/cd3XWr 。近似代码如下：  

    // 0: Default, 1: Golden, 2: Punchy
    #define AGX_LOOK 0

    float3 AgXDefaultContrastApprox(float3 x)
    {
        float3 x2 = x * x;
        float3 x4 = x2 * x2;
    
        return + 15.5     * x4 * x2
            - 40.14    * x4 * x
            + 31.96    * x4
            - 6.868    * x2 * x
            + 0.4298   * x2
            + 0.1191   * x
            - 0.00232;
    }

    float3 AgX(float3 val)
    {
        const float3x3 agx_mat = float3x3( 0.842479062253094,   0.0784335999999992,  0.0792237451477643,
                                        0.0423282422610123,  0.878468636469772,   0.0791661274605434,
                                        0.0423756549057051,  0.0784336,           0.879142973793104);
            
        const float min_ev = -12.47393f;
        const float max_ev = 4.026069f;
        
        // Input transform
        val = mul(agx_mat, val);
        
        // Log2 space encoding
        val = clamp(log2(val), min_ev, max_ev);
        val = (val - min_ev) / (max_ev - min_ev);
        
        // Apply sigmoid function approximation
        val = AgXDefaultContrastApprox(val);
        
        return val;
    }

    float3 AgXEotf(float3 val)
    {
        const float3x3 agx_mat_inv = float3x3(  1.19687900512017,    -0.0980208811401368,  -0.0990297440797205,
                                            -0.0528968517574562,   1.15190312990417,    -0.0989611768448433,
                                            -0.0529716355144438,  -0.0980434501171241,   1.15107367264116);
            
        // Undo input transform
        val = mul(agx_mat_inv, val);
        
        // sRGB IEC 61966-2-1 2.2 Exponent Reference EOTF Display
        // NOTE: We're linearizing the output here. Comment/adjust when not using a sRGB render target
        val = pow(val, 2.2);
        
        return val;
    }

    float3 AgXLook(float3 val)
    {
        const float3 lw = float3(0.2126, 0.7152, 0.0722);
        float luma = dot(val, lw);
        
        // Default look
        float3 offset = float3(0.0, 0.0, 0.0);
        float3 slope = float3(1.0, 1.0, 1.0);
        float3 power = float3(1.0, 1.0, 1.0);
        float sat = 1.0;

        #if AGX_LOOK == 1
        // Golden
        slope = float3(1.0, 0.9, 0.5);
        power = float3(0.8, 0.8, 0.8);
        sat = 0.8;
        #elif AGX_LOOK == 2
        // Punchy
        slope = float3(1.0, 1.0, 1.0);
        power = float3(1.35, 1.35, 1.35);
        sat = 1.4;
        #endif
        
        // ASC CDL
        val = pow(val * slope + offset, power);
        return luma + sat * (val - luma);
    }

    float3 AgXApprox(float3 color)
    {
        color = AgX(color);
        color = AgXLook(color);
        color = AgXEotf(color);
        return color;
    }

<div  align="center">  
<img src="https://s2.loli.net/2025/03/21/PSK4FfIHzbQ8C7l.jpg" width = "80%" height = "80%" alt="图102 - AgX default"/>
</div>

<div  align="center">  
<img src="https://s2.loli.net/2025/03/21/dyEtIKrWqRm5P4Y.jpg" width = "80%" height = "80%" alt="图103 - AgX Punchy"/>
</div>


# Color Grading
从理论上来说，调色阶段可以分为两个步骤：①**色彩校正 Color Correction**，该步骤的主要目的是让画面进入一个中性的基线，以便开始调色工作，其中包括调整白平衡、平衡曝光、减少高光、增加中间色调和降噪等等。色彩校正完进入 ②**色彩分级 Color Grading**，即风格化地调整颜色，以满足美学上的需求。但是在实际操作中，人们不会特别地区分这两个步骤，常常将它们合并称为 Color Grading 或者 Color Correction。而且对于游戏来说，我们得到的渲染图在没调色前，通常来讲就已经处于相对来说较为中性的状态，故可以直接进行 Color Grading 工作。

下面要讲解的 Color Grading 工具有：**Color Adjustments**、**White Balance** 和 **Shadows Midtones Highlights**，基本上参考的是 URP 的实现。其他 Color Grading 工具诸如 **Spilt Toning**、**Channel Mixer**、**Lift Gamma Gain**、**Color Curves** 等就不在这里一一进行阐述了，直接抄 URP 或者 HDRP 的实现就可以。而诸如 **Vignette**、**Film Grain**、**Chromatic Aberration**、**Depth of Field**、**Len Distortion**、**Len Flare**、**Panini Projection** 等等效果，并不属于 Color Grading 范畴，故不在文章中说明了，在 URP 或 HDRP 中，这些效果会和 **Bloom** 一起在名为 UberPost 或 FinalPass 的 shader 里进行混合计算。Unity 也会将所有的 Color Grading 工具（包含 **Tone Mapping**）烘焙进一个 LUT Texture，然后也是在 UberPost shader 中进行混合，应用至原渲染图，这个在后面讲 LUT 时会提到。

上述所有后处理效果的执行，其实是有个顺序的问题的，比如 Bloom 应该在 Color Grading 前还是后，不同的顺序出来的效果是不同的。但是代码中的顺序是固定的，要调整需要直接更改代码，较为麻烦，除非专门开发一个连连看的工具用于生成 Shader 代码。整个后处理流程的顺序我基本上参考了 URP 或者 HDRP 的执行顺序，而 Color Grading 各个工具的执行顺序我参考了调色的基本顺序，即先整体再局部，最后艺术风格调整，和 URP 或者 HDRP 略有不同。其实这个执行顺序是仁者见仁智者见智的，我们写 SRP 的时候应该弄一个普适的顺序，然后在具体项目当中，再根据需求去调整顺序，甚至删减未用到的后处理效果，最终都是要服务于需求的。下面介绍的工具的应用顺序，我是先 White Balance -> Color Filter & Hue Shift -> Exposure -> Saturation -> Contrast -> Shadows Midtones Highlights & Split Toning & Lift Gamma Gain 的。

这里额外提一下，从理论上来说 Color Grading 应该发生在 Tone Mapping 和 Gamma Correction 后的 sRGB 色彩空间上，因为这样 Color Grading 最直观也最容易使用。但是这样存在一个问题，就是对于 HDR 显示屏而言，无法统一 HDR 和 LDR 的 Color Grading 的参数，因为 HDR 不需要 Tone Mapping，这就造成了 HDR 和 LDR 的不一致性。为了统一参数，我们会将 Color Grading 发生在 Tone Mapping 之前，这样无论输出在 HDR 还是 LDR 的显示屏上，颜色是一致的。

## Color Adjustments
Color Adjustments 工具主要包括曝光度 Exposure、对比度 Contrast、饱和度 Saturation、色调改变 Hue Shift 和颜色过滤 Color Filter。这些参数的范围，可以参考如下 VolumeComponent 代码（摘自 URP）：

``` C#
public class GlobalColorCorrections : VolumeComponent, IPostProcessComponent
{
    public FloatParameter postExposure = new FloatParameter(0f);

    public ClampedFloatParameter contrast = new ClampedFloatParameter(0f, -100f, 100f);

    public ColorParameter colorFilter = new ColorParameter(Color.white, true, false, true);

    public ClampedFloatParameter hueShift = new ClampedFloatParameter(0f, -180f, 180f);

    public ClampedFloatParameter saturation = new ClampedFloatParameter(0f, -100f, 100f);
}
```

然后传递给 Shader：  

``` C#
buffer.SetGlobalVector(colorAdjustmentsId, new Vector4(
        Mathf.Pow(2f, m_ColorAdjustments.postExposure.value),
        m_ColorAdjustment.contrast.value * 0.01f + 1f,
        m_ColorAdjustment.hueShift.value * (1f / 360f),
        m_ColorAdjustment.saturation.value * 0.01f + 1f
    ));
buffer.SetGlobalColor(colorFilterId, m_ColorAdjustment.colorFilter.value.linear);
```

### Exposure Saturation Contrast
亮度、饱和度和对比度调整的逻辑在[《Unity Shader入门精要》读书笔记（四）](https://ybniaobu.github.io/2023/12/19/2023-12-19-UnityShader4/#%E8%B0%83%E6%95%B4%E5%B1%8F%E5%B9%95%E7%9A%84%E4%BA%AE%E5%BA%A6%E3%80%81%E9%A5%B1%E5%92%8C%E5%BA%A6%E5%92%8C%E5%AF%B9%E6%AF%94%E5%BA%A6) 中有涉及到，可以参考里面的实现。实现的逻辑和下面要讲的是类似的。

**①Exposure**：  
最常见的表达曝光等级的参数是摄像机镜头的**光圈系数 F-stop**，光圈系数表达了镜头焦距除以入射瞳直径，即相对孔径的倒数。在实际使用中，通常都是用光圈系数来间接表示相对孔径的大小，如 f/1，f/1.4，f/2，f/2.8，f/4，f/5.6，f/8，f/11，f/16，f/22，f/32，f/44，f/64，可以看出每个级数之间的比为 $\,\sqrt 2\,$。而光圈 f 值愈小，在同一单位时间内的进光量便愈多，而且上一级的进光量刚是下一级的两倍。这也是为什么传递 Exposure 参数时，Exposure 作为 2 的指数。然后直接在 Shader 中使用传递过来的曝光参数做乘积即可：  

    float3 Exposure(float3 color)
    {
        return color * _ColorAdjustmentsParams.x;
    }

**②Saturation**：  
饱和度就是在灰色版本的图片和原图片之间进行插值，获取灰色版本的图片就是将 RGB 值转换为 3 个通道都一样的值，但保留像素原本的亮度，这样才能区分物体，否则整张图片就是统一的灰色，这将和对比度没有区别了。但是由于眼睛对红、绿、蓝的感知力是不同的，所以需要亮度转换公式，Core RP Library 的 Color.hlsl 提供了转换的函数 `Luminance()`。

    float3 Saturation(float3 color)
    {
        float luminance = Luminance(color);
        return lerp(float3(luminance, luminance, luminance), color, _ColorAdjustmentsParams.w);
    }

**③Contrast**：  
对比度就是在中灰度（rgb 都为 0.5）和原图片之间进行插值，但是直接插值可能会导致阴影处的值低于 0，从而丢失暗处细节，所以会先将 linear 空间颜色转换至 log 空间进行对比度调整，再转换回线性空间，以确保值不会低于 0：  

    float3 Contrast(float3 color)
    {
        color = LinearToLogC(color);
        color = lerp(float3(0.5, 0.5, 0.5), color, _ColorAdjustmentsParams.y);
        return LogCToLinear(color);
    }

Exposure Saturation Contrast 三个效果展示如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/01/yFCUYxaeV3Hpubw.png" width = "100%" height = "100%" alt="图104 - 左图：原图，右图：调整 Exposure 为 1，Saturation 为 50，Contrast 为 20。"/>
</div>

### Color Filter & Hue Shift
Color Filter 就不用讲了，直接乘上颜色就行。Hue Shift 就是将 RGB 转换至 HSV 颜色空间，然后调整色相 H 就行。之所以将 hueShift 参数设置为 -180 到 180，是为了模拟色轮 color wheel：  

    float3 HueShift(float3 color)
    {
        color = RgbToHsv(color);
        float hue = color.x + _ColorAdjustmentsParams.z;
        color.x = RotateHue(hue, 0.0, 1.0);
        return HsvToRgb(color);
    }

效果就不展示了。

## White Balance
关于色温和白平衡等相关色度学知识，足以开好几篇文章进行讲解，由于篇幅所限，这里就简单介绍一下，不深入。以后 SRP 需要基于物理的灯光和摄像机时再详细处理（又挖了个坑）。

首先人眼具有**色彩恒常性 Color Constancy**，能够补偿光源变化带来的颜色变化从而使物体被视为相同的颜色，即拥有自动白平衡的能力。这种能力使我们在日光、阴天、室内照明等不同光照条件下，仍然能够识别出物体的颜色。例如，一张白纸在日光下和在钨丝灯下看起来仍然是白色，即使这两种光源的色温差异很大。但是图像传感器是没有自动白平衡这个功能的，故不同色温的光下白色物体是不同颜色的。

所以**白平衡 White Balance** 的目标是把不同光源下的白色物体都矫正成人眼认知中的白色，让图像中不存在色偏。由于白色不含任何色度信息，其显色效果最佳，因此它为被选定为基准色，即所谓的**白点 White Point**。每个色彩空间都有一个特定的白点，例如 sRGB 、 Adobe RGB 和 Display-P3，白点是 D65，色温约为 6500K；对于 DCI-P3 白点是 DCI 白点，色温约为 6300K。

白平衡的算法其实有很多种，包含**完美反射算法 White Patch Algorithm**、**灰色世界算法 Gray World Algorithm** 等等，这里先直接抄 Unity 的实现，日后有更高级的需求时，再了解算法并修改。Unity 的实现中，有两个参数 Temperature 和 Tint，我们使用 Core RP library 的 `ColorUtils.ColorBalanceToLMSCoeffs()` 函数将这两个系数转换为 LMS 色彩空间的系数：  

``` C#
public class WhiteBalance : VolumeComponent, IPostProcessComponent
{
        public ClampedFloatParameter temperature = new ClampedFloatParameter(0f, -100, 100f);
        public ClampedFloatParameter tint = new ClampedFloatParameter(0f, -100, 100f);
}
```

``` C#
data.buffer.SetGlobalVector(k_WhiteBalanceId, ColorUtils.ColorBalanceToLMSCoeffs(m_WhiteBalance.temperature.value, m_WhiteBalance.tint.value));
```

然后在 Shader 中在 LMS 色彩空间乘上系数即可：  

    float3 WhiteBalance(float3 color)
    {
        color = LinearToLMS(color);
        color *= _WhiteBalance.rgb;
        return LMSToLinear(color);
    }

白平衡效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/01/luvLU8Z3rQmBVtJ.png" width = "100%" height = "100%" alt="图105 - 左图：画面偏冷（temperature 为 -10），右图：画面偏暖（temperature 为 20）。"/>
</div>

## Shadows Midtones Highlights
**Shadows Midtones Highlights**、**Split Toning** 和 **Lift Gamma Gain** 这三个工具实现的方法和目的都是类似的，都是调整图片的**亮部 Highlights**、**暗部 Shadows** 或者**灰部 Midtones**。他们的区别在于 Shadows Midtones Highlights 工具对亮部暗部以及灰部的调整具有一定的截断性，相对来说不是很平滑。而 Lift Gamma Gain 工具对亮部暗部以及灰部的调整比较平滑，但是存在相互影响的问题。Split Toning 工具则是为了增加明暗对比度，调整明暗颜色，所以它只有对亮部和暗部的调整。这里只介绍 Shadows Midtones Highlights 的实现，其余的实现直接看 URP 或 HDRP。

Shadows Midtones Highlights 工具的参数有如下：  

``` C#
public class ShadowsMidtonesHighlights : VolumeComponent, IPostProcessComponent
{
    public Vector4Parameter shadows = new Vector4Parameter(new Vector4(1f, 1f, 1f, 0f));
    public Vector4Parameter midtones = new Vector4Parameter(new Vector4(1f, 1f, 1f, 0f));
    public Vector4Parameter highlights = new Vector4Parameter(new Vector4(1f, 1f, 1f, 0f));
    
    public MinFloatParameter shadowsStart = new MinFloatParameter(0f, 0f);
    public MinFloatParameter shadowsEnd = new MinFloatParameter(0.5f, 0f);
    public MinFloatParameter highlightsStart = new MinFloatParameter(0.5f, 0f);
    public MinFloatParameter highlightsEnd = new MinFloatParameter(1f, 0f);
}
```

如果想要 URP 或 HDRP 编辑器中的色轮工具对参数进行调整，抄 VolumeComponentEditor 的相关代码，因代码量比较大，就不展示了。然后还是使用 Core RP library 的 ColorUtils 内的一个函数传递参数：  

``` C#
var (shadows, midtones, highlights) = ColorUtils.PrepareShadowsMidtonesHighlights(m_ShadowsMidtonesHighlights.shadows.value, 
    m_ShadowsMidtonesHighlights.midtones.value, m_ShadowsMidtonesHighlights.highlights.value);
data.buffer.SetGlobalVector(k_SMHShadowsID, shadows);
data.buffer.SetGlobalVector(k_SMHMidtonesID, midtones);
data.buffer.SetGlobalVector(k_SMHHighlightsID, highlights);
data.buffer.SetGlobalVector(k_SMHRangeID, new Vector4(m_ShadowsMidtonesHighlights.shadowsStart.value, m_ShadowsMidtonesHighlights.shadowsEnd.value, 
    m_ShadowsMidtonesHighlights.highlightsStart.value, m_ShadowsMidtonesHighlights.highlightsEnd.value));
```

然后 Shader 中：  

    float3 ShadowsMidtonesHighlights(float3 color)
    {
        float luminance = Luminance(color);
        float shadowsWeight = 1.0 - smoothstep(_SMHRange.x, _SMHRange.y, luminance);
        float highlightsWeight = smoothstep(_SMHRange.z, _SMHRange.w, luminance);
        float midtonesWeight = 1.0 - shadowsWeight - highlightsWeight;
        return color * _SMHShadows.rgb * shadowsWeight + color * _SMHMidtones.rgb * midtonesWeight + color * _SMHHighlights.rgb * highlightsWeight;
    }

为了更好展示 Shadows Midtones Highlights 工具的作用，我将亮部暗部以及灰部的颜色调整得较为奇葩，亮部为红色，灰部为蓝色，暗部为绿色（还别有一番风味）：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/01/hxuJZeGcRvAEN8D.jpg" width = "60%" height = "60%" alt="图106 - Shadows Midtones Highlights 效果展示"/>
</div>

## ACES 色彩空间下调色
之前讲 ACES 工作流程时大致提到过，在 ACES 的工作流程下，调色最好在 ACES 色彩空间下。我们主要在 ACEScg 这个线性色彩空间下进行调色，故需要先将线性空间转换至 ACEScg 线性空间：  

    color = unity_to_ACEScg(color);

但是 WhiteBalance 除外，因为它是在 LMS 色彩空间中调整的。Color Adjustments 工具中基本都需要在 ACEScg 中进行调整，除了 Contrast，因为 Contrast 是在 log 色彩空间中进行的，而 ACES 的 log 空间为 ACEScc，故需要将 ACEScg 转换至 ACEScc：  

    float3 Contrast_ACES(float3 color)
    {
        color = ACES_to_ACEScc(ACEScg_to_ACES(color));
        color = lerp(ACEScc_MIDGRAY, color, _ColorAdjustmentsParams.z);
        return ACES_to_ACEScg(ACEScc_to_ACES(color));
    }

ACEScc 色彩空间的中灰点也是不同的，使用 Unity 提供的宏 `ACEScc_MIDGRAY`。然后 ACEScg 色彩空间中计算亮度 Luminance 也是不同的，要调用 `AcesLuminance()` 而不是 `Luminance()` 方法，故 Saturation 方法也要调整：  

    float3 Saturation_ACES(float3 color)
    {
        float luminance = AcesLuminance(color);
        return lerp(float3(luminance, luminance, luminance), color, _ColorAdjustmentsParams.w);
    }

其他工具若也计算了 Luminance 也是同样的处理方法，就不额外说明了。

## Lookup Table (LUT)
直接对屏幕的每个像素进行 color grading 计算相对比较消耗性能，我们可以将所有的 color grading 工具烘焙进一个 LUT 贴图，然后对其进行采样，这样子计算量会小很多。LUT 是一个 3D 纹理，可以将其转换为一个 2D 纹理，比如 32 × 32 × 32 转化为 1024 × 32。我们可以在 PipelineAsset 文件中设置一个 LUTSize 属性，用于控制 LUT 的分辨率，分辨率分别为 16、32、64，然后根据属性生成 LUT 贴图。

``` C#
int lutHeight = asset.bakedLUTResolution;
int lutWidth = lutHeight * lutHeight;
data.buffer.GetTemporaryRT(RenderTargetIDs.k_ColorGradingLutTextureId, lutWidth, lutHeight, 0, FilterMode.Bilinear, RenderTextureFormat.DefaultHDR);
```

除了烘焙 color grading，tone mapping 也可以烘焙进去。代码如何更改 CPU 端就不详细展示了，下面讲一下 GPU 端，反正就是根据不同 tone mapping 算法，选择不同的 Pass，将 color grading 和 tone mapping 共同绘制进 LUT Texture，最后别忘了 `ReleaseTemporaryRT()`。

### 生成 LUT 颜色
首先 LUT 的基本颜色是根据 uv 去生成的，Core RP Library 的 Color.hlsl 提供了相关函数 `GetLutStripValue()`。生成颜色后再进行 Color Grading 改变颜色：  

    float3 GetColorGradedLUT(float2 uv) 
    {
        float3 color = GetLutStripValue(uv, _ColorGradingLUTParameters);
        return ColorGrading(color);
    }

`_ColorGradingLUTParameters` 是 LUT Texture 的分辨率相关参数：  

``` C#
data.buffer.SetGlobalVector(k_ColorGradingLUTParamsId, new Vector4(lutHeight, 0.5f / lutWidth, 0.5f / lutHeight, lutHeight / (lutHeight - 1.0f)));
```

然后根据 tone mapping 的设置去选择不同的 tone mapping 算法：  

    float4 ColorGradingReinhardSimpleFrag(Varyings IN) : SV_TARGET
    {
        float3 color = GetColorGradedLUT(IN.uv);
        return float4(Reinhard(color), 1.0);
    }

    ...

    float4 ColorGradingACESFullFrag(Varyings IN) : SV_TARGET
    {
        float3 color = GetColorGradedLUT_ACES(IN.uv);
        return float4(AcesTonemap(ACEScg_to_ACES(color)), 1.0);
    }

    ...

这样子就可以生成 LUT 贴图了，如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/01/AMBCKTvOcEHh3V2.png" width = "75%" height = "75%" alt="图107 - 未做 Color Grading 的 Lut 贴图。上：None；中：Reinhard Simple；下：ACES"/>
</div>

### Log C LUT
上面的 LUT 的问题在于，`GetLutStripValue()` 函数生成的基本颜色值是在 0 - 1 的范围内的，这样子生成的 Lut 就不支持 HDR 颜色了。为了解决这个问题，我们可以假设 `GetLutStripValue()` 函数生成的基本颜色值是在 Log 颜色空间，Log 颜色空间能够更高效地存储更广的亮度范围，通过非线性对数曲线压缩了亮度信息，并且通常归一化至 0 - 1 的范围。

然后在 ColorGrading 前将颜色转换回线性空间即可：  

    float3 GetColorGradedLUT(float2 uv) 
    {
        float3 color = GetLutStripValue(uv, _ColorGradingLUTParameters);
        return ColorGrade(LogCToLinear(color));
    }

这样子生成的 LUT 贴图如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/01/PRq1jdgDY2azxQN.png" width = "75%" height = "75%" alt="图108 - log 空间 lut 贴图。上：None；中：Reinhard Simple；下：ACES"/>
</div>

当然我们要根据是否使用 HDR 工作流程来选择使用 Log LUT 还是 Linear LUT。毕竟对于 LDR 流程来说，Log LUT 会损失一定的颜色精度。

## UberPost Shader
在将 Color Grading 和 Tone Mapping 烘焙进 LUT 后，我们要在另外一个 Shader 里采样 LUT 应用至渲染图中。这个 Shader 叫做 **UberPost**，即超级后处理，之所以叫这个名字，是因为要在该 Shader 中处理所有后处理工具以及工具产生的资源。比如 Bloom，我们之前将模糊后的渲染图叠加到原渲染图上产生泛光效果，那么我们不在 Bloom 阶段做叠加，Bloom 阶段只输出模糊后的渲染图这么一个资源，命名为 BloomTexture。然后将所有资源，包括 BloomTexture、LUT 以及其他后处理效果，一起在 UberPost 中一个一个叠加到原渲染图上。  

Bloom 以及其他后处理，比如 Vignette、Film Grain、Chromatic Aberration、Depth of Field、Len Distortion、Len Flare、Panini Projection 等等效果，在 UberPost 中的代码具体详见 URP 或者 HDRP，就不在这里展示了，直接抄 Unity 的就完事了，想要比 Unity 的实现更好的效果，以后可以根据需求一个一个去处理。

这里只展示如何应用 Baked Color Grading Lut，首先 Core RP Library 的 Color.hlsl 提供了采样 LUT 的函数叫 `ApplyLut2D()`：

    float4 UberPostProcessingFrag(Varyings IN) : SV_TARGET
    {
        float3 color = SAMPLE_TEXTURE2D_LOD(_BlitTexture, sampler_LinearClamp, IN.uv, 0).rgb;
        color = ApplyLut2D(_ColorGradingLutTexture, sampler_LinearClamp, saturate(LinearToLogC(color)), _ColorGradingLutParams.xyz);
        ...
    }

别忘了将颜色从线性空间转换至 Log 空间进行采样。`_ColorGradingLutParams` 的各个参数如下：  

    data.buffer.SetGlobalVector.SetVector(k_ColorGradingLutParamsId, new Vector4(1.0f / lutWidth, 1.0f / lutHeight, lutHeight - 1.0f));

我将大部分后处理效果实现后，搞出了如下效果图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/01/7REkTWumt1sSlZn.jpg" width = "90%" height = "90%" alt="图109 - 瞎几把搞出来的效果图"/>
</div>

## Color Banding
**色彩分层/色彩断代 Color Banding** 是一个非常常见的 artifact 现象。这个现象产生的原因是由于颜色位深（即精度）不够，特别是在色调调整后，很容易出现环状阶梯型的颜色带，如下图所示：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/01/tcXxHMRJuysiELq.png" width = "60%" height = "60%" alt="图110 - Color Banding 现象"/>
</div>

我们应用 LUT 时，因为使用 bilinear 采样器，这种现象会在一定程度上缓解，就会产生如上图的相对平滑的 Color Banding。若使用 point 采样器，这种现象会格外明显。这种现象的缓解方法就是大名鼎鼎的**抖动 Dither**，即向图片增加噪声，随机化误差从而减少大尺度的 pattern，效果如下图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/01/bXGWlEh8t6jYg4n.png" width = "75%" height = "75%" alt="图111 - 左：原图；中：转换至低位深后的图片；右：添加抖动后的图片"/>
</div>

抖动除了可以缓解色带问题，也可以用于缓解阴影的噪点问题。但是在这里，先不讲解如何应用 Dither，我打算在处理 TAA 的时候一并实现。

> 类似于 Dither 还有一个也叫抖动的称为 Jitter，它俩的区别在于，Dither 强调空间维度，Jitter 强调时间维度。Jitter 通常运用在体积光、雾的渲染中，以及光线跟踪的重要性采用中。


# Multiple Cameras
本章节讲解如何支持多相机渲染，多相机渲染的需求多种多样，有多人分屏、UI 中的 3D 人物（3D character portraits）、上帝视角游戏的全局视角、拿起物体时的物体特写等等。其实对于第二个摄像头，未必需要跟第一个摄像头具有一样的渲染流程，可以简化很多，比如为物体特写实现 MatCap。下面介绍的几种需求的特定方案，没必要在 SRP 中同时支持，故这里以摘抄翻译记录为主，之后根据项目需求选择特定方案即可。

## Split Screen
分屏其实很简单，只要将第一个相机的 viewport rect 的 width 设置为 0.5，第二个相机的 width 也设置为 0.5 以及 X position 设置为 0.5，就可以了。但是我们会发现，这样做不成功了，每个相机都渲染出了图片，但是拉伸覆盖了整个屏幕。产生这个现象的原因是，我们没有调用 `CommandBuffer.SetViewport()` 设置视口：

``` C#
public static void BlitCameraTarget(CommandBuffer cmd, int sourceID, Rect cameraRect, Material material, int pass)
{
    cmd.SetGlobalTexture(k_BlitTextureId, new RenderTargetIdentifier(sourceID));
    cmd.SetRenderTarget(new RenderTargetIdentifier(BuiltinRenderTextureType.CameraTarget), RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store);
    cmd.SetViewport(cameraRect);
    cmd.DrawProcedural(Matrix4x4.identity, material, pass, MeshTopology.Triangles, 3);
}
```

然后在后处理最后的输出阶段调用上述函数，并传递进 `camera.pixelRect` 即可支持分屏。  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/04/EJiCOMdhYqc2BT1.jpg" width = "50%" height = "50%" alt="图112 - 分屏"/>
</div>

catlikecoding 教程中还提到 tile-based GPU 可能在视口边缘部分出现 artifact，解决方法如下，先判断是否视口是否有缩放，再决定 RenderBufferLoadAction：

``` C#
Rect fullViewRect = new Rect(0f, 0f, 1f, 1f);
...
cmd.SetRenderTarget(BuiltinRenderTextureType.CameraTarget,
			camera.rect == fullViewRect ? RenderBufferLoadAction.DontCare : RenderBufferLoadAction.Load,
			RenderBufferStoreAction.Store);
```

## Layering Cameras
相机叠加也是同理，可以第二个相机的 width 和 height 设置为 0.5，XY position 设置为 0.25，这样第二个相机的图片就会叠加在第一张图片的上面了。但是因为我们之前实现 Post Processing 时，直接清除了 color buffer 和 depth buffer，导致之前的 CameraClearFlags 的 Depth Only 模式失效了，即不渲染 Skybox 直接将第二张图片叠加上去，像是物体悬浮在第一张图片上面一样，如下图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/07/RmMwILdXvyjNPVO.jpg" width = "50%" height = "50%" alt="图113 - Layering Cameras"/>
</div>

要想实现这个效果，我们可以在最后的 blit 进 CameraTarget 的 pass 中将混合因子改为 One OneMinusSrcAlpha，为什么不是 SrcAlpha OneMinusSrcAlpha 下面会说：  

    Pass
    {
        Name "Final Pass"
        
        Blend One OneMinusSrcAlpha
        
        HLSLPROGRAM
        #pragma target 3.5
        #pragma vertex CopyVert
        #pragma fragment UberPostProcessingFrag
        ...
        ENDHLSL
    }


并且将 RenderBufferLoadAction 改为 Load 模式：  

``` C#
...
cmd.SetRenderTarget(BuiltinRenderTextureType.CameraTarget, RenderBufferLoadAction.Load, RenderBufferStoreAction.Store);
...
```

其实这样理论上就应该可以实现之前 Depth Only 模式的效果了，只要我们将相机的 Background color 的 alpha 设置为 0（当然别忘了当时 ClearRenderTarget 用 data.camera.backgroundColor.linear 覆盖颜色）。但是现在背景仍然不能处于透明状态，这是因为我们没有在片元着色器中输出 Alpha 值，Alpha 值直接默认为 1 了。

这里注意一下，如果我们是 Bloom 完将 Bloom 模糊结果和原图叠加后，再进行 Post Processing 的，就需要在 Bloom 的最后的 pass 以及后面的 UberPost 的 pass，将 `_BlitTexture` 的 Alpha 输出。若 Bloom 模糊结果和其他 Post Processing 一起在 UberPost Shader 中做混合，则只需要在 UberPost Shader 将 `_BlitTexture` 的 Alpha 输出，类似如下：  

    float4 UberPostProcessingFrag(Varyings IN) : SV_TARGET
    {
        float4 inputColor = SAMPLE_TEXTURE2D_LOD(_BlitTexture, sampler_LinearClamp, IN.uv, 0);
        ...
        return float4(color, inputColor.a);
    }

这样子就可以实现之前的 Depth Only 模式的效果了，但是若使用 SrcAlpha OneMinusSrcAlpha 的话，bloom 效果无法保留，因为 bloom 模糊的区域 Alpha 为 0，所以要使用 One OneMinusSrcAlpha。当然我觉得在 Bloom 阶段把 Alpha 也做模糊处理可能效果更好，我没试过，有需求的话可以尝试一下。

<div  align="center">  
<img src="https://s2.loli.net/2025/04/07/plRy8WqO7sCPkZ1.png" width = "75%" height = "75%" alt="图114 - 左：SrcAlpha OneMinusSrcAlpha；右：One OneMinusSrcAlpha"/>
</div>

## UI Render Texture
除了上述的方法外，还可以将相机渲染至 Render Texture，然后将 Render Texture 作为 UI 的 Raw Image 显示。首先通过 Assets / Create / Render Texture 创建渲染纹理，再将该渲染纹理设置为 camera 的 Target Texture。然后通过 GameObject / UI / Raw Image 创建 raw image component，将其 Texture 属性设置为我们创建的渲染纹理，即可在屏幕上显示。

> 不知道为什么，我将相机的 Target Texture 设置为 Render Texture，但是打开 Frame Debugger 后发现并没有渲染至该纹理，而是渲染进了 SceneView RT 当中。我不知道是不是 BuiltinRenderTextureType.CameraTarget 的原因，打算之后使用 RTHandle API 时再解决。这里先翻译一下教程内容做个记录。

raw image 使用默认的 UI 材质，使用的是标准的 SrcAlpha OneMinusSrcAlpha 混合，所以上面提到的 Bloom 没效果的问题也会存在。要想解决，我们需要复制下来默认的 Default-UI shader，并改变里面的混合模式。这个 Shader 里面的代码就不摘抄下来了。


# Rendering Layers
Rendering Layers 主要是用于控制物体是否受到灯光的影响，这个功能我觉得没有什么特别的应用场景，所以我不打算支持，故下面只是简单摘抄翻译原教程的内容，略微介绍如何实现，不打算详细说明。

## Culling Masks
场景中物体都可以设置一个 Layer。在编辑模式下（场景的右上方），有个 Layers 下拉菜单可以设置，选择 Show/Hide Layer，就可以决定该物体是否显示。类似的，在 Game 模式下，我们可以设置相机的 Culling Mask 属性来限制场景中显示的物体。

灯光也有 Culling Mask 属性，理论上它应该可以限制物体是否受到灯光的影响。但是如果我们尝试一下就会发现，目前该属性只能限制物体是否受到阴影的影响。这是因为我们在 SRP 做过两次 Culling，一次是摄像机的 Culling，一次是阴影的 Culling。相机的 Culling Mask 属性决定了物体是否在渲染阶段被剔除，灯光的 Culling Mask 属性决定了物体是否在阴影贴图阶段被剔除。而原先 Unity 灯光的 Culling Mask 是通过 per-object light indices 实现的，而我们不支持。

其实 HDRP 也去除掉了 Light 的 Culling Mask 这个功能。SRP 另外提供了 rendering layers 作为该功能的替补。rendering layers 最多可以有 32 个 layers，每个 layer 对应一个 bit 值。

## Sending Rendering Layer to the GPU
首先我们需要在 UnityInput 里的 UnityPerDraw 添加 unity_RenderingLayer，注意放置的位置：  

    CBUFFER_START(UnityPerDraw)
    ...
    real4 unity_WorldTransformParams; // w is usually 1.0, or -1.0 for odd-negative scale transforms

    // Render Layer block feature
    // Only the first channel (x) contains valid data and the float must be reinterpreted using asuint() to extract the original 32 bits values.
    float4 unity_RenderingLayer;

    half4 unity_LightData;
    ...
    CBUFFER_END

然后我们就可以通过 `asuint(unity_RenderingLayer.x);` 获取到物体的 Rendering Layer Mask 的值了。除了物体外，我们还需要获取灯光的 Rendering Layer Mask 值，我们可以通过 `light.renderingLayerMask` 获取到，CPU 传递的工作就不再赘述了。到了 GPU 里，我们需要使用按位与 `&` 来比较物体和灯光的 Rendering Layer Mask 的值，类似如下：  

    bool RenderingLayersOverlap (Surface surface, Light light) 
    {
        return (surface.renderingLayerMask & light.renderingLayerMask) != 0;
    }

然后根据比较结果选择是否进行光照计算，代码类似如下：  

    for (int i = 0; i < GetDirectionalLightCount(); i++) 
    {
        Light light = GetDirectionalLight(i, surfaceWS, shadowData);
        if (RenderingLayersOverlap(surfaceWS, light)) 
        {
            color += Lighting(surfaceWS, brdf, light);
        }
    }

## Reinterpreting an Int as a Float
理论上，上面应该已经可以实现 Rendering Layers 的功能了，但是还存在一个问题。`light.renderingLayerMask` 属性获取到的是 int 值，而不是 bit 值，我们传递该 int 值至 GPU 时，会被自动转换为 float。所以我们需要在 CPU 端将 int 按位转换为 float，但是 C# 没有 asuint 函数。

教程中的转换方法有点复杂，C# 中的 System 的 BitConverter 类可以做类似的工作（`BitConverter.UInt32BitsToSingle()`）。我没有实现过，不知道可不可以成功，需要尝试过。

## Camera Rendering Layer Mask
我们也可以为 Camera 实现 rendering layer mask 功能，但是 Camera 默认不自带 rendering layer mask 属性，我们需要为它自定义一个，如何自定义这里就不写了。如何通过 FilteringSettings 的 renderingLayerMask 属性过滤掉不想渲染的物体：  

``` C#
var filteringSettings = new FilteringSettings(RenderQueueRange.opaque, renderingLayerMask: (uint) renderingLayerMask);
```

这样就可以实现 Camera 的 rendering layer mask 功能了，只是这样做，物体不会渲染，但是阴影还在。