---
title: Unity Custom SRP 基础（一）
date: 2024-11-26 12:43:30
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2024/12/09/SGVnTxOM6BURAJI.gif
description: 本笔记的主要内容有 CommandBuffer 的简单介绍；如何创建基本的自定义 SRP；如何让管线支持合批，比如 SRP batcher；如何在管线中实现方向光；自定义 ShaderGUI 的简单介绍（在方向光章节中）。
---

> 本笔记是关于 Unity 的**自定义可编程渲染管线**的入门基础，即 **SRP (Scriptable Rendering Pipeline)**，主要参考了著名的教程 https://catlikecoding.com/ 的 Custom SRP Tutorial，以及知乎上各位图形学大神们的文章。  
>    
> 笔者使用的 Unity 版本是 6000.0.27f1，Core RP Library 的版本是 17.0.3。

# 基础概念介绍
## SRP 和 CommandBuffer
Unity 的 **CommandBuffer** 是一个渲染命令列表，类似于 DirectX 中的**命令列表 Command list**，它们的作用主要是为了 CPU 与 GPU 间的交互。每个 GPU 都至少有一个**命令队列 Command queue**，借助图形 API（比如 Direct3D），CPU 可以利用**命令列表 Command list** 将命令提交到这个队列中。所以 Unity 的 SRP 可以理解为是对图形 API 的封装，并提供了 C# 调用接口。我们使用 C# 脚本来配置渲染命令，告诉 Unity C++ 底层图形框架去执行这些命令，然后 Unity 底层再传递指令给图形 API。

而在 SRP 中，连接 C# 渲染管线代码和底层 C++ 图形框架的正是 **ScriptableRenderContext** 类。SRP 渲染工作使用的是延迟执行，我们使用 `ScriptableRenderContext.ExecuteCommandBuffer` 将 CommandBuffer 队列信息缓存到 ScriptableRenderContext 中，最后 `ScriptableRenderContext.Submit` 集中提交绘制指令。

一个简易的渲染流程图如下：  

<div  align="center">  
<img src="https://s2.loli.net/2024/11/26/hwqQCckXT4eUOn1.png" width = "30%" height = "30%" alt="图1 - 简易渲染流程"/>
</div>

## RenderTarget
**渲染目标 RenderTarget** 是现代图形处理单元 GPU 的一个特征，它允许将 3D 场景渲染到中间存储缓冲区或**渲染目标纹理 RenderTargetTexture（RTT）**，而不是帧缓冲区或后缓冲区。然后可以通过像素着色器操纵此 RTT ，以便在显示最终图像之前将其他效果应用于最终图像。

而**延迟渲染 Deferred Rendering** 正是得益于硬件对于**多重渲染目标 Multiple Render Target（MRT）** 的支持。其核心在于 **G-Buffer** ，延迟渲染将计算光照的参数渲染至 G-Buffer 中，也就是若干张纹理贴图，通常包含了 Depth，Normal，Albedo，Roughness，Specular 和 Metallic。最后将上述几张贴图进行相关计算混合，生成了游戏中的一帧。

## CBuffer
**常量缓冲区 Constant Buffer** 是 GPU 显存中的一个区域，其数据可供着色器程序引用，专门用于存储一些全局变量（适用于需要 CPU 每帧更新的全局变量），比如变换矩阵、材质属性、光照信息等。而将变量放入 CBuffer 中，可以将其缓存在 GPU 的显存中，Shader 在执行时可以直接从缓存中读取数据，从而提高了渲染性能。

CBuffer 有一个打包规则，里面的变量会按从上往下顺序被打包进一个 16 字节的向量，float4、int4、uint4 刚好是 16 字节。**因此合理的安排 CBuffer 的变量位置可以节省显存**，比如如果一个 float3 + float2 的 CBuffer 会被打包进一个 2 个 16 字节的向量。

    //  2 x 16byte elements
    cbuffer IE
    {
        float4 Val1;
        float2 Val2;  // starts a new vector
        float2 Val3;
    };

    //  3 x 16byte elements
    cbuffer IE
    {
        float2 Val1;
        float4 Val2;  // starts a new vector
        float2 Val3;  // starts a new vector
    };

# 创建基本的自定义 SRP
## 前置工作
①切换色彩空间：在 Edit -> Project Settings -> Player -> Other Settings -> Color Space 中选择 Linear。  
②在 PackageManager 下载 Core RP Library，这个包里有一些可以重复利用的代码来帮助我们创建自己的渲染管线。

## 创建管线资产和实例
要创建基于 SRP 的自定义渲染管线，项目中必须包含以下三个要素：  
①需要有一个脚本继承自 `RenderPipelineAsset` 并覆写它的 `CreatePipeline()` 方法，这个脚本定义了**渲染管线资产 Render Pipeline Asset**；  
②需要有一个脚本继承自 `RenderPipeline` 并覆写它的 `Render()` 方法，这个脚本定义了**渲染管线实例 Render Pipeline Instance**，同时也是写自定义渲染代码的入口；  
③一个从第一步的脚本创建出来的渲染管线资产，这个资产作为创建渲染管线实例的工厂。

首先第一步：创建一个名为 XXRenderPipelineAsset 的 C# 脚本：  

``` C#
using UnityEngine;
using UnityEngine.Rendering;

[CreateAssetMenu(menuName = "XXXX/XXRenderPipelineAsset")]
public class XXRenderPipelineAsset : RenderPipelineAsset
{
    protected override RenderPipeline CreatePipeline() 
    {
        return new XXRenderPipeline();
    }
}
```

Unity 在渲染第一帧前会调用 `CreatePipeline()` 方法，如果 Render Pipeline Asset 的设置发生了改变，Unity 会释放当前的 Render Pipeline Instance 并在下一帧渲染前重新调用这个方法。而这个 Render Pipeline Asset 则是一个存储渲染设置的地方，只不过这里暂时还没有设置，我们可以在 Render Pipeline Asset 存储一些信息以便 Render Pipeline Instance 使用。

第二步：创建一个名为 XXRenderPipeline 的 C# 脚本：  

``` C#
using System.Collections.Generic;
using UnityEngine;
using UnityEngine.Rendering;

public class XXRenderPipeline : RenderPipeline
{
    public XXRenderPipeline() {}

    protected override void Render(ScriptableRenderContext context, Camera[] cameras) 
    {
        // Older version of the Render function that can generate garbage, needed for backwards compatibility
    }

    protected override void Render(ScriptableRenderContext context, List<Camera> cameras) {}
}
```

这个 `Render()` 方法就是我们自定义渲染循环 render loop 的入口，这个方法接受一个 ScriptableRenderContext 和摄像机数组。我们可以看到两个 `Render()` 方法，一个接受摄像机数组，一个接受集合中的列表。而接受摄像机数组的方法由于会因为数组长度变化而造成堆分配的原因而不推荐使用，所以我们使用那个接受摄像机列表的 `Render()` 方法。但是即使我们不使用接受摄像机数组的 `Render()` 方法，我们仍然需要覆写它，因为它是一个抽象方法。

第三步：根据第一步脚本自己设定的 AssetMenu 位置，在项目中右键创建自己的渲染管线资产 XXRenderPipelineAsset，并在 Edit -> Project Settings -> Graphics -> Default Render Pipeline 中替换为自己的渲染管线资产 XXRenderPipelineAsset。此时屏幕会全黑，因为我们还没有设置任何东西，无法渲染。

## 创建渲染循环
Unity 会在渲染的每一帧调用 Render Pipeline Instance 的 `Render()` 方法，并传递一个 ScriptableRenderContext 结构体，用于连接 Unity C++ 底层，以及 Cameras 列表。通过传递过来的 Cameras 列表，可以使用一个循环遍历所有摄像机对 ScriptableRenderContext 进行绘制。

一个简单的渲染循环包含以下三个基本的操作：  
①**Clearing the render target**，摄像机的渲染目标要么是**帧缓冲 framebuffer** 要么是**渲染纹理 render texture**。但无论是哪一个，上一帧绘制的东西仍然在那里，所以我们要清除掉；  
②**Culling**，剔除掉摄像机视锥体以外的物体；  
③**Drawing**，告诉 GPU 绘制哪些几何物体，怎么去绘制。

### Clearing the render target
正如文章开头所说，因为 ScriptableRenderContext 是延迟提交的，直到我们调用 `ScriptableRenderContext.Submit()` 方法把所有安排好的 commands 提交到渲染循环进行执行。所以一开始我们要配置 ScriptableRenderContext 去添加 commands。

首先我们要创建 CommandBuffer 实例，同时给 CommandBuffer 一个名字用于在 Frame debugger 中显示，再加上清除渲染目标的内容，代码如下：  

``` C#
const string bufferName = "Custom Rendering Event";
CommandBuffer buffer = new CommandBuffer 
{
    name = bufferName
};

protected override void Render(ScriptableRenderContext context, List<Camera> cameras) 
{
    foreach(Camera camera in cameras)
    {
        context.SetupCameraProperties(camera);

        // Clearing the render target
        buffer.ClearRenderTarget(true, true, Color.clear);
        buffer.BeginSample(bufferName);
        context.ExecuteCommandBuffer(buffer);
        buffer.Clear();
                
        // Submit all scheduled commands
        buffer.EndSample(bufferName);
        context.ExecuteCommandBuffer(buffer);
        buffer.Clear();
        context.Submit();
    }
}
```

上述代码解释：  
①首先 `CommandBuffer.BeginSample()` 和 `CommandBuffer.EndSample()` 方法是用于添加 Unity 的 performance profiling sample 命令的，也就是 Window -> Analysis 里的 Profiler 和 Frame Debugger 这两个分析工具。其实这两个方法以及之间的代码都可以不要了，后面的 Drawing 里会提到原因，使用老的废弃的 API `ScriptableRenderContext.DrawRenderers()`，`ScriptableRenderContext.DrawSkybox()` 等时才需要用到；  
②使用 `ScriptableRenderContext.ExecuteCommandBuffer(buffer)` API 提交 CommandBuffer 到 ScriptableRenderContext 的命令队列当中去。提交 CommandBuffer 后，CommandBuffer 并不会被清空，为了防止之后再提交 CommandBuffer 时产生重复的命令，需要将其清空，即 `CommandBuffer.Clear()`。最后使用 `ScriptableRenderContext.Submit()` API 让图形 API 去执行 ScriptableRenderContext 中的命令队列；  
③接下来就是 `CommandBuffer.ClearRenderTarget()` 了，它至少接受 3 个参数，一是是否清除深度缓冲 depth buffer 和模板缓冲 stencil buffer，二是是否清除颜色缓冲 color buffer，三是清除颜色缓冲使用的颜色，使用了 Color.clear。depth buffer 的默认替代值为 1.0，stencil buffer 的默认替代值为 0.0。但是为什么 `CommandBuffer.ClearRenderTarget()` 要放在 `CommandBuffer.BeginSample()` 上面，是因为 ClearRenderTarget() 方法自带了带有 CommandBuffer 名字的 profiling sample 命令，不放在上面会导致 Frame Debugger 里嵌套 profiling sample 命令；  
④如果不在 `CommandBuffer.ClearRenderTarget()` 之前设置好摄像机的属性的话，ClearRenderTarget() 会调用一个 Draw GL 的渲染事件（可以在 Frame Debugger 里看到），它会使用 Hidden/InternalClear shader 绘制一个全屏幕的矩形写入 RenderTarget，这并不是一个最有效率的做法。如果在清除渲染目标前设置相机属性，就可以在 Frame Debugger 中看到 Clear(Color + Depth + Stencil)，如下图所示：  

<div  align="center">  
<img src="https://s2.loli.net/2024/11/28/JraiSnjGtOxkQu3.png" width = "35%" height = "35%" alt="图2 - 上图：不设置摄像机属性；下图：正确的 Clear，在 ClearRenderTarget() 前设置摄像机属性"/>
</div>

同时 `ScriptableRenderContext.SetupCameraProperties(camera);` 的主要作用是设置摄像机的观察投影矩阵全局 shader 变量，即 unity_MatrixVP。

### Culling
为了知道哪些物体可以被裁切，我们需要知道摄像机的一些设置以及相关矩阵，这些数据可以用 `ScriptableCullingParameters` 结构体存储，并通过 `Camera.TryGetCullingParameters(out ScriptableCullingParameters)` 获取这些数据。然后我们使用 `ScriptableRenderContext.Cull(ref ScriptableCullingParameters)`（这里是引用参数的原因是为了防止形参复制导致的内存分配）返回 `CullingResults` 结构体，这个结构体包含了可见物体、光源以及反射探针的信息，并且在 Drawing 时会被使用。上述的代码如下（我写在了 SetupCameraProperties 前）：  

``` C#
if (camera.TryGetCullingParameters(out ScriptableCullingParameters cullingParameters))
{
    CullingResults cullingResults = context.Cull(ref cullingParameters);
}
else
{
    return;
}
                
context.SetupCameraProperties(camera);
```

`Camera.TryGetCullingParameters()` 方法会返回一个 bool 来判断摄像机是否会无效渲染，比如不正确的远近裁切平面等。如果返回 false，就直接退出 `Render()` 方法。这里也用到了一个小语法糖，即直接在方法调用时的参数列表中声明变量：out ScriptableCullingParameters cullingParameters。


### Drawing
在 catlikecoding 的教程中，使用的是老的 API `ScriptableRenderContext.DrawRenderers()`，`ScriptableRenderContext.DrawSkybox()`，`ScriptableRenderContext.DrawShadows()`。但是在调用这些方法前，需要调用 `ScriptableRenderContext.ExecuteCommandBuffer(buffer)`。比如 catlikecoding 中绘制 Skybox 的代码中：  

``` C#
// Clearing the render target
buffer.ClearRenderTarget(true, true, Color.clear);
buffer.BeginSample(bufferName);
context.ExecuteCommandBuffer(buffer);
buffer.Clear();

// Drawing
context.DrawSkybox(camera);
                
// Submit all scheduled commands
buffer.EndSample(bufferName);
context.ExecuteCommandBuffer(buffer);
buffer.Clear();
context.Submit();
```

如果删除 `buffer.BeginSample(bufferName);` 后的 `context.ExecuteCommandBuffer(buffer);` 以及 `buffer.Clear();`，就会先绘制 Skybox 再 ClearRenderTarget，因为我们提交 CommandBuffer 在绘制 Skybox 的后面。而且我们还需要把 `context.DrawSkybox(camera)` 包裹在 BeginSample 和 EndSample 之间，也没法把绘制 Skybox 放置在最后面。这就很蠢，不能集中提交 CommandBuffer 了，所以 Unity 废弃了这些 API（`DrawRenderers()`、`DrawSkybox()`、`DrawShadows()`），并使用 `CommandBuffer.DrawRendererList()` API 替代了这些方法。我下面使用的就是新的 API。

---

Drawing 的过程可以分为 4 个步骤：  
①创建并配置 `FilteringSettings` 结构体，它描述了如何过滤这些绘制指令接受的物体的相关设置，比如可以传递进  `RenderQueueRange.transparent` 或 `RenderQueueRange.opaque` 来只绘制不透明或透明物体；  
②创建并配置 `DrawingSettings` 结构体，它描述了怎么去排序可见的物体以及使用哪个 Shader Pass 去绘制，所以它的构造函数接受两个参数，一个是 `SortingSettings` 结构体，一个是 `ShaderTagId`（就是我们写的 Shader 里 Pass 的 LightMode 标签）；   
③创建并配置 `RendererListParams` 结构体，并传递进 `CullingResults`、`FilteringSettings`、`DrawingSettings`；  
④使用 `ScriptableRenderContext.CreateRendererList(ref RendererListParams)` 方法创建 RendererList，最后使用 `CommandBuffer.DrawRendererList()` 添加命令。

> 上述的结构体的属性设置内容并没有全部提到，强烈建议看看官方文档，很多设置之后肯定是要用到的。

``` C#
// Clear the render target
buffer.ClearRenderTarget(true, true, Color.clear);

// Draw Opaque
FilteringSettings filteringSettings = new FilteringSettings(RenderQueueRange.opaque);

SortingSettings sortingSettings = new SortingSettings(camera)
{
    criteria = SortingCriteria.CommonOpaque
};

ShaderTagId unlitShaderTagId = new ShaderTagId("SRPDefaultUnlit");

DrawingSettings drawingSettings = new DrawingSettings(unlitShaderTagId, sortingSettings);

RendererListParams rendererListParams =
    new RendererListParams(cullingResults, drawingSettings, filteringSettings);

RendererList opaqueRendererList = context.CreateRendererList(ref rendererListParams);
buffer.DrawRendererList(opaqueRendererList);

// Draw Skybox
RendererList skyboxRendererList = context.CreateSkyboxRendererList(camera);
buffer.DrawRendererList(skyboxRendererList);

// Draw Transparent
filteringSettings.renderQueueRange = RenderQueueRange.transparent;
sortingSettings.criteria = SortingCriteria.CommonTransparent;
drawingSettings.sortingSettings = sortingSettings;
rendererListParams.filteringSettings = filteringSettings;
rendererListParams.drawSettings = drawingSettings;

RendererList transparentRendererList = context.CreateRendererList(ref rendererListParams);
buffer.DrawRendererList(transparentRendererList);

// Submit all scheduled commands
context.ExecuteCommandBuffer(buffer);
buffer.Clear();
context.Submit();
```

上述代码一共绘制了不透明物体、Skybox 以及透明物体，注意透明物体一定要最后渲染，忘记原因了回去看《Unity Shader入门精要》的第七章透明效果，而先画不透明物体再画 Skybox 可以减少一些被遮挡的像素的重复绘制。  
①先从最简单的 Skybox 说起：Skybox 的 RendererList 的获取比较简单，不需要像代码的上面说的传递 RendererListParams 给 `CreateRendererList`，只需要传递进 camera 就行，即 `context.CreateSkyboxRendererList(camera)`，然后用 `DrawRendererList` 添加命令即可；  
②接下来就是绘制不透明物体了：首先是 `FilteringSettings`，可以使用构造函数加参数创建对象，也可以使用对象初始化语句，这两个方法的默认值并不一样，详见官方文档 [FilteringSettings](https://docs.unity3d.com/ScriptReference/Rendering.FilteringSettings-ctor.html) 。比较建议使用构造函数加参数，这样 layerMask、renderingLayerMask 就不会被设置为 0 了，同时传递进 RenderQueueRange.opaque，在渲染时过滤掉其他非不透明物体了；然后是 `SortingSettings`，它的构造函数只接受 camera，所以要使用对象初始化语句设置一些属性，比如 criteria，其规定了如何在渲染时为物体排序，建议为不透明物体使用 SortingCriteria.CommonOpaque，为透明物体使用 SortingCriteria.CommonTransparent；接下来是 `ShaderTagId`，这个就是 Pass 里的 LightMode，对于没设置 LightMode 的 Pass，SRP 仍然可以用 SRPDefaultUnlit 索引到这些 Pass；将 SortingSettings 和  ShaderTagId 传递给 `DrawingSettings`，以便 `RendererListParams` 使用。接下来就不说了，之前都提到过；   
③绘制透明物体和绘制不透明物体类似，修改一下 filteringSettings 和 sortingSettings 就行。

总之，Drawing 的目的就是根据我们的设置，用我们选择的 Pass 绘制物体至 RenderTarget。另外提一下，上述代码删除了 CommandBuffer.BeginSample 和 CommandBuffer.EndSample，是因为 CommandBuffer.DrawRendererList 跟 ClearRenderTarget 一样自带了带有 CommandBuffer 名字的 profiling sample 命令。我们不需要重复嵌套，上述代码的 Frame Debugger 如下图所示：

<div  align="center">  
<img src="https://s2.loli.net/2024/11/29/9wZmjEY3DBh2Rpu.png" width = "35%" height = "35%" alt="图3 - 绘制不透明物体、Skybox 以及透明物体"/>
</div>

## 编辑器渲染
### 绘制错误材质
目前为止，使用其他非 SRPDefaultUnlit 的 Pass 的物体不会被渲染，所以在场景中它们是不可见的。这就会让我们不知道场景中哪些物体使用了不受支持的 Shader。比如，如果想让 Unity 的 Built-in 的 Shader 都显示为错误的紫色，首先 Built-in 的 LightMode tag（shaders tag IDs）有 Always, ForwardBase, PrepassBase, Vertex, VertexLMRGBM, and VertexLM，如下：  

``` C#
static ShaderTagId[] legacyShaderTagIds = 
{
    new ShaderTagId("Always"),
    new ShaderTagId("ForwardBase"),
    new ShaderTagId("PrepassBase"),
    new ShaderTagId("Vertex"),
    new ShaderTagId("VertexLMRGBM"),
    new ShaderTagId("VertexLM")
};
```

接着还是老样子，绘制出这些 Pass：

``` C#
Material errorMaterial = new Material(Shader.Find("Hidden/InternalErrorShader"));

var drawingSettings = new DrawingSettings(legacyShaderTagIds[0], new SortingSettings(camera))
{
    overrideMaterial = errorMaterial
};

for (int i = 1; i < legacyShaderTagIds.Length; i++)
{
    drawingSettings.SetShaderPassName(i, legacyShaderTagIds[i]);
}

var filteringSettings = FilteringSettings.defaultValue;
var rendererListParams = new RendererListParams(cullingResults, drawingSettings, filteringSettings);
RendererList editorRendererList = context.CreateRendererList(ref rendererListParams);
buffer.DrawRendererList(editorRendererList);
```

这些代码可以放在绘制出所有可见物体的后面。我们通过 `DrawingSettings.SetShaderPassName()` 的 API 传递多个 shaders tag IDs，用以绘制多个 Pass。同时使用了 Hidden/InternalErrorShader 覆盖了这些物体的绘制，这样这些使用了 Built-in Shaders 的物体都会显示为紫色。最后，若只想将这些内容绘制在 Editor 中而不是在构建中，就将这些代码包裹在 `#if UNITY_EDITOR` 和 `#endif` 之间。

### 绘制 Gizmos
目前为止 Scene 和 Game 窗口中都看不到 Gizmos。为了绘制它们，我们首先可以通过 `UnityEditor.Handles.ShouldRenderGizmos()` 来确认我们是否在 Scene 和 Game 窗口的右上方开启了 Gizmos。然后通过 `ScriptableRenderContext.CreateGizmoRendererList()` 来创建 Gizmos 渲染列表，Gizmo 有两个子集可以绘制，一个是 PreImageEffects，一个是 PostImageEffects。目前我们还没支持屏幕后处理的效果，所以都先直接绘制了。

``` C#
#if UNITY_EDITOR
    if (Handles.ShouldRenderGizmos()) 
    {
        RendererList gizmosRendererList = context.CreateGizmoRendererList(camera, GizmoSubset.PreImageEffects);
        buffer.DrawRendererList(gizmosRendererList);
        gizmosRendererList = context.CreateGizmoRendererList(camera, GizmoSubset.PostImageEffects);
        buffer.DrawRendererList(gizmosRendererList);
    }
#endif
```

### 绘制 UI
我们在场景中创建一个 UI -> Button。此时 Canvas 的默认的 Render Mode 为 Screen Space - Overlay，在 frame debugger 中，UI 的渲染和我们自建的 RP 是分开的，如下图：  

<div  align="center">  
<img src="https://s2.loli.net/2024/12/02/8OfhYlv62s4Jgdt.png" width = "35%" height = "35%" alt="图4 - UI - Screen Space Overlay"/>
</div>

如果我们把 Render Mode 设置为 Screen Space - Camera，UI 渲染会成为自定义 RP 中透明物体渲染的一部分，如下：  

<div  align="center">  
<img src="https://s2.loli.net/2024/12/02/qgx9zMFd7tUPRIE.png" width = "35%" height = "35%" alt="图5 - UI - Screen Space Camera"/>
</div>

在 Game 窗口中 UI 是正常绘制的，但是在 Scene 中是看不到的。我们需要通过 `ScriptableRenderContext.EmitWorldGeometryForSceneView()` 静态方法将 UI 物体添加到世界的物体当中去，代码如下：  

``` C#
#if UNITY_EDITOR
    if (camera.cameraType == CameraType.SceneView) 
    {
        ScriptableRenderContext.EmitWorldGeometryForSceneView(camera);
    }
#endif

// Culling codes
```

同时添加 UI 物体的操作必须要在 culling 操作之前。现在 Scene 界面中就能正常绘制出 UI 物体了。

## 多相机渲染
每个相机都会有个 Depth 的属性，该属性决定了渲染的顺序（从低到高）。如果我们新建一个摄像机，将它的 Depth 设置为 0（Main Camera 的默认 Depth 为 -1），那么 Game 窗口就会被新摄像机的渲染所覆盖，并且场景渲染了两次，第一次渲染的结果被 clear 了，如下：  

<div  align="center">  
<img src="https://s2.loli.net/2024/12/02/tBXJcQ2Ko8VeZYF.png" width = "35%" height = "35%" alt="图6 - Two Cameras"/>
</div>

我们可以通过设置 buffer.name 让不同的摄像机渲染在 frame debugger 中有不同的范围：  

``` C#
#if UNITY_EDITOR
    buffer.name = camera.name;
#endif
```

代码写在 for 循环中的最前面，效果如下：

<div  align="center">  
<img src="https://s2.loli.net/2024/12/02/nWV68CA3ES7cD1Q.png" width = "35%" height = "35%" alt="图7 - Separate samples per camera"/>
</div>

### Layers
目前相机已经支持剔除特定的 Layer 的物体进行渲染了，我们可以把某些物体的 Layer 设置为 Ignore Raycast layer。然后在摄像机的 Occlusion Culling 属性中，只选择 Ignore Raycast，这样我们就只会看到设置为 Ignore Raycast 的物体了。

### Clear Flags
相机的 Clear Flags 属性仍然只有部分效果，如果我们把 Clear Flags 属性设置为不是 Skybox，再打开 frame debugger，会发现 Camera.RenderSkybox 不见了，但是 Solid Color 的 BackGround Color 仍然不起效，这是因为我们还没做好相关设置。首先我们要通过 `CameraClearFlags` 枚举获取到摄像机的 Clear Flags 属性：  

``` C#
...
context.SetupCameraProperties(camera);
CameraClearFlags flags = camera.clearFlags;
buffer.ClearRenderTarget(true, true, Color.clear);
...
```

`CameraClearFlags` 枚举有四个值：Skybox、SolidColor、Depth、Nothing，分别对应 0 到 3。为什么名字叫 ClearFlags 呢，是因为它本质是在设置 ClearRenderTarget 的属性，比如 Solid Color，就是在 ClearRenderTarget 时用设置的 BackGround 颜色替换掉 RenderTarget；而 Depth Only，指 ClearRenderTarget 时只清除掉 depth buffer；Don't clear 就是不清除 RenderTarget。

首先设置 CommandBuffer.ClearRenderTarget 的第一个参数，即是否清除 depth buffer。当 camera.clearFlags 小于 3 时，就清除：  

    buffer.ClearRenderTarget(flags < CameraClearFlags.Nothing, true, Color.clear);

接下来是第二个参数，即是否清除 color buffer。当 camera.clearFlags 小于 2 时，就清除：  

    buffer.ClearRenderTarget(flags < CameraClearFlags.Nothing, flags < CameraClearFlags.Depth, Color.clear);

最后替换颜色改为 `camera.backgroundColor.linear`：

    buffer.ClearRenderTarget(flags < CameraClearFlags.Nothing, flags < CameraClearFlags.Depth, camera.backgroundColor.linear);

第二个相机的 Clear Flags 属性本质上是用于决定第一个摄像机和第二个摄像机的混合模式，如果是 Skybox 和 Solid Color，则是完全替换掉上个相机的结果。当选择 Depth Only 时，第一个摄像机的结果则是第二个摄像机的渲染背景，并且第二个摄像机渲染的物体会全部覆盖到第一个摄像机的结果上面。当选择 Don't clear 时，第二个摄像机渲染的物体会和第一个摄像机的物体有正确的遮挡关系，但是透明物体除外，因为透明物体没有深度信息，就产生了渲染顺序上的冲突。

# Draw Calls
Catlike Coding 教程的这一章节主要是教我们如何写 Shader，如何支持 SRP batcher, GPU instancing, and dynamic batching 以减少 Draw Call，以及创建透明度混合或透明度测试的 shader。这里面大部分内容我在之前写的 [Unity URP 基础](https://ybniaobu.github.io/2024/02/23/2024-02-23-URP%E5%9F%BA%E7%A1%80/) 的文章中都有提到过，我就不再重复介绍了。我这里主要介绍一下，之前的文章中没提到过的，创建自己管线的 ShaderLibrary 的相关内容，以及如何在自己管线中开启 SRP batcher、GPU Instancing 以及 dynamic batching。

## ShaderLibrary
在 URP 中写 Shader 时一般都会直接引用 URP 资产中的 core 文件，即 Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl。在这个 core 文件中引入了很多 Core RP Library 的文件，比如 Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl 、Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl 等等。Common.hlsl 定义了很多用以支持不同平台的宏，SpaceTransforms.hlsl 则定义了很多空间变换函数。

所以在我们的自定义 RP 中，也需要有个类似的 core 文件，可以命名为 XXPipelineCore.hlsl，放在 ShaderLibrary 文件夹当中，这个文件的主要目的跟 URP 的 core 文件类似，引入了这个文件后，就可以在自定义 RP 中正常写 Shader，比如使用一些矩阵变换函数或者光源参数等等。为了后续写 Shader 方便，我们希望在 XXPass.hlsl 中只需要引入 XXPipelineCore.hlsl 就行了，而其他需要引入的宏、变量、函数等都可以在 XXPipelineCore.hlsl 文件中引入，比如我们接下来要创建的 UnityInput.hlsl。

UnityInput.hlsl 这个文件主要是用来创建由 Unity 引擎底层传递进来的数据的对应参数，比如一些变换矩阵、光照参数以及时间参数等等，在 URP 中也有一个同名的文件，我们可以参考参考。我们还没讲到光照，所以先定义一些矩阵（MVP 矩阵），如下：  

    CBUFFER_START(UnityPerDraw)
        float4x4 unity_ObjectToWorld;
        float4x4 unity_WorldToObject;
        float4 unity_LODFade;
        float4 unity_WorldTransformParams;

        float4x4 unity_MatrixPreviousM;
        float4x4 unity_MatrixPreviousMI;
    CBUFFER_END

    float4x4 glstate_matrix_projection;
    float4x4 unity_MatrixV;
    float4x4 unity_MatrixInvV;
    float4x4 unity_MatrixInvP;
    float4x4 unity_MatrixVP;
    float4x4 unity_MatrixInvVP;

我们先不管 CBUFFER_START(UnityPerDraw) 和 CBUFFER_END，这个在之后 SRP batcher 中说。我们得到了这些变换矩阵后，就可以写一些使用这些矩阵进行空间变换的函数了，当然 Core RP Library 中的 SpaceTransforms.hlsl 已经帮我们写好了（虽然里面的函数不全，日后我们可以自己补充，可以参考 URP 的 ShaderVariablesFunctions.hlsl），我们只需要在 XXPipelineCore.hlsl 引入这些文件就行了：  

    #include "UnityInput.hlsl"
    #include "Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl"

但是这样仍然不行，因为 SpaceTransforms.hlsl 没有假定 UnityInput.hlsl 中这些内置参数的存在，而是使用了类似于 UNITY_MATRIX_M 这样的宏，所以我们需要在 UnityInput.hlsl 中定义这些宏，如下：  

    #define UNITY_MATRIX_M          unity_ObjectToWorld
    #define UNITY_MATRIX_I_M        unity_WorldToObject
    #define UNITY_MATRIX_V          unity_MatrixV
    #define UNITY_MATRIX_I_V        unity_MatrixInvV
    #define UNITY_MATRIX_P          glstate_matrix_projection
    #define UNITY_MATRIX_I_P        unity_MatrixInvP
    #define UNITY_MATRIX_VP         unity_MatrixVP
    #define UNITY_MATRIX_I_VP       unity_MatrixInvVP
    #define UNITY_MATRIX_MV         mul(UNITY_MATRIX_V, UNITY_MATRIX_M)
    #define UNITY_MATRIX_T_MV       transpose(UNITY_MATRIX_MV)
    #define UNITY_MATRIX_IT_MV      transpose(mul(UNITY_MATRIX_I_M, UNITY_MATRIX_I_V))
    #define UNITY_MATRIX_MVP        mul(UNITY_MATRIX_VP, UNITY_MATRIX_M)
    #define UNITY_PREV_MATRIX_M     unity_MatrixPreviousM
    #define UNITY_PREV_MATRIX_I_M   unity_MatrixPreviousMI

这样就可以正常编译了。同时我们还需要在 XXPipelineCore.hlsl 中引入 Common.hlsl，因为这里面定义了很多基础的宏，比如 CBUFFER_START 和 CBUFFER_END 这两个宏就是在 Common.hlsl 引入的各图形 API 的 HLSL 文件中定义的。

    #include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
    #include "UnityInput.hlsl"
    #include "Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl"

## Batching
### SRP Batcher
如何让 shader 支持 SRP Batcher，在 [Unity URP 基础](https://ybniaobu.github.io/2024/02/23/2024-02-23-URP%E5%9F%BA%E7%A1%80/) 中已经介绍过了。这里说一下，之所以使用 `CBUFFER_START` 和 `CBUFFER_END` 这两个宏，而不直接使用 `cbuffer UnityPerMaterial {}` 或 `cbuffer UnityPerDraw {}`，是因为有些古早平台并不支持 cbuffer，而我们前面引入的 Common.hlsl 帮我们处理好了这件事情，并定义了 CBUFFER_START 和 CBUFFER_END 这两个宏以防止 cbuffer 代码在那些平台中不存在。

SRP Batcher 需要的那两个 CBuffer 这么取名的原因我认为是如下所述：UnityPerDraw 里面的参数是由 CPU 端每次绘制就重新设置一次的参数，但是在每次绘制期间保持常量。UnityPerMaterial 里面的参数则是在运行时不会变的材质属性常量。

> 我不能理解的是，UnityInput.hlsl 中定义的 VP 矩阵相关的参数为什么不放在 cbuffer 里，我看了看 URP 的 UnityInput.hlsl 文件中也确实没有放入 cbuffer，但是 Built-in 管线的 UnityCG.cginc 里放入了 CBUFFER_START(UnityPerFrame) 里，我不知道为什么 URP 里就不放入了。我目前这里还是参照了 URP 的做法。

为了让我们定义的管线支持 SRP batcher，我们需要在渲染管线实例中将 `GraphicsSettings.useScriptableRenderPipelineBatching` 设置为 true，我们在构造函数中设置：  

``` C#
public XXRenderPipeline()
{
    GraphicsSettings.useScriptableRenderPipelineBatching = true;
}
```

### GPU Instancing
如何让 shader 支持 GPU Instancing，在 [Unity URP 基础](https://ybniaobu.github.io/2024/02/23/2024-02-23-URP%E5%9F%BA%E7%A1%80/) 中也已经介绍过了。然而 GPU Instancing 作为大量物体绘制的一项技术，其原理大概是将这些静态的物件如植被等全部从场景中剔除，而保存其变换矩阵、纹理、颜色等相关信息，并一次绘制调用中渲染多个实例，每个实例具有不同保存的信息，但是具有相同的几何数据。虽然能通过使用较小的 gpu 消耗去解放大量的 cpu 性能，但是仍然解放得不够彻底，它降低了输入数据和 draw call 的性能消耗，但是没有解决巨量的剔除和输入数据等 cpu 消耗。

GPU Instancing 这门技术逐渐被 **GPU Driven Pipeline** 所代替，GPU Driven 管线本质是将剔除和构建 DrawCall 的流程从 CPU 搬到 GPU。我这里先简单介绍一下，日后建议详细了解。顺便提一下，Unity 中可以通过 `Graphics.DrawMeshInstancedIndirect` API 来使用 Compute Buffer 传递进来的 GPU 剔除的结果。

### Dynamic Batching
这门技术也过于古早了，动态合批甚至可能会负优化，所以不建议开启。想要开启它，需要把 GPU instancing、SRP batcher 都关了，因为它们的优先级都比动态合批高。首先在 DrawingSettings 里开启动态合批（默认是关闭的）、关闭 GPU instancing（默认是开启的）：  

``` C#
var drawingSettings = new DrawingSettings(unlitShaderTagId, sortingSettings) 
{
	enableDynamicBatching = true,
	enableInstancing = false
};
```

然后关闭 SRP batcher：

    GraphicsSettings.useScriptableRenderPipelineBatching = false;

Static Batching 跟 Dynamic Batching 类似，建议日后 GPU Driven Pipeline 时关闭，可以提高性能，在 Edit -> Project Setting -> Player -> Other Setting 里关闭。

### 配置 Batching
我们可以在 XXRenderPipelineAsset 里添加字段，方便我们配置 Batching，然后在 XXRenderPipeline 中添加管线资产字段，通过构造函数传递进来，如下：

``` C#
public class XXRenderPipelineAsset : RenderPipelineAsset
{
    public bool enableSRPBatcher = true;
    public bool enableGPUInstancing = true;

    protected override RenderPipeline CreatePipeline () 
    {
        return new XXRenderPipeline(this);
    }
}
```

``` C#
public class XXRenderPipeline : RenderPipeline
{
    private XXRenderPipelineAsset m_RenderPipelineAsset;
    
    public XXRenderPipeline(XXRenderPipelineAsset asset)
    {
        m_RenderPipelineAsset = asset;
    }
}
```

然后在 XXRenderPipeline 的构造函数中设置 SRP batcher，即：  

    GraphicsSettings.useScriptableRenderPipelineBatching = asset.enableSRPBatcher;

在 drawingSettings 设置 GPU Instancing：  

``` C#
var drawingSettings = new DrawingSettings(unlitShaderTagId, sortingSettings) 
{
	enableInstancing = m_RenderPipelineAsset.enableGPUInstancing
};
```


# Directional Lights
这个章节的内容就可以开始和 [Custom Better PBR in Unity](https://ybniaobu.github.io/2024/10/22/2024-10-22-BetterPBR1/) 的直接光部分结合在一起了。从这里开始，我会和我的 YPipeline 结合在一起边学边写，所以有些做法和写的函数会和 Catlike Coding 教程不太一样，教程中 BRDF 的内容我也会跳过。另外，本章内容只考虑**方向光 Directional Light**。

## Lit Shader
Lit Shader 之前 [Custom Better PBR in Unity](https://ybniaobu.github.io/2024/10/22/2024-10-22-BetterPBR1/) 都写好了，只是之前引入了很多 URP 的文件，需要改很多引入文件。因为我们接下来先只处理方向光 Directional Light，故需要将多光源、IBL 相关代码先注释掉。Pass 里面的 LightMode 也需要改为自己管线内设置的 ShaderTag，我的命名为了 YPipelineForward。然后在 drawingSettings 中添加这个 pass：  

``` C#
private ShaderTagId m_ForwardLitShaderTagId = new ShaderTagId("YPipelineForward");
...
{
    ...
    drawingSettings.SetShaderPassName(1, m_ForwardLitShaderTagId);
    ...
}
```

## Light Data
### Light Structure
目前为止，我们仍然没有将任何光源的参数传递给 Shader，所以还无法渲染。我们需要一个结构体来存储不同光源的参数，我在之前写的 Better PBR 时，写的一个名为 PunctualLightsLibrary.hlsl 的文件里面已经有了一个名为 LightParams 的结构体，只不过当时使用的是 URP 传递进来的光源参数：

    struct LightParams
    {
        float3 color;
        float4 positionWS;
        float3 L;
        float3 H;
        float distanceAttenuation;
        float angleAttenuation;
        float shadowAttenuation;
        uint layerMask;
    };

顺便提一嘴，Punctual Light 中包括 Directional light、Point light 和 Spot light。Directional light 平行光，没有位置，只有方向，所以它的 positionWS 参数里放置的是方向，和 L 是一致的；Point light 只有位置没有方向，故 positionWS 存储位置，与物体位置的差值为方向 L；Spot light 同时具有位置和方向。

### 向 GPU 传递光源数据
接下来就是从 CPU 传递光源数据到 GPU 并初始化这些光源的参数了。先讲 GPU 端，我新建了一个 YPipelineInput.hlsl 的文件，用以区分 YPipeline 传递进去的参数和 Unity 引擎底层传递进去的参数。我们先在其中定义几个平行光变量，并放在 cbuffer 里：  

    CBUFFER_START(PunctualLights)
        float4 _DirectionalLightColor;
        float4 _DirectionalLightDirection;
        uint _DirectionalLightLayerMask;
    CBUFFER_END

初始化 Light 结构体的函数之前也写过，把 URP 的变量改为我们自己的变量就可以了：  

    void InitializeDirectionalLightParams(out LightParams directionalLightParams, float3 V)
    {
        directionalLightParams.color = _DirectionalLightColor.rgb;
        directionalLightParams.positionWS = _DirectionalLightDirection; //Directional Light has no position
        directionalLightParams.L = normalize(directionalLightParams.positionWS.xyz);
        directionalLightParams.H = normalize(directionalLightParams.L + V);
        directionalLightParams.distanceAttenuation = 1.0;
        directionalLightParams.angleAttenuation = 1.0;
        directionalLightParams.shadowAttenuation = 1.0;
        directionalLightParams.layerMask = _DirectionalLightLayerMask;
    }

平行光没有距离和角度衰减，阴影先暂时设置为 1.0，以后讲阴影时再处理。然后就是 CPU 端了，我新建了一个 LightingNode 的类，在里面使用 `CommandBuffer.SetGlobalVector()` 传递光源数据：  

``` C#
public class LightingNode : PipelineNode
{
    private int m_DirectionalLightColorId;
    private int m_DirectionalLightDirectionId;

    public LightingNode() : base()
    {
        m_DirectionalLightColorId = Shader.PropertyToID("_DirectionalLightColor");
        m_DirectionalLightDirectionId = Shader.PropertyToID("_DirectionalLightDirection");
    }
    
    private void SetupDirectionalLight(ref PipelineData data)
    {
        Light mainLight = RenderSettings.sun;
        data.buffer.SetGlobalVector(m_DirectionalLightColorId, mainLight.color.linear * mainLight.intensity);
        data.buffer.SetGlobalVector(m_DirectionalLightDirectionId, -mainLight.transform.forward);
    }
}
```

上述代码中，场景中的主光源通过 `RenderSettings.sun` 获取，这个 RenderSettings 就是 Window -> Rendering -> Lighting -> Environment 里面的相关设置。其中 Sun Source 就是 RenderSettings.sun，如果我们没在 Lighting 里设置的话，它就会选取场景中最亮的平行光。同时注意，shader 里的光照方向应该是指向光源的，所以要在 mainLight.transform.forward 前面加个负号。还有就是 `commandBuffer.SetGlobalVector()` 这个 API 只接受 Vector4，即使传递 Vector3 也会隐式转换为 Vector4。

然后在 Render() 函数中调用 LightingNode 就可以渲染了。图片就不放出了。

### 多个平行光
当进行剔除时，Unity 也会计算哪些光源会影响相机可视的空间。所以我们可以利用剔除信息，让 LightingNode 获取剔除信息中的  visibleLights 以便支持多个平行光：  

``` C#
NativeArray<VisibleLight> visibleLights = data.cullingResults.visibleLights;
```

因为我们要传递多个平行光的数据，所以要把之前光源的颜色、方向参数都改为放在一个 Vector4 数组里，同时设定支持最大平行光源数为 4：  

``` C#
private const int m_MaxDirLightCount = 4;
        
private int m_DirLightCountId;
private int m_DirLightColorsId;
private int m_DirLightDirectionsId;

private Vector4[] m_DirLightColors;
private Vector4[] m_DirLightDirections;

public LightingNode() : base()
{
    m_DirLightCountId = Shader.PropertyToID("_DirectionalLightCount");
    m_DirLightColorsId = Shader.PropertyToID("_DirectionalLightColors");
    m_DirLightDirectionsId = Shader.PropertyToID("_DirectionalLightDirections");
    
    m_DirLightColors = new Vector4[m_MaxDirLightCount];
    m_DirLightDirections = new Vector4[m_MaxDirLightCount];
}
```

接下来就是用一个循环遍历所有的 visibleLights 设置颜色、方向数组的值，并传递给 GPU：

``` C#
private void SetupDirectionalLights(ref PipelineData data)
{
    NativeArray<VisibleLight> visibleLights = data.cullingResults.visibleLights;

    int dirLightCount = 0;
    for (int i = 0; i < visibleLights.Length; i++)
    {
        VisibleLight visibleLight = visibleLights[i];
        if (visibleLight.lightType == LightType.Directional)
        {
            m_DirLightColors[dirLightCount] = visibleLight.finalColor;
            m_DirLightDirections[dirLightCount] = -visibleLight.localToWorldMatrix.GetColumn(2);
            dirLightCount++;
            if (dirLightCount >= m_MaxDirLightCount) break;
        }
    }
    
    data.buffer.SetGlobalInt(m_DirLightCountId, dirLightCount);
    data.buffer.SetGlobalVectorArray(m_DirLightColorsId, m_DirLightColors);
    data.buffer.SetGlobalVectorArray(m_DirLightDirectionsId, m_DirLightDirections);
}
```

`dirLightCount` 记录 visibleLights 中平行光的数量，通过 if 语句判断如果 visibleLights 是 `LightType.Directional` 就将当前的平行光的颜色、方向信息添加到数组中。并且如果 dirLightCount 大于了最大支持的平行光数量就退出循环，dirLightCount 最大为 4。

另外要注意，我们是通过 `visibleLight.finalColor` 属性来获取到光源的颜色的，finalColor 属性是颜色乘以强度，但是颜色默认都是 sRGB 颜色，我们需要在 YRenderPipeline 的构造函数中设置 `GraphicsSettings.lightsUseLinearIntensity` 来将其转至线性空间：  

``` C#
public YRenderPipeline(YRenderPipelineAsset asset)
{
    ...
    GraphicsSettings.useScriptableRenderPipelineBatching = asset.enableSRPBatcher;
    GraphicsSettings.lightsUseLinearIntensity = true;
}
```

接下来就是修改 shader 的 YPipelineInput.hlsl 中光源相关参数了，让 CPU 和 GPU 数据格式相互匹配：

    #define MAX_DIRECTIONAL_LIGHT_COUNT 4

    CBUFFER_START(PunctualLights)
        float4 _DirectionalLightColors[MAX_DIRECTIONAL_LIGHT_COUNT];
        float4 _DirectionalLightDirections[MAX_DIRECTIONAL_LIGHT_COUNT];
        int _DirectionalLightCount;
    CBUFFER_END

并修改 PunctualLightsLibrary.hlsl 中 `InitializeDirectionalLightParams` 函数：  

    void InitializeDirectionalLightParams(out LightParams directionalLightParams, int lightIndex, float3 V)
    {
        directionalLightParams.color = _DirectionalLightColors[lightIndex].rgb;
        directionalLightParams.positionWS = _DirectionalLightDirections[lightIndex]; //Directional Light has no position
        directionalLightParams.L = normalize(directionalLightParams.positionWS.xyz);
        directionalLightParams.H = normalize(directionalLightParams.L + V);
        directionalLightParams.distanceAttenuation = 1.0;
        directionalLightParams.angleAttenuation = 1.0;
        directionalLightParams.shadowAttenuation = 1.0;
    }

然后在 PBR 的 Shader 里利用循环计算多个平行光源：  

    for (int i = 0; i < _DirectionalLightCount; i++)
    {
        LightParams dirLightParams = (LightParams) 0;
        InitializeDirectionalLightParams(dirLightParams, i, standardPBRParams.V);

        BRDFParams dirBRDFParams = (BRDFParams) 0;
        InitializeBRDFParams(dirBRDFParams, standardPBRParams.N, dirLightParams.L, standardPBRParams.V, dirLightParams.H);

        renderingEquationContent.directPunctualLights += CalculatePunctualLight(dirLightParams) * StandardPBR_EnergyCompensation(dirBRDFParams, standardPBRParams, energyCompensation);
    }

这样子就可以支持 4 个平行光了，可以在场景中添加 4 个不同方向的平行光看看，图片就不放出了。实际上游戏一般用不到 4 个平行光，支持太阳、月亮两个平行光基本就足够了（有时会同时出现在天空中），这个就视自己的项目去决定了。

## Transparency
对于透明物体来说，漫反射部分会透过物体，但是镜面反射部分仍然存在，这样子按之前的透明度混合的 SrcAlpha 和 OneMinusSrcAlpha 进行混合会导致镜面反射部分也会被变淡（透明化）。所以解决方案就是把透明度混合的 SrcFactor 改为 1，DstFactor 还是 OneMinusSrcAlpha，然后再在 Shader 的代码里自行将 Alpha 乘在 Diffuse Color 里，这样就只有漫反射部分会变透明了，保留了镜面反射部分。

这部分代码很简单就不放出来。玻璃材质还有个折射的现象要考虑，建议以后写玻璃材质的时候统一在一起写。

## 自定义 Shader Inspector
Catlike Coding 的这部分讲得不深，我也就简单摘录一下，知道有个可以自定义 Shader Inspector 的途径就行了。要更深入自定义 ShaderGUI 的内容，建议以后可以专门花时间找其他文章资料学习，毕竟本质上这些内容和制作游戏本身关系不大，更多是为了 Inspector 美观上的需求。同时，URP 的 [Editor / ShaderGUI](https://github.com/Unity-Technologies/Graphics/tree/master/Packages/com.unity.render-pipelines.universal/Editor/ShaderGUI) 文件夹内的代码是不错的学习资料。

### 自定义 ShaderGUI 类
我们可以在 Shader 的 ShaderLab 代码的底部添加一个 `CustomEditor` 语句：  

    Shader "YPipeline/Unlit"
    {
        ...
        CustomEditor "YPipeline.Editor.UnlitShaderGUI"
    }

这样可以指示 Unity 编辑器使用 UnlitShaderGUI 类实例来绘制使用了 Unlit 着色器的材质的 inspector。这个类我创建在了 YPipeline / Editor / ShaderGUI 文件夹里，该类的基础代码如下，需要继承 `ShaderGUI` 类，并覆写 `OnGUI()` 方法，并传递进 `MaterialEditor` 和 `MaterialProperty` 数组：  

``` C#
using UnityEditor;
using UnityEngine;
using UnityEngine.Rendering;

namespace YPipeline.Editor
{
    public class UnlitShaderGUI : ShaderGUI 
    {
        public override void OnGUI(MaterialEditor materialEditor, MaterialProperty[] properties)
        {
            base.OnGUI(materialEditor, properties);
        }
    }
}
```

### 设置属性
我们将常用的三个变量存储到字段当中，第一个字段是材质编辑器 MaterialEditor 对象，就是我们点击一个材质所显示的 Inspector。第二个字段是正在被编辑的材质们的引用，比如我们连续选择使用同一个 Shader 的材质，它们会显示同一个 Inspector，那么这些材质可以通过 `MaterialEditor.targets` 获取到，它返回我们点击的材质的数组（Object 数组）。第三个字段就是材质属性数组：  

``` C#
private MaterialEditor m_MaterialEditor;
private Object[] m_Materials;
private MaterialProperty[] m_Properties;

public override void OnGUI(MaterialEditor materialEditor, MaterialProperty[] properties) 
{
	base.OnGUI(materialEditor, properties);
	m_MaterialEditor = materialEditor;
	m_Materials = materialEditor.targets;
	m_Properties = properties;
}
```

获取到材质属性 MaterialProperty 可以使用 `ShaderGUI.FindPropery()` 方法，建议查看官方文档。而有些属性则需要通过 Material 类获取，比如材质的 renderQueue 通过 `Material.renderQueue` 属性获取以及改变，又或者让选定的材质激活特定的 Shader Keyword 则通过 `Material.EnableKeyword()` 方法实现。

### 按钮
可以使用 `GUILayout.Button()` 方法来创建一个按钮，可以为它传一个字符串，作为按钮的名称。如果这个方法返回 true，代表它被按下了。在应用按钮之前，我们可以向编辑器注册一个撤销 Undo 行为（Ctrl + Z），可以通过调用 `MaterialEditor.RegisterPropertyChangeUndo()` 方法并传入来实现：  

``` C#
private bool PresetButton(string name) 
{
    if (GUILayout.Button(name)) 
    {
        editor.RegisterPropertyChangeUndo(name);
        return true;
    }
    return false;
}
```

我们可以创建一个方法，让按下按钮时设置一些材质属性，我这里就不放出代码了。之后在 `OnGUI()` 函数入口中调用按钮相关方法就行了。另外，`EditorGUILayout` 类的静态方法可以实现一些特殊的 GUI 布局效果，比如折叠效果 foldout 等等，这里我也不详细展开了。