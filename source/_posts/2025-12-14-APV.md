---
title: Custom SRP 接入 Unity APV
date: 2025-12-14 19:52:47
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2025/12/14/nqFkiKMduaWEc13.jpg
description: 本笔记的主要内容有 Irradiance Volume 技术的简单介绍；Unity APV 的大致实现细节以及使用教程；如何在自定义的管线中实现 APV。
---

> 在 YPipeline 中实现了 SSAO 相关算法之后，本想直接开始实现 SSGI（已经鸽了无数次了，笑）。然而，由于屏幕空间光照信息的缺失，屏幕空间的全局光照算法的效果较不稳定。所以最好的方式是将 SSGI 和其他基于世界空间的预烘焙全局光照技术相互结合，SSGI 来补充预烘焙光照缺失的高频信息，这样的 Hybrid Diffuse Global Illumination 效果更好。而之前 YPipeline 只实现了 Unity 内置的 Light Map 和 Light Probe Groups，然而这两个技术都是 Per Object 的，无法直接作为 SSGI 的 Fallback，为此我打算先接入 Per Pixel 的**自适应探针体积 Adaptive Probe Volumes (APV)**，当然 Reflection Probe 作为 SSGI fallback 也是可以的，但是 YPipeline 目前也只支持了 Per Object 的 Reflection Probe，也暂时无法使用，并且 Reflection Probe 作为 fallback 会有跳变的缺点，故我选择先了解和实现 APV。实现 Per Pixel 的 Reflection Probe 我打算在后面实现镜面反射全局光照算法时（比如 SSR 等）一同解决。  
>   
> 本篇文章使用的 Unity 版本为 6000.3.1f1，SRP Core 版本为 17.3.0。

# APV 介绍
Unity APV 全名 **Adaptive Probe Volumes**，本质上属于 **Irradiance Volume** 技术。故这里先简单介绍一下 Irradiance Volume，参考的资料有如下：  

> **① Shirley, Peter. “The Irradiance Volume.” IEEE Computer Graphics and Applications, 1998.** ：https://www.sci.utah.edu/~bigler/images/msthesis/The%20irradiance%20volume.pdf ；  
> **② Irradiance Volumes for Games（GDC 2005）**：https://www.chrisoat.com/papers/Oat_GDC2005_IrradianceVolumesForGames.pdf ；  
> ③ Volumetric Global Illumination At Treyarch（SIGGRAPH 2016）：https://www.activision.com/cdn/research/Volumetric_Global_Illumination_at_Treyarch.pdf ；  
> ④ Multi-Scale Global Illumination in Quantum Break (SIGGRAPH 2015)：https://advances.realtimerendering.com/s2015/SIGGRAPH_2015_Remedy_Notes.pdf 。
> 因为 Irradiance Volume 即使时至今日都可以说是主流的全局光照技术，相关的演讲或文章是比较多的，SIGGRAPH、GDC 中关于 Global Illumination 的演讲多多少少都会提到 Irradiance Volume，有兴趣可以自己去查阅，但核心都是基于第一篇文章。

## Irradiance Volume 简单介绍
Irradiance Volume 也是基于 Light Probe 的技术，只不过使用 3D 纹理来存储数据，由此可以在 GPU 中逐像素地获取 Irradiance，而传统 Light Probe 只能逐物体地在 CPU 中插值。除此之外，还有烘焙速度快，自动化 Probes 摆放，更好地支持 Geometry LOD 以及可以让体积雾应用烘焙光照等优点。

根据论文（参考资料 ①），Irradiance Volume 实现的基本步骤如下：  
①将场景的包围盒分为多个统一大小的**网格 Grids**，称为 **first-level grid**，然后对于包含几何体的**网格单元 Cells** 进行再次划分，新划分出来的区域称为 **second-level grid**，这个方法其实就是**稀疏体素八叉树 Sparse Voxel Octree, SVO**。然后在网格的顶点位置放置光照探针，用于烘焙 irradiance 信息，如下图：  

<div align="center">  
<img src="https://s2.loli.net/2025/12/16/KycNjF94BhsImEg.png" width = "60%" height = "60%" alt="图1 - 使用双层网格划分区域，中图展示了 first-level grid，右图为 second-level grid。"/>
</div>

②然后就是在每个光照探针处向一定数量的方向采样 radiance，并计算存储 irradiance distribution function。论文因为比较早，用 uv 贴图的方式存储 irradiance，当时球谐光照应该还不成熟（球谐光照论文的公开是在 2003 年的，Precomputed Radiance Transfer 的公开在 SIGGRAPH 2001-2002）。现在最常用的存储 illumination/irradiance 的方式就是球谐函数，只不过在 Light Probe 中，我们常常使用三阶球谐系数来表示光照。在 Irridiance Volume 中，因为内存空间有限，我们一般会使用二阶球谐或者压缩的二阶球谐来存储光照信息。  
③查询 Irradiance Volume：Irridiance Volume 中的数据结构可分为 samples, cells, grids。Sample 就是存储 irradiance 的探针，Cell 就是包含 8 个探针的 box，Grid 则是三维 cell 序列。首先在 first-level grid 查找哪个 cell 包含了采样点，若 cell 中包含了 grid，则在 second-level grid 查找哪个 cell 包含了采样点，最后根据 cell 上的 samples 三线性插值出采样点的 irradiance。

## APV 的大致实现细节
Unity APV 正是基于 Irradiance Volume 开发的，在 2022 年以实验性包的形式出现，并推出了名为 Enemies – real-time cinematic teaser 的 demo 展示了 APV 的效果，以及在 SIGGRAPH 2022 的演讲 [Probe-based lighting, strand-based hair system, and physical hair shading in Unity’s ‘Enemies’](https://advances.realtimerendering.com/s2022/SIGGRAPH2022-Advances-Enemies-Ciardi%20et%20al.pdf) 中介绍了 APV 技术，最后于 Unity 6 版本正式推出。下面就根据演讲的 PPT 大致说明一下 APV 中的一些基本概念或技术实现细节。 

### APV 数据结构
①自适应摆放：靠近 geometry 的 probe 密度会更高，艺术家可以自己摆放额外的 probe volumes（local volume）作为应该摆放 probe 区域的标记，用于改变 probe 的稀疏，但是 volume 的所有数据都是存储在一个全局结构里的；  
②Bricks and Cells：probe 会被放置在称为 **Brick** 的基本单位当中，每个 Brick 最小包含 4 × 4 × 4 个 probes。注意这个 Brick 应该是 Irradiance Volume 中的概念 first-level grid 和 second-level grid 的合体，也就是 Grid，但是划分层级会更多。我实际测试下来，Brick 会根据设置的 probe 最小最大摆放距离被细分为 3 到 6 个层级，源码中我看到最大细分层级为 7。Brick 的具体划分是通过 SDF 实现的，靠近 geometry 时会被更精细地划分。然后 Brick 中的数据会被打包进名为 **Cell** 的单位，而 cell 是可流式加载的基本单元，数据会通过少量 CPU 暂存内存从硬盘加载至 GPU。

<div align="center">  
<img src="https://s2.loli.net/2025/12/18/9HF6Wd3Mz1XrBYm.png" width = "70%" height = "70%" alt="图2 - 左：Bricks，右：Cells"/>
</div>

③数据结构被分为了两部分：  
&emsp;&emsp; - **球谐池**：球谐纹理池是一组固定大小的 3D 纹理，纹理深度一定是 4（最小 Brick 的深度）。池的限制是每个 brick 中的数据必须是连续的，以便进行过滤和索引操作，但其他数据可以是相互脱节的，比如 Brick 存放的纹理位置。同时 SH 的数据会被压缩，压缩方式参考了 [Precomputed Global Illumination in Frostbite (GDC 2018)](https://media.contentapi.ea.com/content/dam/eacom/frostbite/files/gdc2018-precomputedgiobalilluminationinfrostbite.pdf) ，L0 用更高的精度存储，L1/L2 用基于 L0 的函数表达，因此可以以更低精度存储。  
&emsp;&emsp; - **An indirection buffer**：包括一个 global cell indirection structure 和 per-cell brick indirection structure，这个 indirection buffer 的主要目的是从世界坐标索引至球谐池。  
&emsp;&emsp;&emsp;&emsp; 1. **Global Cell Indirection Structure**：这个数据结构用于存储每个 cell 的信息，每个 cell 都拥有一个条目，包括 cell 包含的 brick 的最高细分级别、cell 中所有 brick 的 AABB 以及指向 per-cell brick indirection 的索引。根据源码，我猜测就是名为 `_APVResCellIndices` 的 Structured Buffer，C# 代码在 SRP core -> Runtime -> Lighting -> ProbeVolume -> ProbeIndexOfIndices.cs 里；  
&emsp;&emsp;&emsp;&emsp; 2. **Per-Cell Brick Indirection structure**：这个数据结构用于存储每个 cell 中的 brick 的信息，每一个潜在的 brick 都拥有一个条目。比如，假设一个 cell 的三个维度都是 81 米，在 cell 内最小的 brick 是 3 米，那么这个 cell 就拥有 27 × 27 × 27 个条目，每个条目都存储了它在球谐纹理的位置坐标，以及该 brick 所处的细分级别 Subdivision Level。同时 Buffer 中 brick 的顺序是根据它相对于 cell 中心点的位置来决定的。根据源码，我猜测就是名为 `_APVResIndex` 的 Structured Buffer，C# 代码在 ProbeBrickIndex.cs 里；  

> 不得不吐槽 Unity 代码的命名 ε=( o｀ω′)ノ。  

④运行时采样：GPU 中计算索引的代码如下（这部分代码是在 PPT 里的，和目前版本的 SRP Core 里的有较大不同，但大致逻辑是相同的）  

<div align="center">  
<img src="https://s2.loli.net/2025/12/19/4rSgAoLUHQuI92B.png" width = "65%" height = "65%" alt="图3 - GPU Runtime Sampling"/>
</div>

代码 1、2 就是通过世界坐标可以计算出采样点位于哪个 cell 当中，由此可以获得其在 Global Cell Indirection Structure 中正确的索引，从而找到对应的 Per-Cell Brick Indirection structure。代码 3 就是计算采样点在 Brick 中的索引，从而获取其在 per cell indirection buffer 中的数据，即球谐纹理坐标（brickUVM）。代码 4 就是采样球谐数据。

### 解决 Artifact
①**Borders of Levels**：这个 artifact 的来源就是相邻的采样点周围的 probe 位于不同的细分层级，这会导致接缝现象，图片我就不展示了，解决方案就是增加 noise；  
②**Probes Within Geometry**：probe 可能会被摆放到几何体内部，Unity 会根据烘焙时 probe 可以看到多少 backface 来计算一个有效性分数，根据分数阈值（Lighting -> Probe Invalidity Settings -> Validity Threshold）来判断 Probe 是否是有效的。这个 artifact 的解决方案有两个，就是 Probe Invalidity Settings 中的两个，一为 **扩大 Dilation**，二为 **虚拟偏移 Virtual Offset**：  
&emsp;&emsp; - **Dilation**：就是在烘焙期间，无效的 probe 从其周围有效的数个 probe 中获取数据，并使用距离平方的倒数作为权重。这个方法的缺点是会加剧漏光 Leak 现象，因为无效的 probe 很可能会从室内扩大到室外场景。  
&emsp;&emsp; - **Virtual Offset**：就是烘焙期间，将几何体内部 probe 推出几何体。这个方法的缺点是推出的方向不可控，很有可能推到一个不好的位置，导致新的漏光现象，所以 Unity 建议 Dilation 和 Virtual Offset 一起使用。  
③**Light Leak**：漏光现象，Unity 解决漏光现象的方案都是运行时的方案，有如下：  
&emsp;&emsp; - **Biases**：跟 Shadow Map 类似，就是将采样点往法线或视角方向推，即 **Normal Bias** 和 **View Bias**，Unity 中在后处理 volume 中的 Adaptive Probe Volumes Options 就有这两个数据可以调整；  
&emsp;&emsp; - **Warping of Trilinear Sample Location**：就是通过改变三线性插值的位置来改变权重的大小，就是 Adaptive Probe Volumes Options 中的 **Leak Reduction Mode**，建议使用 Quality Mode，效果比较好。具体改变权重的方法是根据 probe 是否有效来计算的，即 Validity Based Weighting。

除了上述方法外，Unity 还提供了一个艺术家可以控制的 volume（PPT 里称为 Touch-up Volume），就是 Light -> **Probe Adjustment Volume**。Probe Adjustment Volume 可以覆盖指定区域的一些设置，比如无效化该区域的 probe，让更细粒度的控制成为了可能。

## 如何使用 APV
### 烘焙流程
这里以 URP 为基础，大致阐述一下如何进行 Irradiance Volume 烘焙。Irradiance Volume 烘焙本质上也使用了 Progressive Lightmapper 系统，流程跟 Lightmap 烘焙是基本一样的：  
①首先要在 URP Asset 里面将 **Light Probe System 改为 Adaptive Probe Volumes**；  
②在场景中**添加 Adaptive Probe Volume** 物体（在 Light 里），Mode 有三个，测试的话随便选一个就行。Global Mode 的意思就是 Volume 会包含所有 Baking Set 场景中的 Contribute Global Illumination 的 renderers；Scene Mode 就是包含当前场景中的 renderers；Local Mode 就是自己设置 volume 大小，当然 Local Mode 也可以用于调整不同区域的 probe 的密度；  
③将需要烘焙的光源设置为 **Mixed 或 Baked** 模式，并将场景中的物体的 MeshRenderer 组件中勾选上 **Contribute Global Illumination** 选项，并将 Receive Global Illumination 改为 **Light Probes**；  
④打开 Window -> Rendering -> Lighting 激活 **Baked Global Illumination**，然后 Generate Lighting 就可以烘焙了。注意，Lightmapping Settings 里的参数是会对烘焙效果产生影响的，建议将值都拉高。

我在 URP 建立了一个 ‌Cornell Box 场景，烘焙效果如下（Dilation 和 Virtual Offset 都开了，然后 Leak Reduction Mode 是 Quality，Sampling Noise 拉到了最大，只使用了 Normal Bias）：  

<div align="center">  
<img src="https://s2.loli.net/2025/12/19/NGf1hynRKUB7iv4.png" width = "50%" height = "50%" alt="图4 - URP 中 APV 测试效果"/>
</div>

我感觉效果还是挺好的，虽然有些许光照瑕疵的，可以尝试使用 Probe Adjustment Volume 调整，我这里就不尝试了。

### 光照变换
因为 Irradiance Volume 本质上是静态的，所以实现运行时改变光照或者 time of day 需要额外的支持。对此 Unity 提供了两个不同的方案，一是**光照场景 Lighting Scenarios**，二是**天空遮蔽 Sky Occlusion**。Lighting Scenarios 和 Sky Occlusion 也可以搭配使用，Lighting Scenarios 更新太阳的位置，Sky Occlusion 更新天空光。

#### Lighting Scenarios
光照场景 Lighting Scenarios 顾名思义就是烘焙多个不同光照下的场景，并通过插值系数在不同的场景之间进行插值，从而实现全局光照变换的“假象”。具体流程如下：  
①激活 Lighting Scenarios：在 URP Asset 文件里开启 Lighting -> Light Probe System -> Enable Lighting Scenarios，开启后 Enable Lighting Scenario Blending 应该会出现并自动开启，若未开启点击开启；  
②烘焙 Lighting Scenarios：在 Window -> Rendering -> Lighting -> Adaptive Probe Volumes -> Lighting Scenarios 添加多个场景，并烘焙；  
③在运行时使用脚本在不同光照场景之间进行插值，代码如下：  

``` C#
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class BlendLightingScenarios : MonoBehaviour
{
    UnityEngine.Rendering.ProbeReferenceVolume probeRefVolume;
    public string scenario01 = "Scenario01Name";
    public string scenario02 = "Scenario02Name";
    [Range(0, 1)] public float blendingFactor = 0.5f;
    [Min(1)] public int numberOfCellsBlendedPerFrame = 10;

    void Start()
    {
        probeRefVolume = UnityEngine.Rendering.ProbeReferenceVolume.instance;
        probeRefVolume.lightingScenario = scenario01;
        probeRefVolume.numberOfCellsBlendedPerFrame = numberOfCellsBlendedPerFrame;
    }

    void Update()
    {
        probeRefVolume.BlendLightingScenario(scenario02, blendingFactor);
    }
}
```

上述代码中 `numberOfCellsBlendedPerFrame` 是用于控制每帧插值的 cell 的个数的，有优化需求可以调整这个参数。不同光照场景的混合是通过 compute shader 计算的（应该就是 SRP Core -> Runtime -> Lighting -> ProbeVolume 里的 ProbeVolumeBlendStates.compute），用 Unity 的 Frame Debugger 是看不到的，用 RenderDoc 可以看到，在 Compile/ExecuteRenderGraph 之前，有个叫做 APVScenarioBlendingUpdate 的阶段，它混合了两个场景的球谐数据 `_APVResL0_L1RX`, `_APVResL1B_L1Rz`，`_APVResL1G_L1Ry` 以便后续光照计算使用。我使用了经典的 Cryteck's Sponza 场景，光照场景混合的效果大致如下（因为我的免费图床只支持 5 mb 大小，所以 gif 分辨率很低，而且还进行了压缩，所以看起来会有点糊/(ㄒoㄒ)/~~，但大致效果还是可以看到的）：  

<div align="center">  
<img src="https://s2.loli.net/2025/12/23/u71mtPe4vhifz3R.gif" width = "60%" height = "60%" alt="图5 - Lighting Scenarios Blending 效果"/>
</div>

除此之外，我们也可以在 Window -> Analysis -> Rendering Debugger 中预览光照场景的混合，打开后设置 Scenario Blend Target 和 Scenario Blending Factor 就可以预览了。Lighting Scenarios 的使用还有一点需要注意，就是不同的光照场景下 Light Probe 的数量与位置必须是一致的，为了防止移动静态物体时造成的 Light Probe 的数量或位置变换，我们在烘焙时，需要在 Probe Placement 里将 **Probe Positions** 设置为 **Don’t Recalculate** 再进行烘焙。

#### Sky Occlusion
Sky Occlusion 就是烘焙 APV 时，额外烘焙出一个静态的值，存储在 `_SkyOcclusionTexL0L1` 的球谐 3d 纹理当中，这个球谐值就是只采样天空盒产生的间接光。然后在运行时，APV 会通过两个值来估计天空光，一是来源自天空盒的天空颜色（也是球谐），二就是烘焙出的 Sky Occlusion 值，具体代码我会在下面讲代码时提到，看代码理解会更深一点，其实本身实现也非常简单。激活 Sky Occlusion 烘焙在 Window -> Rendering -> Lighting -> Adaptive Probe Volumes -> Sky Occlusion Settings 里，勾选 Sky Occlusion 即可，然后 Generate Lighting。

烘焙完 Sky Occlusion 后，改变 Window -> Rendering -> Lighting -> Environment 里的 Environment Lighting 的 Intensity Multiplier 值，或者将 Source 设置为 Gradient 或 Color 后改变颜色，就可以看到 Sky Occlusion 产生的光照变化了。当 Source 为 Skybox 时，也可以通过 `DynamicGI.UpdateEnvironment` API 来在运行时更新天空盒数据（这个 API 我猜测就是 Unity 在运行时计算新的球谐系数），或者自己每几帧计算球谐系数直接上传到 `RenderSettings.ambientProbe` 里。`RenderSettings.ambientProbe` 就是 Unity Generate Lighting 时，计算存储并上传天空盒的球谐光照的地方。你也可以修改 APV 的 Shader 代码来让其使用自己管线设置的球谐系数，而非 Unity Built-in 的球谐系数，后面讲代码时，也会大致提及。

Sky Occlusion Settings 里还有一个叫做 Sky Direction 的选项可以勾选，勾选上后，Unity 会额外烘焙一个方向信息，即 Sky Direction，就是名为 `_SkyPrecomputedDirections` 的 Structured Buffer，在采样时通过 `_SkyShadingDirectionIndicesTex` 3d 纹理进行索引。这个 Sky Direction 有点类似于环境法线 Bent Normal，默认情况下，Unity 采样 ambient probe 使用的是 Normal，但是 Normal 没有考虑遮蔽信息。开启此项，可以计算出一个方向，用这个方向可以采样到更准确的 ambient probe 颜色。为了更准确的 Sky Occlusion 效果可以考虑开启此项，但是我觉得在有 SSAO 的情况下开启的必要性不大。

最后 Sky Occlusion 有一个限制就是，当 Unity 计算 Sky Occlusion 时，会将所有物体表面的颜色视作灰色，若想调亮或调暗这个灰色，可以在 Sky Occlusion Settings 调整 Albedo Override 的值。说实话，我不是很明白 Unity 为什么这么设计。

### 其他说明
**① Baking Sets**  
如果同时加载了多个场景，此时就需要将这些场景都加入同一个 Baking Set，这样才能同时烘焙这些场景。创建 Baking Set 就在 Window -> Rendering -> Lighting 里，将 Baking Mode 设置为 Baking Set，创建新的 Baking Set 添加场景即可。  

**② Streaming**  
Unity 自带了 APV 数据的 streaming，在 URP Asset 的 Lighting -> Light Probe System 中有两个选项，一是 Enable GPU Streaming，二是 Enable Disk Streaming（需先开启 GPU Streaming）。开启后，在运行时，Unity 会自动加载在摄像机视锥体内部的 cell（之前说过 cell 是 Streaming 的最小单位）。除了 Unity 自带的 streaming，我们还可以使用 AssetBundles 或 Addressables 来加载 APV 数据，但是要先在 Project Settings -> Graphics -> Pipeline Specific Settings 中开启 Probe Volume Disable Streaming Assets 选项。

# 在 SRP 中接入 APV
下面参考 URP 中的 APV 相关代码，将 APV 的功能接入 SRP。

## CPU 中的工作
### RenderPipelineAsset
首先要 RenderPipelineAsset 要继承 `IProbeVolumeEnabledRenderPipeline`，这个接口有三个属性要继承：  

``` C#
public interface IProbeVolumeEnabledRenderPipeline
{
    bool supportProbeVolume { get; }
    ProbeVolumeSHBands maxSHBands { get; }
    [System.Obsolete("This field is no longer necessary. #from(2023.3)")]
    ProbeVolumeSceneData probeVolumeSceneData { get; }
}
```

然后 RenderPipelineAsset 要声明以下参数，并继承上述属性，以 URP 为例如下：  

``` C#
public class UniversalRenderPipelineAsset : RenderPipelineAsset<UniversalRenderPipeline>, IProbeVolumeEnabledRenderPipeline
{
    ...
    [SerializeField] LightProbeSystem m_LightProbeSystem = LightProbeSystem.LegacyLightProbes;
    [SerializeField] ProbeVolumeTextureMemoryBudget m_ProbeVolumeMemoryBudget = ProbeVolumeTextureMemoryBudget.MemoryBudgetMedium;
    [SerializeField] ProbeVolumeBlendingTextureMemoryBudget m_ProbeVolumeBlendingMemoryBudget = ProbeVolumeBlendingTextureMemoryBudget.MemoryBudgetMedium;
    [SerializeField] [FormerlySerializedAs("m_SupportProbeVolumeStreaming")] bool m_SupportProbeVolumeGPUStreaming = false;
    [SerializeField] bool m_SupportProbeVolumeDiskStreaming = false;
    [SerializeField] bool m_SupportProbeVolumeScenarios = false;
    [SerializeField] bool m_SupportProbeVolumeScenarioBlending = false;
    [SerializeField] ProbeVolumeSHBands m_ProbeVolumeSHBands = ProbeVolumeSHBands.SphericalHarmonicsL1;
    ...
    public bool supportProbeVolume
    {
        get => lightProbeSystem == LightProbeSystem.ProbeVolumes;
    }

    public ProbeVolumeSHBands maxSHBands
    {
        get
        {
            if (lightProbeSystem == LightProbeSystem.ProbeVolumes)
                return probeVolumeSHBands;
            else
                return ProbeVolumeSHBands.SphericalHarmonicsL1;
        }
    }

    [Obsolete("This property is no longer necessary. #from(2023.3)")]
    public ProbeVolumeSceneData probeVolumeSceneData => null;
}
```

这些参数对应的就是如下的设置：  

<div align="center">  
<img src="https://s2.loli.net/2025/12/23/aIEDGcOZtpzR6u3.png" width = "45%" height = "45%" alt="图6 - RenderPipelineAsset 中关于 APV 的设置"/>
</div>

### RenderPipeline
然后就是在 RenderPipeline 的构造函数中用上面的参数初始化 APV 的 `ProbeReferenceVolume`：  

``` C#
public class UniversalRenderPipeline : RenderPipeline
{
    ...
    // Store locally the value on the instance due as the Render Pipeline Asset data might change before the disposal of the asset, making some APV Resources leak.
    internal bool apvIsEnabled = false;
    ...
    public UniversalRenderPipeline(UniversalRenderPipelineAsset asset)
    {
        ...
        apvIsEnabled = asset != null && asset.lightProbeSystem == LightProbeSystem.ProbeVolumes;
        SupportedRenderingFeatures.active.overridesLightProbeSystem = apvIsEnabled;
        SupportedRenderingFeatures.active.skyOcclusion = apvIsEnabled;
        if (apvIsEnabled)
        {
            ProbeReferenceVolume.instance.Initialize(new ProbeVolumeSystemParameters
            {
                memoryBudget = asset.probeVolumeMemoryBudget,
                blendingMemoryBudget = asset.probeVolumeBlendingMemoryBudget,
                shBands = asset.probeVolumeSHBands,
                supportGPUStreaming = asset.supportProbeVolumeGPUStreaming,
                supportDiskStreaming = asset.supportProbeVolumeDiskStreaming,
                supportScenarios = asset.supportProbeVolumeScenarios,
                supportScenarioBlending = asset.supportProbeVolumeScenarioBlending,
#pragma warning disable 618
                sceneData = m_GlobalSettings.GetOrCreateAPVSceneData(),
#pragma warning restore 618
            });
        }
        ...
    }

    protected override void Dispose(bool disposing)
    {
        if (apvIsEnabled) ProbeReferenceVolume.instance.Cleanup();
        ...
    }
    ...
}
```

sceneData 这部分后续版本应该是会被移除的，可以忽略掉。

### Bind APV data
初始化 APV 后，就需要每帧 streaming APV data，以及上传和绑定 GPU 资源了（之前说的球谐池和 indirection structure buffer），这部分工作最好在 Cull 之前处理：  

``` C#
static void RenderSingleCamera(ScriptableRenderContext context, UniversalCameraData cameraData)
{
    ...
    if (!TryGetCullingParameters(cameraData, out var cullingParameters)) return;

    ...
    ProbeVolumesOptions apvOptions = null;
    if (camera.TryGetComponent<UniversalAdditionalCameraData>(out var additionalCameraData))
        apvOptions = additionalCameraData.volumeStack?.GetComponent<ProbeVolumesOptions>();

    bool supportProbeVolume = asset != null && asset.lightProbeSystem == LightProbeSystem.ProbeVolumes;
    ProbeReferenceVolume.instance.SetEnableStateFromSRP(supportProbeVolume);
    ProbeReferenceVolume.instance.SetVertexSamplingEnabled(asset.shEvalMode  == ShEvalMode.PerVertex || asset.shEvalMode  == ShEvalMode.Mixed);
    // We need to verify and flush any pending asset loading for probe volume.
    if (supportProbeVolume && ProbeReferenceVolume.instance.isInitialized)
    {
        ProbeReferenceVolume.instance.PerformPendingOperations();
        if (camera.cameraType != CameraType.Reflection &&
            camera.cameraType != CameraType.Preview)
        {
            ProbeReferenceVolume.instance.UpdateCellStreaming(cmd, camera, apvOptions);
        }
    }

    ...
    if (supportProbeVolume) ProbeReferenceVolume.instance.BindAPVRuntimeResources(cmd, true);
    // Must be called before culling because it emits intermediate renderers via Graphics.DrawInstanced.
    ProbeReferenceVolume.instance.RenderDebug(camera, apvOptions, Texture2D.whiteTexture);

    ...
    data.cullResults = context.Cull(ref cullingParameters);
}
```

①`ProbeVolumesOptions` 就是后处理 volume 中的 Adaptive Probe Volumes Options。URP 为不同的相机都各自创建了一个 VolumeStack，因为后处理 volume 需要根据相机位置进行混合，若管线使用的是 VolumeManager 里提供的 global volume stack，可以直接：  

``` C#
var stack = VolumeManager.instance.stack;
ProbeVolumesOptions apvOptions = stack.GetComponent<ProbeVolumesOptions>();
```

②若不需要支持 PerVertex 的 SH，可以直接 `SetVertexSamplingEnabled(false)`;  
③`RenderDebug` 就是 Window -> Analysis -> Render Debugger 里 Probe Volumes 的相关功能；  
④`PerformPendingOperations` 和 `UpdateCellStreaming` 都是跟 Streaming 有关的操作；  
⑤`BindAPVRuntimeResources` 绑定了如下 3d 纹理和 structured buffer，在 Frame Debugger 里也可以看到这些资源：  

``` C#
cmdBuffer.SetGlobalBuffer(ShaderIDs._APVResIndex, rr.index);
cmdBuffer.SetGlobalBuffer(ShaderIDs._APVResCellIndices, rr.cellIndices);

cmdBuffer.SetGlobalTexture(ShaderIDs._APVResL0_L1Rx, rr.L0_L1rx);
cmdBuffer.SetGlobalTexture(ShaderIDs._APVResL1G_L1Ry, rr.L1_G_ry);
cmdBuffer.SetGlobalTexture(ShaderIDs._APVResL1B_L1Rz, rr.L1_B_rz);
cmdBuffer.SetGlobalTexture(ShaderIDs._APVResValidity, rr.Validity);

cmdBuffer.SetGlobalTexture(ShaderIDs._SkyOcclusionTexL0L1, rr.SkyOcclusionL0L1 ?? (RenderTargetIdentifier)CoreUtils.blackVolumeTexture);
cmdBuffer.SetGlobalTexture(ShaderIDs._SkyShadingDirectionIndicesTex, rr.SkyShadingDirectionIndices ?? (RenderTargetIdentifier)CoreUtils.blackVolumeTexture);
cmdBuffer.SetGlobalBuffer(ShaderIDs._SkyPrecomputedDirections, rr.SkyPrecomputedDirections);
cmdBuffer.SetGlobalBuffer(ShaderIDs._AntiLeakData, rr.QualityLeakReductionData);

if (refVolume.shBands == ProbeVolumeSHBands.SphericalHarmonicsL2)
{
    cmdBuffer.SetGlobalTexture(ShaderIDs._APVResL2_0, rr.L2_0);
    cmdBuffer.SetGlobalTexture(ShaderIDs._APVResL2_1, rr.L2_1);
    cmdBuffer.SetGlobalTexture(ShaderIDs._APVResL2_2, rr.L2_2);
    cmdBuffer.SetGlobalTexture(ShaderIDs._APVResL2_3, rr.L2_3);
}

cmdBuffer.SetGlobalTexture(ShaderIDs._APVProbeOcclusion, rr.ProbeOcclusion ?? (RenderTargetIdentifier)CoreUtils.whiteVolumeTexture);
```

### Update APV Shader Variables
到目前为止，还没有上传后处理 volume 里 ProbeVolumesOptions 的相关数据：  

<div align="center">  
<img src="https://s2.loli.net/2025/12/23/q2yfh59anzPWV4m.png" width = "45%" height = "45%" alt="图7 - Adaptive Probe Volumes Options"/>
</div>

这些内容在 URP 的 ForwardLights.cs 中处理：  

``` C#
internal void SetupLights(UnsafeCommandBuffer cmd, UniversalRenderingData renderingData, UniversalCameraData cameraData, UniversalLightData lightData)
{
    ...
    ProbeVolumeSHBands probeVolumeSHBands = asset.probeVolumeSHBands;
    cmd.SetKeyword(ShaderGlobalKeywords.ProbeVolumeL1, apvIsEnabled && probeVolumeSHBands == ProbeVolumeSHBands.SphericalHarmonicsL1);
    cmd.SetKeyword(ShaderGlobalKeywords.ProbeVolumeL2, apvIsEnabled && probeVolumeSHBands == ProbeVolumeSHBands.SphericalHarmonicsL2);
    ...
    bool enableProbeVolumes = ProbeReferenceVolume.instance.UpdateShaderVariablesProbeVolumes(
        CommandBufferHelpers.GetNativeCommandBuffer(cmd),
        stack.GetComponent<ProbeVolumesOptions>(),
        cameraData.IsTemporalAAEnabled() ? Time.frameCount : 0,
        lightData.supportsLightLayers);

    cmd.SetGlobalInt("_EnableProbeVolumes", enableProbeVolumes ? 1 : 0);
    ...
}
```

ProbeVolumeL1 和 ProbeVolumeL2 这两个 Shader Keyword 后面 Shader 里会提及：  

``` C#
public const string ProbeVolumeL1 = "PROBE_VOLUMES_L1";
public const string ProbeVolumeL2 = "PROBE_VOLUMES_L2";
```

至此接入 APV 的 CPU 中的工作基本都完成了。

### Debug Probe Sampling
虽然在上面我们调用了 `ProbeReferenceVolume.instance.RenderDebug()` 实现了 Render Debugger 里 Probe Volumes 的相关功能，但是有个功能是没有实现的无法使用的，就是 Debug Probe Sampling。这个功能可以让我们在屏幕上选择一个 pixel（根据鼠标的位置），来看到该 pixel 采样到了哪 4 个 probe（会受到 Normal Bias、View Bias 的影响）。要实现这个功能，需要使用到 depth texture 和 normal texture，具体详见 URP -> Runtime -> Passes -> ProbeVolumeDebugPass.cs，以及 URP -> Shaders -> Debug -> ProbeVolumeSamplingDebugPositionNormal.compute，代码我这里就不摘抄了。

## GPU 中的工作
下面参考 URP 的 Lit.shader，来看看如何在 Shader 里实现 APV 的效果。

### Probe Volume
首先 Lit.shader 的 Forward pass 引入了 `Packages/com.unity.render-pipelines.universal/ShaderLibrary/ProbeVolumeVariants.hlsl`，这个文件就是引入两个 Shader Keyword，前面提到过，它们是用于控制采样 APV 时是否使用三阶球谐：  

    #pragma multi_compile _ PROBE_VOLUMES_L1 PROBE_VOLUMES_L2

然后进入 LitForwardPass.hlsl，可以看到 URP 计算 APV 间接光照的函数叫 `InitializeBakedGIData()`：  

    void InitializeBakedGIData(Varyings input, inout InputData inputData)
    {
        ...
        #elif !defined(LIGHTMAP_ON) && (defined(PROBE_VOLUMES_L1) || defined(PROBE_VOLUMES_L2))
        inputData.bakedGI = SAMPLE_GI(input.vertexSH,
            GetAbsolutePositionWS(inputData.positionWS),
            inputData.normalWS,
            inputData.viewDirectionWS,
            input.positionCS.xy,
            input.probeOcclusion,
            inputData.shadowMask);
        #else
        ...
        #endif
    }

> 额外提一下：`SAMPLE_GI` 的最后两个参数 input.probeOcclusion、inputData.shadowMask 分别对应 vertexProbeOcclusion 和 probeOcclusion。我们需要区分一下 **Probe Occlusion** 和 **Sky Occlusion**，这个 Probe Occlusion 跟 Lightmap 里 Shadowmask 模式下的 Occlusion Probe 功能是一模一样的（之前 SRP 讲 Lightmap 的文章里讲过），只要开启了 Shadowmask 烘焙模式，APV 也会额外烘焙出 probeOcclusion 数据。若自定义管线不支持 Shadowmask 模式，可以完全忽略掉它。

`SAMPLE_GI` 这个宏是在 URP 的 GlobalIllumination.hlsl 里定义的，它调用了同在一个文件里的 `SampleProbeVolumePixel` 函数：  

    half3 SampleProbeVolumePixel(in half3 vertexValue, in float3 absolutePositionWS, in float3 normalWS, in float3 viewDir, in float2 positionSS, in float4 vertexProbeOcclusion, out float4 probeOcclusion)
    {
        probeOcclusion = 1.0;

    #if defined(EVALUATE_SH_VERTEX) || defined(EVALUATE_SH_MIXED)
        probeOcclusion = vertexProbeOcclusion;
        return vertexValue;
    #elif defined(PROBE_VOLUMES_L1) || defined(PROBE_VOLUMES_L2)
        half3 bakedGI;
        if (_EnableProbeVolumes)
        {
            EvaluateAdaptiveProbeVolume(absolutePositionWS, normalWS, viewDir, positionSS, GetMeshRenderingLayer(), bakedGI, probeOcclusion);
        }
        else
        {
            bakedGI = EvaluateAmbientProbe(normalWS);
        }
    #ifdef UNITY_COLORSPACE_GAMMA
            bakedGI = LinearToSRGB(bakedGI);
    #endif
        return bakedGI;
    #else
        return half3(0, 0, 0);
    #endif
    }

    half3 SampleProbeVolumePixel(in half3 vertexValue, in float3 absolutePositionWS, in float3 normalWS, in float3 viewDir, in float2 positionSS)
    {
        float4 unusedProbeOcclusion = 0;
        return SampleProbeVolumePixel(vertexValue, absolutePositionWS, normalWS, viewDir, positionSS, unusedProbeOcclusion, unusedProbeOcclusion);
    }
    ...
    #elif defined(PROBE_VOLUMES_L1) || defined(PROBE_VOLUMES_L2)
    #ifdef USE_APV_PROBE_OCCLUSION
        #define SAMPLE_GI(shName, absolutePositionWS, normalWS, viewDir, positionSS, vertexProbeOcclusion, probeOcclusion) SampleProbeVolumePixel(shName, absolutePositionWS, normalWS, viewDir, positionSS, vertexProbeOcclusion, probeOcclusion)
    #else
        #define SAMPLE_GI(shName, absolutePositionWS, normalWS, viewDir, positionSS, vertexProbeOcclusion, probeOcclusion) SampleProbeVolumePixel(shName, absolutePositionWS, normalWS, viewDir, positionSS)
    #endif

需要注意的是参数 positionSS 是像素在屏幕空间的像素坐标，非屏幕 uv 坐标。可以看到上述代码中的重点在于 `EvaluateAdaptiveProbeVolume()`，这个函数是在 SRP Core 的 AmbientProbe.hlsl 定义的，具体位置在 SRP Core -> Runtime -> Lighting -> ProbeVolume 里，也就是存放所有 APV 相关代码的文件夹。所以别忘了引入该文件：  

    #if defined(PROBE_VOLUMES_L1) || defined(PROBE_VOLUMES_L2)
    #include "Packages/com.unity.render-pipelines.core/Runtime/Lighting/ProbeVolume/ProbeVolume.hlsl"
    #endif

其实到目前为止，只要将上述所以代码内容都“复制”进自己的自定义管线中去，就可以实现 APV 了。`EvaluateAdaptiveProbeVolume()` 的具体实现我就不在这里做具体的探讨了，有兴趣可以自己去翻阅文件，简而言之就是根据 APV 数据结构中的 indirection buffer 采样到像素点对应的球谐系数，然后计算出球谐光照。

### 其他说明
**① Sky Occlusion**  
Sky Occlusion 的实现也是在 `EvaluateAdaptiveProbeVolume()` 函数当中，里面调用了 `EvaluateOccludedSky()` 函数：  

    float EvalSHSkyOcclusion(float3 dir, APVSample apvSample)
    {
        // L0 L1
        float4 temp = float4(kSHBasis0, kSHBasis1 * dir.x, kSHBasis1 * dir.y, kSHBasis1 * dir.z);
        return _APVSkyOcclusionWeight * dot(temp, apvSample.skyOcclusionL0L1);
    }

    float3 EvaluateOccludedSky(APVSample apvSample, float3 N)
    {
        float occValue = EvalSHSkyOcclusion(N, apvSample);
        float3 shadingNormal = N;

        if (_APVSkyDirectionWeight > 0)
        {
            shadingNormal = apvSample.skyShadingDirection;
            float normSquared = dot(shadingNormal, shadingNormal);
            if (normSquared < 0.2f)
                shadingNormal = N;
            else
            {
                shadingNormal = shadingNormal * rsqrt(normSquared);
            }
        }
        return occValue * EvaluateAmbientProbe(shadingNormal);
    }

其实实现很简单，就是拿法线方向采样 Sky Occlusion 的球谐数据，如果开启了 Sky Direction，就拿 Sky Direction 采样 AmbientProbe，也就是天空盒的球谐数据。最后将天空盒的球谐光照和 Sky Occlusion 的球谐光照颜色合并。需要注意的是，因为采样了 AmbientProbe，需要引入 AmbientProbe.hlsl：  

    #include "Packages/com.unity.render-pipelines.core/ShaderLibrary/AmbientProbe.hlsl"

**②Meta Pass**  
因为 APV 烘焙使用的是 Progressive Lightmapper 系统，所以烘焙的结果跟 Lightmap 一样是受到 Meta Pass 的影响的。Unity 的烘焙效果说实话还算不错的，我在 Blender 用 Cycle 渲染器也渲染了一张图片，和 APV 做对比，场景中都只有一个纯白的平行光：  

<div align="center">  
<img src="https://s2.loli.net/2025/12/24/g38nFbAKVxqpX9U.png" width = "100%" height = "100%" alt="图8 - 左图是 Blender Cycle 渲染出来的结果，右图是 URP APV。下面都是只包含 Diffuse GI 的结果"/>
</div>

可以看到，整体来说 APV 的效果真的还挺不错的，对于游戏来说肯定是足够了，就是整体颜色对比 Blender 更黄一点。Blender 对颜色纹理的解析和 Unity 总感觉有点不太一样，不知道是不是我的错觉，Unity 的纹理颜色更像是 Blender 里的 Filmic sRGB 的效果。如果想让 APV 间接光更偏白一点，就可以通过 Meta Pass “造假”。