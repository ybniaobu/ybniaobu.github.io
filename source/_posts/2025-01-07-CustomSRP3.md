---
title: Unity Custom SRP 基础（三）
date: 2025-01-07 20:43:16
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2025/01/14/ELDF7wHpCPn2mKM.gif
mathjax: true
description: 本笔记的主要内容有XXXXXXXXXXXXXXXXXXXXXXXXXX。
---

> 本笔记是关于 Unity 的**自定义可编程渲染管线**的入门基础，即 **SRP (Scriptable Rendering Pipeline)**，主要参考了著名的教程 https://catlikecoding.com/ 的 Custom SRP Tutorial，以及知乎上各位图形学大神们的文章。  
>    
> 笔者使用的 Unity 版本是 6000.0.27f1，Core RP Library 的版本是 17.0.3。

# 烘焙光照
这一章节主要讲的就是 Unity 的**离线烘焙 Lightmap 系统**，即 **Progressive Lightmapper**，以及如何将我们的 SRP 和该系统结合在一起，该系统同时会烘焙光照探针 light probe 和反射探针 reflection probe。而 Unity 的**实时 Lightmap 系统**，即 **Enlighten** 系统，Unity 对其的说明是，它只适用于缓慢移动的光照，比如太阳在天空中的移动，不适用于变化特别快的光照，因为 Enlighten 在 CPU 上的计算造成了一定的性能瓶颈，只适合较为高端的设备使用。在 catlikecoding 的教程中没有将 Enlighten 系统融合进 SRP，故我这里也不做详细说明，但想要使用也相对比较简单，可以参考 URP。

我们知道 Lightmap 是**全局光照 Global Illumination** 的离线实现方式，而全局光照的实现包括直接光漫反射、直接光镜面反射、间接光漫反射以及间接光镜面反射。直接光漫反射和直接光镜面反射之前实现了一部分，即方向光的实现。间接光漫反射的实现，可以采用 Skybox 的球谐函数（即 IBL 的漫反射部分）、Lightmap 或者光照探针 light probe，这就是本章节的内容。而间接光镜面反射，则采用 Spilt sum approximation 的方式，要么采样预过滤的 Skybox，要么采样预过滤的反射探针 reflection probe，这个在之后的章节中实现。至于更加高级的全局光照方案，诸如 **SSGI (Screen Space Global Illumination)、VXGI (Voxel Global Illumination)、DDGI (Dynamic Diffuse Global Illumination)** 等等，我会在完成 catlikecoding 教程后再进行学习并实现，这些都是间接光漫反射的实现。至于间接光镜面反射，则有 **SSR (Screen Space Reflection)**、**SSPR (Screen Space Planar Reflection)** 等等。

我在之前的文章 [IBL 基于图像的光照（二）](https://ybniaobu.github.io/2024/08/16/2024-08-16-IBL_Basics2/#Unity-URP-%E7%9A%84-IBL-%E5%AE%9E%E7%8E%B0) 的 Unity URP 的 IBL 实现章节中有大致说明过 URP 如何实现 Lightmap。我会略微修改 URP 的实现方式，使其更适配我选用的 PBR 模型以及之前 IBL 的相关知识，之后会详细说明。

## Lighting Settings
我们可以使用 <kbd>Ctrl</kbd> + <kbd>9</kbd> 打开 Lighting Settings，这个 Lighting Settings 是 per scene 的，我们每个场景都可以配置。我们目前先只打开 Mixed Lighting 的 Baked Global Illumination，Lighting Mode 先选择为 Baked Indirect（后面一章节讲 Shadowmask），Directional Mode 先设置为 Non-Directional：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/15/MsnF27gWoHQCG39.png" width = "45%" height = "45%" alt="图44 - Lighting Settings"/>
</div>

其他设置随意。为了更好地体现烘焙光照的效果，我创建了一个由简单的几何体组成的场景：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/15/8e9z5dNJmgVFHvc.jpg" width = "50%" height = "50%" alt="图45 - 只有直接光影响的一个简单场景"/>
</div>

此时还不能进行烘焙，我们需要在场景中的物体的 MeshRenderer 组件中勾选上 Contribute Global Illumination，勾选上后 Receive Global Illumination 会自动切换至 Lightmaps。或者也可以直接在 Static 下拉菜单中勾选 Contribute GI，又或者直接勾选 Static 成为完全静态物体。也就是说 Lightmap 只影响静态物体。此时再点击 Generate Lighting 按钮，就可以烘焙光照贴图了。

如果场景中的灯光都设置为 Realtime Mode，那么光照贴图就不会包含这些灯光产生的间接光照，只会包含 Skybox 的间接光照。若灯光设置为 Mixed Mode，光照贴图就会包含这些灯光产生的间接光照，而直接光照则由我们的 Shader 计算生成。若灯光设置为 Baked Mode，光照贴图就会包含这些灯光的直接光照和间接光照。

<div  align="center">  
<img src="https://s2.loli.net/2025/01/15/jzt9ArLOG2QKMHi.png" width = "35%" height = "35%" alt="图46 - 上：场景中唯一的方向光为 Realtime Mode，光照贴图只包含 Skybox 的间接光照，烘焙系统同时会把 Skybox 的最亮的部分当作太阳光产生阴影；中：将  Environment Lighting 的 Intensity Multiplier 设置为 0，即天空光的贡献为 0，同时直接光为 Mixed Mode，可以看到只有方向光产生的间接光；下：天空光的贡献为 0，同时直接光为 Baked Mode，包含方向光的直接光和间接光。"/>
</div>

观察上面的光照贴图，有一点比较奇怪，就是我们场景中地面为绿色，但贴图没有一点绿色，这是因为我们还没有定义 **Meta Pass**，Unity 默认物体表面为白色，这个在后面说明。

## 光照贴图实现
### 传递 Light Map UV 坐标
为了让 Shader 可以得到物体在 Light Map 中的 uv 坐标，我们需要在 `DrawingSettings` 设置 `perObjectData`：  

``` C#
DrawingSettings opaqueDrawing = new DrawingSettings(m_UnlitShaderTagId, opaqueSorting)
{
    enableInstancing = asset.enableGPUInstancing,
    perObjectData = PerObjectData.Lightmaps
};
opaqueDrawing.SetShaderPassName(1, m_ForwardLitShaderTagId);
```

此外我们需要在 Shader 中设置 `LIGHTMAP_ON` 关键字，当物体使用 Light Map，Unity 会激活该关键字：  

    #pragma multi_compile _ LIGHTMAP_ON
  
接下来就是在 `Attributes` 和 `Varyings` 结构体中声明 LightMapUV 参数，但是我们直接声明不太好，因为不是所有的物体都使用 LightMap，我们可以定义宏来分支化处理，我放在了我新建的 UnityLightMappingLibrary.hlsl 文件中：  

    #if defined(LIGHTMAP_ON)
        #define LIGHTMAP_UV(index)                      float2 lightMapUV : TEXCOORD##index;
        #define TRANSFER_LIGHTMAP_UV(IN, OUT)           OUT.lightMapUV = IN.lightMapUV * unity_LightmapST.xy + unity_LightmapST.zw;
        #define LIGHTMAP_UV_FRAGMENT(IN)                IN.lightMapUV
    #else
        #define LIGHTMAP_UV(index)
        #define TRANSFER_LIGHTMAP_UV(IN, OUT)
        #define LIGHTMAP_UV_FRAGMENT(IN)                float2(0.0, 0.0)
    #endif

然后在 `Attributes` 和 `Varyings` 结构体中声明，以及在顶点着色器中传递：  

    struct Attributes
    {
        ...
        float2 uv           : TEXCOORD0;
        LIGHTMAP_UV(1)
    };

    struct Varyings
    {
        ...
        float3 binormalWS   : TEXCOORD4;
        LIGHTMAP_UV(5)
    };

    Varyings StandardVert(Attributes IN)
    {
        Varyings OUT;
        ...
        TRANSFER_LIGHTMAP_UV(IN, OUT)
        return OUT;
    }

### 传递 Light Map UV 偏移
在上面的宏定义中的 `unity_LightmapST` 就是我们要传递的 uv 偏移，Unity 会将物体展开至贴图空间，并且多个物体可能会共享一张光照贴图，所以必定会有缩放以及偏移。我们需要将 uv 变换相关参数传递至 `UnityPerDraw` cbuffer 中，比如 `unity_LightmapST`，另外教程里说要加上 `unity_DynamicLightmapST` 否则会破坏 SRP batcher compatibility，但我试了一下不会，可能是版本问题：  

    CBUFFER_START(UnityPerDraw)
      float4x4 unity_ObjectToWorld;
      float4x4 unity_WorldToObject;
      float4 unity_LODFade;
      real4 unity_WorldTransformParams;

      float4 unity_LightmapST;
      float4 unity_DynamicLightmapST;
    CBUFFER_END

### 采样 Light Map
我将 Light Map 相关函数都放在了 UnityLightMappingLibrary.hlsl 文件中，首先最好引入 core RP library 的 EntityLighting.hlsl 文件：  

    #include "Packages/com.unity.render-pipelines.core/ShaderLibrary/EntityLighting.hlsl"
  
这个函数里面定义了一些宏，用于处理 Lightmap 编码和解码相关代码，具体可以查看这篇文档：https://docs.unity3d.com/Manual/Lightmaps-TechnicalInformation.html 。接下来在 UnityInput.hlsl 中定义光照贴图和贴图采样器：  

    TEXTURE2D(unity_Lightmap);
    SAMPLER(samplerunity_Lightmap);
  
创建 `SampleLightMap()` 函数并调用 EntityLighting.hlsl 中的 `SampleSingleLightmap()` 函数：  

    float3 SampleLightMap(float2 lightMapUV)
    {
        #if defined(LIGHTMAP_ON)
            return SampleSingleLightmap(unity_Lightmap, samplerunity_Lightmap, lightMapUV, float4(1, 1, 0, 0), true);
        #else
            return float3(0, 0, 0);
        #endif
    }

`SampleSingleLightmap()` 函数的第四个参数传递的是 Light Map UV 的缩放和偏移，但是之前已经设置好了，所以这里只需要传递 `float4(1, 1, 0, 0)` 就行。第五个参数，当使用离线烘焙的 Lightmap (Progressive Lightmapper) 时选择 true，即 `LIGHTMAP_ON`，当使用实时 Lightmap (Enlighten) 系统时选择 false，即 `DYNAMICLIGHTMAP_ON`。

此时只要将之前 IBL 的 Diffuse 相关代码替换为采样 Light Map 的代码就可以渲染了。我将 Renormalized Disney diffuse 的预计算部分也考虑了进去，实现如下：  

    float3 CalculateLightMap_Diffuse(float2 lightMapUV, StandardPBRParams standardPBRParams, float envBRDF_Diffuse)
    {
        float3 irradiance = SampleLightMap(lightMapUV);
        float3 envBRDFDiffuse = standardPBRParams.albedo * envBRDF_Diffuse;
        float Kd = 1.0 - standardPBRParams.metallic;
        float3 Diffuse = irradiance * envBRDFDiffuse * Kd * standardPBRParams.ao;
        return Diffuse;
    }

    float3 IndirectLighting_Diffuse(float2 lightMapUV, StandardPBRParams standardPBRParams, float envBRDF_Diffuse)
    {
        #if defined(LIGHTMAP_ON)
            return CalculateLightMap_Diffuse(lightMapUV, standardPBRParams, envBRDF_Diffuse);
        #else
            return CalculateIBL_Diffuse(standardPBRParams, envBRDF_Diffuse);
        #endif
    }

最后在片元着色器中计算：  

    float3 envBRDF = SampleEnvLut(_EnvBRDFLut, sampler_Point_Clamp_EnvBRDFLut, standardPBRParams.NoV, standardPBRParams.roughness);

    renderingEquationContent.indirectLightDiffuse += IndirectLighting_Diffuse(LIGHTMAP_UV_FRAGMENT(IN), standardPBRParams, envBRDF.b);

我选用了如下的 HDR 贴图，并且场景中无直接光照，结果如下。若把 Renormalized Disney diffuse 的预计算部分考虑进去，光滑物体边缘会有黑边（比如下面的白球），粗糙物体边缘会有亮边（比如下面的绿球），没实现 Indirect Light Specular 之前区别不大，实现后我觉得考虑进去更好看更有层次感，也更能体现光滑与否：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/15/mqkSbzW4YsdrpZg.png" width = "80%" height = "80%" alt="图47 - 无直接光，只有 Skybox 的烘焙结果。左图：考虑 diffuse 预积分，右图：不考虑 diffuse 预积分"/>
</div>

我同时也对比了只有方向光的情况下（Skybox 的 Intensity Multiplier 设置为 0），3 种不同 mode 下的结果，即 Realtime、Mixed、Baked。这里特别说明一下，Unity 的 LightMap 系统因为比较早，和现在的 PBR 模型在亮度上存在一个 PI 的差别，这也是为什么 Unity 在 URP 的直接光的 Diffuse 计算中没有除以 PI，以及 Specular 项多乘了 PI 的原因。所以在烘焙的时候我把直接光亮度设为了 1，而 Realtime Mode 下亮度设置为了 3.14，Indirect Multiplier 都为 1：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/15/OhZLPeaVwrSFuYq.png" width = "100%" height = "100%" alt="图48 - 左：Realtime Mode，亮度为 3.14，无烘焙；中：Mixed Mode，烘焙时亮度为 1，烘焙完设置回 3.14；右：Baked Mode，亮度为 1。"/>
</div>

我将半影宽度设置为了 5，但还是不如烘焙的半影宽度大。除了半影宽度以外，可以看到 Mixed 和 Baked 效果已经很接近了。我感觉在 URP 下，Mixed 和 Baked 效果应该几乎一模一样。

## 光照探针实现
在 Unity 中，光照贴图只适用于静态物体，对于动态物体，无法使用光照贴图也无法在烘焙贴图时对全局光照做贡献。动态物体只能通过光照探针来接受全局光照的影响。动态物体会使用一定距离内的数个光照探针的球谐光照信息的插值作为接受到的间接漫反射光照。

### Light Probe Group
我们可以通过 GameObject / Light / Light Probe Group 在场景中添加巨量的光照探针。在场景中可以有无数个 Light Probe Group，Unity 会自动生成四面体将所有场景中的光照探针连接在一起。每个动态物体都会处于一个四面体当中，四面体的每个角上的 Light Probe 的数据插值后就是动态物体的间接漫反射光照。如果一个物体超过了光照探针覆盖的区域，它会使用最近的三角形上的每个 Light Probe。

放置 Light Probe 有一些注意点，比如：放置在动态物体需要经过的地方；放置在光线会发生变化的地方等等。Light Probe 还可能造成漏光问题，比如室内的动态物体采样到了室外的光照探针。

### 采样 Light Probe
首先需要传递插值后的球谐光照数据，跟 Light Map 一样，需要设置 `perObjectData`：  

``` C#
perObjectData = PerObjectData.Lightmaps | PerObjectData.LightProbe
```

这就需要在 `UnityPerDraw` cbuffer 中添加 7 个球谐光照参数，Unity 如何实现的球谐光照细节忘了就详见笔记 [IBL 基于图像的光照（一）](https://ybniaobu.github.io/2024/07/09/2024-07-09-IBL_Basics1/#Unity-%E7%9A%84%E7%90%83%E8%B0%90%E5%85%89%E7%85%A7%E5%AE%9E%E7%8E%B0)：  

    CBUFFER_START(UnityPerDraw)
        ...
        float4 unity_SHAr;
        float4 unity_SHAg;
        float4 unity_SHAb;
        float4 unity_SHBr;
        float4 unity_SHBg;
        float4 unity_SHBb;
        float4 unity_SHC;
    CBUFFER_END

然后我们就在 Shader 中采样球谐光照即可，采样的函数跟 IBL Diffuse 是一模一样的，直接用 IBL 的函数就可以：  

    float3 SampleSH(float3 N)
    {
        float3 L0L1;
        float4 vA = float4(N, 1.0);
        L0L1.r = dot(unity_SHAr, vA);
        L0L1.g = dot(unity_SHAg, vA);
        L0L1.b = dot(unity_SHAb, vA);

        float3 L2;
        float4 vB = N.xyzz * N.yzzx;
        L2.r = dot(unity_SHBr, vB);
        L2.g = dot(unity_SHBg, vB);
        L2.b = dot(unity_SHBb, vB);
        
        float vC = N.x * N.x - N.y * N.y;
        L2 += unity_SHC.rgb * vC;

        return L0L1 + L2;
    }

    float3 CalculateIBL_Diffuse(StandardPBRParams standardPBRParams, float envBRDF_Diffuse)
    {
        float3 irradiance = SampleSH(standardPBRParams.N);
        float3 envBRDFDiffuse = standardPBRParams.albedo * envBRDF_Diffuse;
        float Kd = 1.0 - standardPBRParams.metallic;
        float3 IBLDiffuse = irradiance * envBRDFDiffuse * Kd * standardPBRParams.ao;
        return IBLDiffuse;
    }

具体实现效果就不放出来了。这里提一嘴，如果场景中没有 Light Probe，动态物体就会使用 Skybox 的球谐光照，可以在动态物体的 Mesh Renderer 组件的 Probes 中将 Light Probes 选择为 off ，在 Light Probe 插值球谐和 Skybox 的球谐切换对比效果差别。

### Light Probe Proxy Volume
Light Probe 只适用于比较小的动态物体，但是对于大型物体就很容易出现光照错误。比如下图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/16/rj4loLU5mRANOQD.png" width = "40%" height = "40%" alt="图49 - 图中长方体只采样到了处于暗处的光照探针，故整个长方体较暗，不符合光照逻辑"/>
</div>

Unity 对此的解决方案就是 **Light Probe Proxy Volume，LPPV**。直接在长方体添加 LPPV 组件，并且需要将 Mesh Renderer 组件的 Light Probes 属性设置为 Use Proxy Volume。

> URP 没有在管线中支持 LPPV，但 HDRP 和 Built-in Pipeline 支持。

<div  align="center">  
<img src="https://s2.loli.net/2025/01/16/U8YKzNjRwV7furg.png" width = "40%" height = "40%" alt="图50 - Light Probe Proxy Volume"/>
</div>

### 采样 LPPV
我不打算在自己的管线中支持 LPPV，但是支持的方法记录一下。首先同样需要设置 perObjectData 传递数据：  

``` C#
perObjectData = PerObjectData.Lightmaps | PerObjectData.LightProbe | PerObjectData.LightProbeProxyVolume
```

在 UnityPerDraw cbuffer 中添加 4 个参数：  

    CBUFFER_START(UnityPerDraw)
        ...
        // x = Disabled(0)/Enabled(1)
        // y = Computation are done in global space(0) or local space(1)
        // z = Texel size on U texture coordinate
        float4 unity_ProbeVolumeParams;
        float4x4 unity_ProbeVolumeWorldToObject;
        float4 unity_ProbeVolumeSizeInv;
        float4 unity_ProbeVolumeMin;
    CBUFFER_END

LPPV 的数据存储在 3D 纹理当中，定义对应的纹理和采样器：  

    TEXTURE3D(unity_ProbeVolumeSH);
    SAMPLER(samplerunity_ProbeVolumeSH);

core RP library 的 EntityLighting.hlsl 文件中的函数 `SampleProbeVolumeSH4()` 和 `SampleProbeVolumeSH9()` 就是专门用于采样 LPPV 的。我们可以将采样的代码大致写成如下：

    if (unity_ProbeVolumeParams.x) 
    {
        return SampleProbeVolumeSH9(
            unity_ProbeVolumeSH, samplerunity_ProbeVolumeSH,
            IN.positionWS, standardPBRParams.N,
            unity_ProbeVolumeWorldToObject,
            unity_ProbeVolumeParams.y, unity_ProbeVolumeParams.z,
            unity_ProbeVolumeMin.xyz, unity_ProbeVolumeSizeInv.xyz
        );
    }

这样就可以采样了。效果图就不贴了。

## Meta Pass
光照贴图是 indirect diffuse，应该受到物体表面颜色的影响。Unity 通过 **Meta Pass** 使用物体颜色 albedo 或者自发光 emission 值来决定弹射光线的颜色，从而决定烘焙出来的间接光照。若没有定义 Meta Pass，Unity 会默认物体表面为白色，这也是为什么之前烘焙出来的光照贴图没收到物体表面颜色影响的原因。

### 一个简单的 Meta Pass
首先在我们的 Shader 中添加一个新的 Pass，LightMode 必须设置为 Meta，同时要关闭剔除：  

    Pass
    {
        Name "Meta"
        
        Tags { "LightMode" = "Meta" }

        Cull Off

        HLSLPROGRAM
        #pragma target 4.5
        
        #pragma vertex MetaVert
        #pragma fragment MetaFrag
        
        #include "StandardForwardMetaPass.hlsl"
        ENDHLSL
    }

顶点着色器和片元着色器写在 StandardForwardMetaPass.hlsl 里面，一个简单的 Meta Pass 如下，先让片元着色器输出白色：  

    #ifndef YPIPELINE_STANDARD_FORWARD_META_PASS_INCLUDED
    #define YPIPELINE_STANDARD_FORWARD_META_PASS_INCLUDED

    #include "../../ShaderLibrary/Core/YPipelineCore.hlsl"
    #include "../../ShaderLibrary/UnityMetaPassLibrary.hlsl"

    CBUFFER_START (UnityPerMaterial)
        float4 _BaseColor;
        float4 _BaseTex_ST;
    CBUFFER_END

    Texture2D _BaseTex;             SamplerState sampler_Trilinear_Repeat_BaseTex;

    struct Attributes
    {
        float4 positionOS   : POSITION;
        float2 uv           : TEXCOORD0;
        float2 lightMapUV   : TEXCOORD1;
    };

    struct Varyings
    {
        float4 positionHCS  : SV_POSITION;
        float2 uv           : TEXCOORD0;
    };

    Varyings MetaVert(Attributes IN)
    {
        Varyings OUT;
        OUT.positionHCS = MetaVertexPosition(IN.positionOS.xyz, IN.lightMapUV);
        OUT.uv = TRANSFORM_TEX(IN.uv, _BaseTex);
        return OUT;
    }

    float4 MetaFrag(Varyings IN) : SV_TARGET
    {
        return float4(1.0, 1.0, 1.0, 1.0);
    }

    #endif

> 我这里犯了个错误，UnityPerMaterial CBuffer 在所有 Pass 中必须一样，否则会破坏 SRP batcher compatibility，需要注意！！

注意顶点着色器，MetaPass 中输出的 positionHCS 不是裁切空间的坐标，而是 lightMapUV 经过一个比较特殊的坐标变换后的坐标。具体变换逻辑在上面代码中的 `MetaVertexPosition()` 函数中，我写在了 UnityMetaPassLibrary.hlsl 里面，其实 core RP library 中的 MetaPass.hlsl 里面也有类似的函数，叫做 `UnityMetaVertexPosition()`，因为我们目前不使用 DynamicLightmap，所以我把它给简化了：  

    float4 MetaVertexPosition(float3 positionOS, float2 lightMapUV)
    {
        positionOS.xy = lightMapUV * unity_LightmapST.xy + unity_LightmapST.zw;
        positionOS.z = positionOS.z > 0.0 ? FLT_MIN : 0.0;
        return TransformWorldToHClip(positionOS);
    }

注意最后返回的是 `TransformWorldToHClip()`，而不是 `TransformObjectToHClip()`，不要搞错了。我之前搞错了，烘焙一直不对，害得我找了好久问题。至于为什么要这样坐标变换，教程里没讲，我在其他网站上也没找到答案，总之就这么变换就行了。此时已经可以烘焙了，并且烘焙出来的效果和没有 Meta Pass 是一模一样的，也就是物体表面均为白色，图片就没必要放了，反正都一样。

### 传递 Albedo 颜色
之前提到过我们需要向 Meta Pass 传递 albedo 或者 emission，需要通过 `unity_MetaFragmentControl` 来判断传递的是什么数据。x 为 true 时传递 albedo，y 为 true 时传递 emission，具体逻辑详见下面的函数中：  

> URP 的 Meta Pass 的颜色输出为 `metaInput.Albedo = brdfData.diffuse + brdfData.specular * brdfData.roughness * 0.5;`，它计算了 diffuse color 和 F0 颜色并合计在了一起，我觉得没有必要，合计数和物体颜色差不了太多，直接输出物体表面颜色就行。 

    struct UnityMetaParams
    {
        float3 albedo;
        float3 emission;
    };

    CBUFFER_START(UnityMetaPass)
        // x = use uv1 as raster position, lightmap
        // y = use uv2 as raster position, DynamicLightmap
        bool4 unity_MetaVertexControl;

        // x = return albedo
        // y = return emission
        bool4 unity_MetaFragmentControl;
    CBUFFER_END

    float unity_OneOverOutputBoost;
    float unity_MaxOutputValue;

    float4 TransportMetaColor(UnityMetaParams params)
    {
        float4 color;
        if (unity_MetaFragmentControl.x)
        {
            color = float4(params.albedo, 1.0);
            // Apply Albedo Boost from LightmapSettings.
            color.rgb = clamp(pow(abs(color.rgb), saturate(unity_OneOverOutputBoost)), 0, unity_MaxOutputValue);
        }

        if (unity_MetaFragmentControl.y)
        {
            color = float4(params.emission, 1.0);
        }
        
        return color;
    }

然后在片元着色器输出就可以了，我这个材质暂时不支持 emission，故 emission 设为 0：  

    float4 MetaFrag(Varyings IN) : SV_TARGET
    {
        UnityMetaParams meta = (UnityMetaParams) 0.0;
        meta.albedo = SAMPLE_TEXTURE2D(_BaseTex, sampler_Trilinear_Repeat_BaseTex, IN.uv).rgb * _BaseColor.rgb;
        meta.emission = 0.0;
        return TransportMetaColor(meta);
    }

最后输出效果如下，可以和之前的效果对比看看，可以看到间接光有了反射的颜色：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/17/cZSzkfJjdQTw48p.jpg" width = "35%" height = "35%" alt="图51 - Meta Pass 传递颜色"/>
</div>

### 支持 Emission 烘焙
发光物体的烘焙功能不会自动开启，除了需要在 Meta Pass 中输出 emission 值外。还需要在 Shader 中开启 Emission 烘焙功能，但是这个功能是隐藏的，需要通过 ShaderGUI 去开启，有些麻烦：  

``` C#
public override void OnGUI(MaterialEditor materialEditor, MaterialProperty[] properties)
{
    base.OnGUI(materialEditor, properties);
    UnityEmissionProperty();
}

private void UnityEmissionProperty()
{
    EditorGUI.BeginChangeCheck();
    // m_MaterialEditor.LightmapEmissionProperty();
    m_MaterialEditor.LightmapEmissionFlagsProperty(0, true);
    if (EditorGUI.EndChangeCheck())
    {
        foreach (Material m in m_Materials) 
        {
            m.globalIlluminationFlags &= ~MaterialGlobalIlluminationFlags.EmissiveIsBlack;
        }
    }
}
```

`MaterialEditor.LightmapEmissionFlagsProperty` 和 `MaterialEditor.LightmapEmissionProperty` 这两个 API 都是可以的，它们可以让 Emission 的 Global Illumination 下拉菜单出现，默认设置为 None，即不支持 Emission 烘焙到 Light Map，还可以设置为 Realtime 和 Baked。Realtime 是 DynamicLightmap，也就是 Enlighten 系统使用的，我们不支持。

但是这样仍然不够，上述 API 本质是只是绘制 UI。但 Unity 仍然默认所有材质不发光，我们需要取消 `MaterialGlobalIlluminationFlags.EmissiveIsBlack` flag，代码中 `~` 是位非运算符，得到二进制反码。

> ShaderGUI 有需求可以花时间好好了解，上述代码的逻辑我觉得可以更改，重点就在于改变 `Material` 的 `globalIlluminationFlags`，可以不使用 Unity 自带的 LightmapEmissionProperty 来绘制 UI，名称（Global Illumination）很容易让人误解。另外建议查看 globalIlluminationFlags 的官方文档。

<div  align="center">  
<img src="https://s2.loli.net/2025/01/17/HTbc8hAQZNMs9nG.jpg" width = "50%" height = "50%" alt="图52 - Emission Global Illumination 下拉菜单"/>
</div>

<div  align="center">  
<img src="https://s2.loli.net/2025/01/17/QwcAh1lYKEWLiGO.jpg" width = "35%" height = "35%" alt="图53 - Emission 烘焙效果（Unlit Shader）"/>
</div>

### 支持 Transparency 烘焙
烘焙系统会查看材质的渲染队列来决定材质是不透明、透明度裁切还是透明物体。对于透明物体，会自带使用 `_MainTex` 和 `_Color` 这两个属性的 Alpha 通道相乘来作为材质的透明度，或者是使用了 `[MainTexture]` 以及 `[MainColor]` 特性的材质属性，笔记 [Unity URP 基础](https://ybniaobu.github.io/2024/02/23/2024-02-23-URP%E5%9F%BA%E7%A1%80/#%E5%B8%B8%E7%94%A8%E7%9A%84%E6%9D%90%E8%B4%A8%E5%B1%9E%E6%80%A7%E7%9A%84%E7%89%B9%E6%80%A7) 中有对主纹理和主颜色进行说明。对于透明度裁切物体，则自动使用 `_Cutoff` 属性决定透明度测试。烘焙结果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/17/lY5id4SUnyvJj3I.png" width = "70%" height = "70%" alt="图54 - 左：透明度裁切；右：透明度混合"/>
</div>

# Shadow mask
## Lighting Mode
之前在 Lighting Settings 的 Mixed Lighting 的 Lighting Mode 属性选择的是 Baked Indirect。这个属性决定的其实是 Mixed Mode 下 Lights 的表现，包括三种模式：Baked Indirect、Shadowmask、Subtractive：  
**①Baked Indirect**：就是上一章节讲的模式，在该模式下，Mixed Mode 的灯光具有实时的直接光、实时的阴影（限制到自己设定的最大阴影距离）、烘焙的间接光（动态物体通过探针、静态物体通过 lightmaps）；  
**②Shadowmask**：这个模式跟 Baked Indirect 类似，都提供实时直接光和烘焙的间接光。但是若我们想在自己设定的最大阴影距离内使用实时阴影，之外使用烘焙阴影，就需要使用该模式。该模式下，会烘焙出另外一张贴图，叫做 shadow mask，并且会在光照探针中存储更多信息（称为 **Occlusion Probe**），这样在最大阴影距离之外的动态物体可以通过光照探针获得阴影效果。该模式是 Unity 最推荐的模式，也是最真实、效果最好也最耗性能的模式（本章节就是讲这个模式的）。该模式又分为 Distance Shadowmask 和 Shadowmask，在 Edit -> Project Settings -> Quality -> Shadows -> Shadowmask Mode 设置：  
&emsp;&emsp;&emsp;&emsp; - **Distance Shadowmask**：在最大阴影距离之外的动态物体通过光照探针获得静态物体产生的烘焙阴影效果，静态物体通过 shadow mask 贴图获取静态物体产生的烘焙阴影效果。最大阴影距离之内都是完全的实时阴影，即获得静态和动态物体产生的实时阴影；  
&emsp;&emsp;&emsp;&emsp; - **Shadowmask**：动态和静态物体都在最大阴影距离之内接受动态物体产生的实时阴影，并且都在最大阴影距离的内外都接受静态物体的烘焙阴影（分别通过光照探针和 shadow mask 贴图）。  
**③Subtractive**：这个模式比较特殊，它的混合只对一盏方向光（Main Light）有效。所有静态物体的直接光、间接光和阴影都烘焙进光照贴图，只有一盏方向光对动态物体产生直接光和实时阴影，动态物体能通过光照探针接受烘焙阴影和烘焙间接光。但是这样会导致烘焙阴影和实时阴影的结合问题，即同一方向光下的双重阴影，这是不符合常理的，所以 Unity 在 Lighting Settings -> Environment 中提供了 Realtime Shadow Color 的属性，用于一定程度上缓解双重阴影的问题。我不是很明白为什么该模式称为 Subtractive，但这个模式的效果是最不真实的，所以教程并没有支持这种模式，我的 SRP 也不打算支持。  

## Shadowmask map
在 Lighting Mode 设置为 Shadowmask，以及 Shadowmask Mode 设置为 Distance Shadowmask 后（先讲这个模式，之后再讲普通的 Shadowmask 模式），点击烘焙可以看到多了一种贴图，如下（场景中只有一盏 Mixed Mode 的方向光，无 Skybox）：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/18/NtkxGuifyU54Ynv.png" width = "35%" height = "35%" alt="图55 - Shadowmask 贴图"/>
</div>

这张贴图其实就是场上那盏唯一方向光的阴影贴图，只不过是在光照贴图空间下的阴影贴图，并且只包含静态物体的阴影，不包含动态物体的阴影。之所以是红色，是因为目前只有一盏灯，阴影信息存储到了 R 通道。若我们要使用 Shadowmask map，我们得确保光照方向不发生变化，否则采样它得到的阴影将不太合理。另外，如果场景中的灯光都为 Baked Mode，那么将不会烘焙出 Shadowmask 贴图。

其次，一张 Shadowmask 贴图只有 4 个通道，这就意味着至多只可以支持 4 盏 Mixed Light。当场景中有超过 4 盏 Mixed Light，Unity 会把多余的灯光视为 Baked 模式。

### 传递 Shadowmask 和 Occlusion Probe
还是老样子，首先得指示 Unity 传递 Shadowmask 或 Occlusion Probe 相关数据给 GPU，即设置 PerObjectData：  

``` C#
perObjectData = PerObjectData.Lightmaps | PerObjectData.ShadowMask | PerObjectData.LightProbe | PerObjectData.OcclusionProbe
```

然后我们得设置 shader keyword 用于控制 Shader 使用 Distance Shadowmask 还是 Shadowmask。但是我们得首先判断一盏投射阴影的灯是否为 Mixed Mode，且当前烘焙模式为 Shadowmask，只有都为真时，我们才需要激活相关关键字。这些信息都存储在 `LightBakingOutput` 结构体当中，可以通过灯的 `Light.bakingOutput` 属性获取，若判断成功，将 `m_UseShadowMask` 设置为 true，以及记录当前灯光使用了 Shadowmask 贴图的哪个通道，若没有使用，则 `LightBakingOutput.occlusionMaskChannel` 返回 -1。传递 `m_ShadowMaskChannel` 的工作，代码就不放出了，随便找一个向量的某一个通道传递就可以了。

``` C#
...
private bool m_UseShadowMask;
private float m_ShadowMaskChannel;
...

private void RecordDirectLightData(...)
{
    ...
    m_UseShadowMask = false;
    m_ShadowMaskChannel = -1.0f;

    for (int i = 0; i < visibleLights.Length; i++)
    {
        ...
        if (visibleLight.light.shadows != LightShadows.None && visibleLight.light.shadowStrength > 0f && data.cullingResults.GetShadowCasterBounds(i, out Bounds outBounds))
        {
            m_shadowingSunLightCount++;
            m_sunLightColor = new Vector4(color.r, color.g, color.b, visibleLight.light.shadowStrength);

            LightBakingOutput lightBaking = visibleLight.light.bakingOutput;
            if (lightBaking.lightmapBakeType == LightmapBakeType.Mixed && lightBaking.mixedLightingMode == MixedLightingMode.Shadowmask)
            {
                m_UseShadowMask = true;
                m_ShadowMaskChannel = lightBaking.occlusionMaskChannel;
            }
        }
        ...
    }
}
```

之后就是设置关键字了，Distance Shadowmask 还是 Shadowmask 模式分别对应 `_SHADOW_MASK_DISTANCE` 和 `_SHADOW_MASK_NORMAL`。可以使用 `QualitySettings.shadowmaskMode` 判断当前使用了哪个模式。另外要注意，对于不使用的关键字，别忘了 disable 它：  

``` C#
private const string m_ShadowMaskDistance = "_SHADOW_MASK_DISTANCE";
private const string m_ShadowMaskNormal = "_SHADOW_MASK_NORMAL";

private static GlobalKeyword _ShadowMaskDistance;
private static GlobalKeyword _ShadowMaskNormal;

protected override void Initialize()
{
    ...
    // Global Keywords initialize
    _ShadowMaskDistance = GlobalKeyword.Create(m_ShadowMaskDistance);
    _ShadowMaskNormal = GlobalKeyword.Create(m_ShadowMaskNormal);
    ...
}

private void SetKeywords(YRenderPipelineAsset asset, ref PipelinePerFrameData data)
{
    if (QualitySettings.shadowmaskMode == ShadowmaskMode.DistanceShadowmask)
    {
        data.buffer.SetKeyword(_ShadowMaskDistance, m_UseShadowMask);
        data.buffer.SetKeyword(_ShadowMaskNormal, false);
    }
    else
    {
        data.buffer.SetKeyword(_ShadowMaskNormal, m_UseShadowMask);
        data.buffer.SetKeyword(_ShadowMaskDistance, false);
    }
}
```

### 采样 Shadowmask 和 Occlusion Probe
首先在 Shader 中声明传递过来的关键字：  

    #pragma multi_compile _ _SHADOW_MASK_DISTANCE _SHADOW_MASK_NORMAL

然后在 UnityInput.hlsl 中声明 ShadowMask 贴图和采样器以接受 CPU 传递过来的阴影信息并采样。同时动态物体不从 ShadowMask 贴图获取阴影信息，要么接受实时阴影，要么从 light probes 获取阴影信息，Unity 会将额外的阴影信息烘焙进 light probes，称为 **occlusion probes**，我们需要将 `unity_ProbesOcclusion` 放入 `UnityPerDraw` CBuffer 中以获取额外的阴影信息：  

    CBUFFER_START(UnityPerDraw)
        ...
        // Occlusion Probes
        float4 unity_ProbesOcclusion;
        ...
    CBUFFER_END

    TEXTURE2D(unity_ShadowMask);
    SAMPLER(samplerunity_ShadowMask);

因为静态物体和动态物体要采样不同的东西，故定义以下函数以区分，当 `LIGHTMAP_ON` 定义时（拥有 light map 的静态物体），采样 ShadowMask 贴图，否则采样 occlusion probe。

    float4 SampleShadowmask(float2 lightMapUV)
    {
        #if defined(LIGHTMAP_ON)
            return SAMPLE_TEXTURE2D(unity_ShadowMask, samplerunity_ShadowMask, lightMapUV);
        #else
            return unity_ProbesOcclusion;
        #endif
    }

另外，为了让 occlusion probe 数据兼容 Core RP library 的 UnityInstancing.hlsl，不破坏 GPU Instancing，需要定义 `SHADOWS_SHADOWMASK`，可以在引入 UnityInstancing.hlsl 之前定义该宏：  

    ...
    #if defined(_SHADOW_MASK_DISTANCE) || defined(_SHADOW_MASK_NORMAL)
        #define SHADOWS_SHADOWMASK
    #endif

    #include "Packages/com.unity.render-pipelines.core/ShaderLibrary/UnityInstancing.hlsl"
    ...

### 支持 LPPV occlusion probe
这里记录一下 LPPV 如何支持 shadow mask，还是先设置 `PerObjectData`：  

    perObjectData = PerObjectData.Lightmaps | PerObjectData.ShadowMask | PerObjectData.LightProbe | PerObjectData.OcclusionProbe | PerObjectData.LightProbeProxyVolume | PerObjectData.OcclusionProbeProxyVolume

获取 LPPV 的 occlusion 信息要通过 Core RP library 中 EntityLighting.hlsl 文件的 `SampleProbeOcclusion()` 函数获取，而不是之前的 `SampleProbeVolumeSH9()` 函数：  

    float4 SampleShadowmask(float2 lightMapUV, float3 positionWS) 
    {
        #if defined(LIGHTMAP_ON)
            ...
        #else
            if (unity_ProbeVolumeParams.x) 
            {
                return SampleProbeOcclusion(
                    unity_ProbeVolumeSH, samplerunity_ProbeVolumeSH, 
                    IN.positionWS, unity_ProbeVolumeWorldToObject, 
                    unity_ProbeVolumeParams.y, unity_ProbeVolumeParams.z, 
                    unity_ProbeVolumeMin.xyz, unity_ProbeVolumeSizeInv.xyz
                );
            }
            else 
            {
                return unity_ProbesOcclusion;
            }
        #endif
    }


## 混合阴影
### Distance Shadowmask Mode
先讲 Distance Shadowmask 的混合方式，这种模式的重点在于，超过了最大阴影距离就采样 Shadowmask 或者 occlusion probe，没超过就是实时阴影。首先定义一个名为 `MixBakedAndRealtimeShadows()` 的函数，接受参数 `shadowFade` 以便在烘焙阴影和实时阴影之间做混合：  

    float MixBakedAndRealtimeShadows(float2 lightMapUV, int channel, float realtimeShadowAttenuation, float shadowFade)
    {
        float bakedShadowAttenuation = SampleShadowmask(lightMapUV)[channel];
        float shadowAttenuation = lerp(bakedShadowAttenuation, realtimeShadowAttenuation, shadowFade);
        return shadowAttenuation;
    }

这个 shadowFade 参数就是用来判断是否超过了最大阴影距离，我们之前在方向光阴影章节有计算过，略微修改一下计算阴影的代码（`_SunLightShadowFadeParams.w` 传递的是使用了 Shadowmask 的哪个通道）：  

    float GetSunLightShadowFalloff(float2 lightMapUV, float3 positionWS, float3 normalWS, float3 L)
    {
        float cascadeIndex = ComputeCascadeIndex(positionWS);
        float shadowStrength = _SunLightColor.w;
        float shadowFade = 1.0 - step(_SunLightShadowParams.x, cascadeIndex);
        shadowFade *= ComputeDistanceFade(positionWS, _SunLightShadowFadeParams.x, _SunLightShadowFadeParams.y);
        shadowFade *= ComputeCascadeEdgeFade(cascadeIndex, _SunLightShadowParams.x, positionWS, _SunLightShadowFadeParams.z, _CascadeCullingSpheres[_SunLightShadowParams.x - 1]);

        float texelSize = _CascadeCullingSpheres[cascadeIndex].w * 2.0 / _SunLightShadowParams.y;
        float3 positionWS_Bias = ApplyShadowBias(positionWS, texelSize, _SunLightShadowParams.w, normalWS, L);
        float3 positionSS = TransformWorldToSunLightShadowCoord(positionWS_Bias, cascadeIndex);
        float shadowAttenuation = ApplyPCF_SunLight(positionWS, texelSize, positionSS, cascadeIndex);

        #if defined(_SHADOW_MASK_DISTANCE)
            return lerp(1.0, MixBakedAndRealtimeShadows(lightMapUV, _SunLightShadowFadeParams.w ,shadowAttenuation, shadowFade), shadowStrength);
        #else
            return lerp(1.0, shadowAttenuation, shadowStrength * shadowFade);
        #endif
    }

这样就可以混合阴影了，结果就是动态物体投射的阴影从实时阴影 Fade 到不产生阴影，静态物体投射的阴影从实时阴影变为采样得到的烘焙阴影。动态物体接受的阴影从实时阴影到接受 occlusion probe，静态物体接受的阴影还是从实时阴影变为采样得到的烘焙阴影。如下图（实时阴影半影宽度设置为了比较小的值，以便和烘焙阴影区分）：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/21/HBjrwWalNGLZ14f.png" width = "60%" height = "60%" alt="图56 - 左：实时阴影；中：实时阴影过渡到烘焙阴影；右：烘焙阴影。每张图左上角两个白球为动态物体，其余皆为静态物体，可以看到动态物体投射阴影从有到无，接受到的阴影变淡了，即从接受实时阴影到接受 occlusion probe。"/>
</div>

但上面这样计算阴影有个问题就是，即使在最大阴影距离之外还要计算实时阴影，浪费了性能。可以使用 if，在计算实时阴影之前做判断，在 Unity 里运行起来是比不判断要快的，特别是在部分场景在最大阴影距离之外的情形下，就是不知道在不同机型测试下会怎么样，我感觉 if 应该不会比采样多次要慢。

    if (cascadeIndex >= _SunLightShadowParams.x)
    {
        #if defined(_SHADOW_MASK_DISTANCE) || defined(_SHADOW_MASK_NORMAL)
            return lerp(1.0, SampleShadowmask(lightMapUV)[_SunLightShadowFadeParams.w], shadowStrength);
        #else
            return 1.0;
        #endif
    }

除了上述问题外，还有个问题，就是当我们把镜头移到非常远，所有物体（包括下面的平面）都在最大阴影距离之外的时候，所有的阴影都会突然消失，这是因为我们在传递灯光和阴影信息的时候，用 `CullingResults.GetShadowCasterBounds()` 判断了灯光是否影响到至少一个阴影投射物体，影响到了才激活 `_SHADOW_MASK_DISTANCE` 宏，需要修改一下：  

    // 原来的代码
    // if (visibleLight.light.shadows != LightShadows.None && visibleLight.light.shadowStrength > 0f && data.cullingResults.GetShadowCasterBounds(i, out Bounds outBounds))
    // {
    //     m_shadowingSunLightCount++;
    //     ...
    // }

    if (visibleLight.light.shadows != LightShadows.None && visibleLight.light.shadowStrength > 0f)
    {
        if (data.cullingResults.GetShadowCasterBounds(i, out Bounds outBounds)) m_shadowingSunLightCount++;
        ...
    }

### Shadowmask Mode
在这个模式下，所有静态物体无论在哪里都只投射烘焙阴影，而动态物体只在最大阴影范围内投射实时阴影。故在最大阴影范围内都可以接受到动态物体的实时阴影，在最大阴影范围内外都可以接受到静态物体的烘焙阴影（分别通过 Shadowmask 和 Occlusion Probe）。

关键字 `_SHADOW_MASK_NORMAL` 之前讲过，设置好之后，我们需要在 Shader 中判断使用烘焙阴影还是实时阴影，只需要取两者之间较小值就行，原因下面再讲，新建一个名为 `ChooseBakedAndRealtimeShadows()` 的函数：  

    float ChooseBakedAndRealtimeShadows(float2 lightMapUV, int channel, float realtimeShadowAttenuation, float shadowFade)
    {
        float bakedShadowAttenuation = SampleShadowmask(lightMapUV)[channel];
        realtimeShadowAttenuation = lerp(1.0, realtimeShadowAttenuation, shadowFade);
        return min(bakedShadowAttenuation, realtimeShadowAttenuation);
    }

然后在计算阴影的函数中，增加使用该模式的分支即可：  

    float GetSunLightShadowFalloff(float2 lightMapUV, float3 positionWS, float3 normalWS, float3 L)
    {
        ...
        #if defined(_SHADOW_MASK_DISTANCE)
            return lerp(1.0, MixBakedAndRealtimeShadows(lightMapUV, _SunLightShadowFadeParams.w ,shadowAttenuation, shadowFade), shadowStrength);
        #elif defined(_SHADOW_MASK_NORMAL)
            return lerp(1.0, ChooseBakedAndRealtimeShadows(lightMapUV, _SunLightShadowFadeParams.w , shadowAttenuation, shadowFade), shadowStrength);
        #else
            return lerp(1.0, shadowAttenuation, shadowStrength * shadowFade);
        #endif
    }

之所以这样子可行的原因是，在该模式下，Unity 会在阴影贴图中剔除所有的静态物体，只保留了动态物体的距离。所以当某个像素点只接受到了动态物体阴影，此时采样阴影贴图并比较得到 0，采样 Shadowmask 会得到 1，因为没有接受到烘焙阴影，那么较小值就为 0，即在阴影中；当某个像素点只接受到了静态物体阴影，采样 Shadowmask 会得到 0，采样阴影贴图并比较得到 1，同理，在阴影中；同时接受到动态物体阴影和静态阴影就看哪个更暗，这样就不会出现同一灯光下阴影重叠的问题了，不像 Subtractive 模式。具体效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/21/ZwXhQ8nYbAJolUT.jpg" width = "30%" height = "30%" alt="图57 - Shadowmask - Shadowmask Mode 的效果，图中蓝框内两个球为动态，其余皆为静态。"/>
</div>

上述方法因为可以剔除掉更多物体，所以在性能上比 Distance Shadowmask Mode 要好一点。


# LOD Groups
LOD 的作用就是在场景中根据距离相机的远近来显示不同细节程度的模型，距离相机近时，显示细节程度较高的模型（即高模），距离远时，显示细节程度较低的模型（即低模），甚至可以是一张带有 billboard 效果的图片，以此来增加渲染效率。总之，LOD 是一个空间换取时间的技术，因为需要大量的模型文件，增加了包体的大小，并且在运行时也会增加一定的内存消耗。

## LOD Group Component
使用 Unity 的 LOD Group 组件来实现 LOD，最好在场景中新建一个空物体，然后添加 LOD Group 组件，默认是分为 4 个层次：即 LOD 0、LOD 1、LOD 2、Culled。我们可以通过选中一个层次右键 -> Insert Before 插入一个新的层次，如下图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/22/JRga9QvMLxsVNWI.png" width = "40%" height = "40%" alt="图58 - LOD Group Component"/>
</div>

上图中的提示中，Active LOD bias 可以在 Edit -> Project Settings -> Quality -> Level of Detail -> LOD Bias 中设置，他是用来修改不同 Level 过渡所占屏幕的百分比的，也就是上图中的 LOD 1 的 60% 这些百分比。若 LOD bias 为 1，当物体占据屏幕大小为 60% 以上，则显示 LOD 0 模型，60% 以下一点就显示为 LOD 1 模型。LOD bias 则会减少这个比例，比如 LOD bias 为 2，即使显示的百分比为 60%，当物体占据屏幕大小为 30% 以上，都会显示 LOD 0 模型，30% 以下才会显示 LOD 1 模型，总之 LOD bias 越大，Unity 会更倾向于显示高模。

我们可以将不同细节程度的模型直接拖入合适的 LOD level，拖入后，物体会变为这个带有 LOD Group Component 的空物体的子物体。这样移动摄像头就可以根据物体距离看到不同 LOD level 的切换了（我拖入了不同颜色的球体）：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/22/VNdMr9btlXwiz7I.gif" width = "40%" height = "40%" alt="图59 - LOD Transition 效果（颜色有点奇怪是录制压缩的问题）"/>
</div>

相同的物体可以放入不同的 LOD level，不同位置的物体也可以放入不同的 LOD level，总之 LOD Groups 不仅仅只是用来显示不同细节程度的模型，可以定制化一些效果。另外，不同的 LOD level 都可以采样到 light map，即获得 Baked lighting，但是只有 LOD 0 才会对烘焙光照产生影响，也就是场景中只会反应弹射出 LOD 0 模型的间接光。

## LOD Fade
上面的 gif 中，切换不同 LOD level 都是直接突变式的切换，我们是可以给切换一些效果的，首先将 Fade Mode 从 None 改为 Cross Fade。Speed Tree Fade Mode 是专门给 SpeedTree 用的，暂且不谈。选择为 Cross Fade 后，每个 Level 会出现 Fade Transition Width 的属性。该属性为 0 时表示，在不同 Level 之间切换没有 Fade 效果，属性为 1 时，当前 Level 整个级别都在 Fade，属性为 0.5 时，在当前 Level 的 50% 开始 Fade。

但是目前移动摄像机仍然没有效果，是因为我们需要配置 shader，首先声明 `LOD_FADE_CROSSFADE` 关键字：  

    #pragma multi_compile _ LOD_FADE_CROSSFADE

Unity 通过 `unity_LODFade` 来传输 Fade 参数的，我们需要放入 `UnityPerDraw` CBuffer 中：  

    CBUFFER_START(UnityPerDraw)
        ...
        float4 unity_LODFade; // x is the fade value ranging within [0,1]. y is x quantized into 16 levels
        ...
    CBUFFER_END

接下来就是在片元着色器中设置切换效果，可以使用之前提到过的 `InterleavedGradientNoise()` 函数：  

    #if defined(LOD_FADE_CROSSFADE)
        float dither = InterleavedGradientNoise(IN.positionHCS.xy, 0);
        clip(unity_LODFade.x - dither);
    #endif

有些时候也需要在 ShadowCaster Pass 中也加入上述代码，将物体切换效果和阴影匹配起来。我将所有级别的 Fade Transition Width 属性都设置为了 0.5，效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/22/8o94YE6qVmcMzXh.gif" width = "40%" height = "40%" alt="图60 - 虚假的 LOD Cross Fade 效果"/>
</div>

这样做效果仍然一般，因为还是能感受到切换时的明显变化，因为一个级别消失完才切换到另一个物体。这时候就要利用到 `unity_LODFade` 的另外一个特征了，比如物体目前在 LOD 0 的级别上，摄像机越来越远，达到设定 Fade Transition Width 的 50% 时，此时 LOD_FADE_CROSSFADE 被激活，当前级别（LOD 0）的物体的 `unity_LODFade.x` 会从 1 变到 0，但是此时后面一级别（LOD 1）的物体的 `unity_LODFade.x` 会从 -1 变到 0。我们可以利用这一点，让当前级别物体渐渐消失，后面级别物体渐渐出现，这也是为什么这个模式叫做**交叉混合 Cross Fade**。我们只需将代码修改为如下（注意正负号，别搞错了）：  

    #if defined(LOD_FADE_CROSSFADE)
        float dither = InterleavedGradientNoise(IN.positionHCS.xy, 0);
        float isNextLodLevel = step(unity_LODFade.x, 0);
        dither = lerp(-dither, dither, isNextLodLevel);
        clip(unity_LODFade.x + dither);
    #endif

<div  align="center">  
<img src="https://s2.loli.net/2025/01/22/T83v5EGiMQ7mqAn.gif" width = "40%" height = "40%" alt="图61 - 真实的 LOD Cross Fade 效果（可能看不太清楚，但是还是可以感受出效果更好了）"/>
</div>

另外，组件上有个名为 Animate Cross-Fading 的属性，勾选上之后，Fade Transition Width 属性会消失，并且不同 LOD 级别之间切换变为了固定时间，而不是按照超过某一百分比进行切换变化，这个时间默认是 0.5 秒，可以通过 `LODGroup.crossFadeAnimationDuration` API 设置。在编辑模式下切换时间可能会和 play mode 不太一样，建议在 play mode 下测试。


# Reflections
这部分就是间接光的镜面反射部分了，为了更好适配 PBR，我这部分会和教程存在一定差异，但仍然是采样预过滤的 Skybox 或者反射探针 Reflection probe，只不过我会使用 Epic Games 的 Spilt sum approximation 达到更好的效果，Spilt sum approximation 具体详见笔记 [IBL 基于图像的光照（二）](https://ybniaobu.github.io/2024/08/16/2024-08-16-IBL_Basics2/#Spilt-sum-approximation) 以及 [Custom Better PBR in Unity](https://ybniaobu.github.io/2024/10/22/2024-10-22-BetterPBR1/#IBL-Specular)。

但无论如何 Skybox 和 Reflection probe 实现的间接光的镜面反射都只能说是勉强能用，都具有一定的局限性，之后还是要实现 SSR 或 SSPR 的。

## 采样 Skybox
若我们在烘焙菜单界面（Window -> Rendering -> Lighting 或者直接 <kbd>Ctrl</kbd> + <kbd>9</kbd>）的 Environment 的 Skybox Material 属性赋予了天空盒材质，并且 Environment Reflections 的 Source 属性选择的是 **Skybox** 而不是 Custom，此时 Generate Lighting 的话，Unity 会自动生成当前 Skybox Cubemap 的预过滤版本，可以在 Scene Asset 的文件夹中找到，和光照贴图是在同一个路径的。也可以使用自己预过滤的 Skybox ，只要 Source 属性选择的是 **Custom** ，再在出现的 Cubemap 属性中拖入自己预过滤的 Cubemap 就行。另外，选择 Custom 后再 Generate Lighting 是不会再生成 Unity 预过滤的 Skybox 的。

Unity 预过滤的环境贴图和 Epic Games 的 GGX 分布版本的预过滤环境贴图是不太一样的。Unity 预过滤的版本随着粗糙度的变大（mipmap 级数的增加）模糊是线性均匀变化的，而 GGX 分布是模糊程度的变化是非线性的，先变化快再变化慢。为了适配这种差异，Unity 在 Core RP library 的 ImageBasedLighting.hlsl 文件中提供了 `PerceptualRoughnessToMipmapLevel()` 函数，将模糊程度的线性变化转变为非线性变化。我自己实测下来，直接使用 GGX 分布的预过滤环境贴图和使用了 non-linear remapping 后的 Unity 预过滤环境贴图的效果是非常接近的。

还有一点需要注意的是，虽然 Unity 预过滤的环境贴图有 12 个 mipmap（0 到 11），但其实只有 7 个 mipmap 是有效的（0 到 6），所以我们采样 Unity 预过滤环境贴图的时候要注意，**最大 mipmap 级别应该是 6**。同时 Unity 会自动将预过滤的环境贴图的曝光率调低一点，会比自己的预过滤的环境贴图要暗一点。

---

为了能够采样到预过滤的环境贴图还是要先设置 `PerObjectData`，新增 `PerObjectData.ReflectionProbes`：  

``` C#
perObjectData = PerObjectData.ReflectionProbes | PerObjectData.Lightmaps | PerObjectData.ShadowMask | PerObjectData.LightProbe | PerObjectData.OcclusionProbe
```

然后再在 UnityInput.hlsl 中声明预过滤的环境贴图和采样器：  

    TEXTURECUBE(unity_SpecCube0);
    SAMPLER(samplerunity_SpecCube0);

无论 Skybox 还是 Custom 模式的预过滤环境贴图都会通过 `unity_SpecCube0` 传递。然后拿视角反射方向 R 采样即可，具体方法之前的文章都有讲过，就不再赘述了。这里只说明一下需要经过 non-linear remapping 采样 Unity 的预过滤环境贴图的方式，代码如下：  

    float RoughnessToMipmapLevel(float roughness, float maxMipLevel)
    {
        roughness = roughness * (1.7 - 0.7 * roughness);
        return roughness * maxMipLevel;
    }

    float3 CalculateIBL_Specular_RemappedMipmap(StandardPBRParams standardPBRParams, TextureCube prefilteredEnvMap, SamplerState prefilteredEnvMapSampler, float2 envBRDF_Specular, float3 energyCompensation)
    {
        float mipmap = RoughnessToMipmapLevel(standardPBRParams.roughness, 6.0);
        float3 prefilteredColor = SAMPLE_TEXTURECUBE_LOD(prefilteredEnvMap, prefilteredEnvMapSampler, standardPBRParams.R, mipmap).rgb;
        float3 envBRDFSpecular = envBRDF_Specular.xxx * standardPBRParams.F0 + (float3(standardPBRParams.F90, standardPBRParams.F90, standardPBRParams.F90) - standardPBRParams.F0) * envBRDF_Specular.yyy;
        float3 IBLSpecular = prefilteredColor * envBRDFSpecular * energyCompensation * ComputeSpecularAO(standardPBRParams.NoV, standardPBRParams.ao, standardPBRParams.roughness);
        
        IBLSpecular *= ComputeHorizonSpecularOcclusion(standardPBRParams.R, standardPBRParams.N);
        return IBLSpecular;
    }

然后在片元着色器调用该函数就行，使用 Unity 预过滤的环境贴图和 Epic Games 的 GGX 分布预过滤环境贴图的效果分别如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/24/oOmUgc5p8YFql1A.png" width = "50%" height = "50%" alt="图62 - 从左到右粗糙度为 0，0.2，0.4 ... 1。上：Unity 预过滤环境贴图 linear 采样；中：Unity 预过滤环境贴图 non-linear remapping 采样；下：GGX 分布预过滤环境贴图"/>
</div>

我使用的 GGX 分布预过滤环境贴图的亮度仍然比 Unity 预过滤环境贴图略微高一点，亮度一样的话效果应该会更加接近的。

## 采样 Reflection Probes
直接采样 Skybox 的金属感不是很强，当然也跟选择的 HDRI 贴图有关系，上面的天空盒整体颜色单一，所以看起来效果一般。并且采样 Skybox 缺乏局部信息，比如室内的金属物体不可能反射整个天空，此时就需要使用到**反射探针 Reflection Probe** 了。我们可以在场景中通过 GameObject -> Light -> Reflection Probe 来添加反射探针。因为反射探针从它们的坐标渲染附近场景的信息，物体反射信息的正确与否，跟物体和反射探针的相对位置存在一定关系，所以使用反射探针也得不到完全正确的反射信息。为了弥补这点，跟光照探针一样，需要在场景中放置较多的反射探针，以便物体采样时进行选择，我们也可以调整 Reflection Probe Component 上的 Importance 属性来决定哪个反射探针的优先级更高。物体的 Mesh Renderer Component 中的 Probes 属性中也可以看到附近有哪些 Reflection Probe 以及优先级排序。这两个组件的属性如下图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/24/yQ2eExGOh1KlaSP.png" width = "40%" height = "40%" alt="图63 - 上：Reflection Probe Component；下：Mesh Renderer Component 中的 Probes 相关属性"/>
</div>

Reflection Probe 的 Type 有 Baked、Custom、Realtime。Baked 就是 Generate Lighting 的时候烘焙；Custom 就是自己放置个预过滤的 Cubemap 进去，跟 Skybox 的 Custom 类似；Realtime 可以选择刷新模式，On Awake、Every Frame 以及 Via Scripting。还有就是，Mesh Renderer Component 的 Reflection Probes 有三种模式，分别为 Blend Probes、Blend Probes And Skybox 以及 Simple，Blend Probes 就是混合附近两个 Reflection Probe；Blend probes and skybox 就是混合天空盒以及一个 Reflection Probe 或者混合两个 Reflection Probe；Simple 就是只采样一个 Reflection Probe。  

关于 Blend probes 和 Blend Probes And Skybox，教程说，URP 和 HDRP 都不支持，故教程也不打算支持，但是我看了一下[官方文档](https://docs.unity3d.com/6000.0/Documentation/Manual/render-pipelines-feature-comparison.html)，里面说是支持的，应该是教程比较早的原因。我在 URP 感受了一下混合的效果，也比较违和，特别是天空盒和 Reflection Probe 混合变糊后是不太符合常理的，所以我觉得也没必要实现 Blend probes 和 Blend Probes And Skybox，跟教程统一。所以这里无论哪个选项，实现的本质上来说就是 Simple 模式，即采样一个 Reflection Probe。我们无需改变代码，因为也是通过 `unity_SpecCube0` 传递的，效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/24/zWtkhBvrQY95w14.jpg" width = "40%" height = "40%" alt="图64 - 采样 Reflection Probe 后的效果"/>
</div>

可以看到效果真的很好，上图中粗糙度都为 0，两个金属球，两个非金属球。但是还是能发现渲染错误的，我只放置了一个 Reflection Probe，在左上角白色金属球的位置。第一点，红色金属球的反射信息都是错误的，因为采样到了跟白色金属球一样的信息；第二点，白色金属球反射的红色金属球是纯黑色的。第一点只能靠多放 Reflection Probe 解决，第二点需要在 Window -> Rendering -> Lighting -> Environment -> Environment Reflections 的 Bounces 设置为 2 及 2 以上。我将 Bounces 设置为了 3，多放置一个 Reflection Probe 在红色金属球的位置上，效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/01/24/s7QoHJSjvfnILh3.jpg" width = "40%" height = "40%" alt="图65 - Bounces 为 3 并且放置了两个 Reflection Probe 分别在两个金属球的位置"/>
</div>

还有一点，就是如果我们使用的 Cubemap 的压缩格式是 dLDR 和 RGBM 格式，就不能直接对 Cubemap 进行采样，否则颜色会变，此时就需要对 Cubemap 进行解码。首先需要在 `UnityPerDraw` CBuffer 中添加 `unity_SpecCube0_HDR`：  

    CBUFFER_START(UnityPerDraw)
        ...
        float4 unity_ProbesOcclusion;

        // HDR environment map decode instruction
        float4 unity_SpecCube0_HDR;
        ...
    CBUFFER_END

然后再使用 Core RP library 的 EntityLighting.hlsl 文件中的 `DecodeHDREnvironment()` 函数解码：  

    float3 SampleHDREnvironment(TextureCube envMap, SamplerState envMapSampler, float3 dir, float mipmap)
    {
        float4 env = SAMPLE_TEXTURECUBE_LOD(envMap, envMapSampler, dir, mipmap);
        return DecodeHDREnvironment(env, unity_SpecCube0_HDR);
    }

现在将 .hdr 和 .exr 格式导入 Unity 会自动压缩为 BC6H 格式，该格式在 DX11 级 PC 硬件以及 PS4 和 XboxOne 及以上都受支持，而且相对来说比其他压缩方式效果更好。BC6H 格式无需在 Shader 里解码，直接采样即可。所以上述代码，如果确保不使用 RGBM encoding 和 Double Low Dynamic Range (dLDR) encoding 就无需添加在 Shader 里。关于压缩格式的更多知识，可以以后遇到了，或者有不同平台的需求时再详细了解。


# Complex Maps
这一章节主要讲解 **Secondary Map (Detail Map)**，以及对应的用于控制 Detail Map 应用范围的 **Detail Mask**。Detail Map 主要目的是更方便地给材质添加微观级别的细节，比如皮肤毛孔、金属划痕、砖块裂缝等等，将这些细节信息实时地与其他材质信息（比如 Albedo、Smoothness 以及 Normal）进行混合。但是这些细节是可以在离线制作主要贴图时一起绘制进去的，无需额外使用 Detail Map，故我将不在自己的管线中支持。但 Detail Map 的思想值得借鉴，故将其如何实现记录如下。

## Mask Map
Unity HDRP 将 Metallic、Ambient Occlusion、Detail Mask 和 Smoothness 四个通道信息打包成了一张贴图，即 **Mask Map (MODS map)**。使用这类贴图，跟 Metallic、Smoothness 贴图一样，需要关闭 sRGB (Color Texture)。

<div  align="center">  
<img src="https://s2.loli.net/2025/02/01/KzPVX1I3teOMjaq.png" width = "45%" height = "45%" alt="图66 - Mask Map (MODS map)"/>
</div>

在 Shader 中添加 Mask Map 属性，默认值建议为白色，如下：  

    [NoScaleOffset] _MaskMap("Mask (MODS)", 2D) = "white" {}

如何采样并使用 Metallic、Ambient Occlusion、Smoothness 这三个通道的信息比较基础，没什么好讲的。Detail Mask 的使用见下面 Detail Map。

## Detail Map
Detail map 同样包含了 4 个通道的信息，按顺序分别为 Desaturated albedo、Normal Y、Smoothness、Normal X，如下图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/02/01/PihxCoNKptyqlJn.png" width = "45%" height = "45%" alt="图67 - Detail Map"/>
</div>

该贴图的默认值建议设置为 0.5，因为该贴图的 R、B 通道表达的是变化程度：当 R、B 通道为 0.5 时，detail map 不修改 albedo 和 smoothness；当 R、B 通道小于 0.5 时，会让材质的 albedo 变暗以及减少光滑度。

    _DetailMap("Details", 2D) = "linearGrey" {}

Detail Map 的 UV 会和 Base Map 不一样，所以别忘了在顶点着色器和片元着色器中声明 uv，并应用 tiling 和 offset。

还有一点就是，因为 Detail Map 的信息一般只在距离摄像机较近的位置才有效，摄像机较远时，这些细节基本就看不清了。为了防止这些小细节在较远处变得杂乱无章，我们可以在 Detail Map 的 Import Settings 里面勾选 Mipmap 的 Fadeout to Gray，此时会显示 Fade range 滑动块，可以控制哪个 mipmap 级别开始 fade 哪个级别结束 fade，然后再将纹理的 Filter Mode 设置为 Trilinear，这样 Detail Map 的 mipmap 就会插值到灰色，在较远处就不会显示 Detail Map 中的细节信息了。

### Detailed Albedo
Detail Map 混合 albedo 的代码大致如下，首先因为 0.5 是中值，需要重映射至 -1 到 1，以便将 albedo 变暗或者变亮。然后再根据符号，决定将 albedo 插值到 0 还是 1：  

    float3 baseColor = _BaseColor.rgb;
    float3 albedo = SAMPLE_TEXTURE2D(_BaseTex, sampler_BaseMap, baseUV).rgb;
    float albedoDetail = SAMPLE_TEXTURE2D(_DetailMap, sampler_DetailMap, detailUV).r;
    albedoDetail = albedoDetail * 2.0 - 1.0;

    albedo = lerp(albedo, albedoDetail < 0.0 ? 0.0 : 1.0, abs(albedoDetail));
    return albedo * baseColor;

这样子操作有一个问题就是，变亮的效果会比变暗的效果明显，这是因为是在线性空间下进行的插值，而人眼对暗部的敏感度要高于亮部的敏感度。所以我们可以将其转换至 gamma 空间下插值，或者开根号来近似，如下：  

    albedo = lerp(sqrt(albedo), albedoDetail < 0.0 ? 0.0 : 1.0, abs(albedoDetail));
    albedo *= albedo;

然后我们就可以使用 Mask Map 中的 Detail Mask 来控制 Detail Map 修改的范围：  

    float mask = SAMPLE_TEXTURE2D(_MaskMap, sampler_BaseMap, baseUV).b;
    albedo = lerp(sqrt(albedo), albedoDetail < 0.0 ? 0.0 : 1.0, abs(albedoDetail) * mask);

为了更好地控制 Detailed Albedo 的修改程度，可以再加一个变量：  

    _DetailAlbedo("Detail Albedo", Range(0, 1)) = 1
    ...
    float albedoDetail = SAMPLE_TEXTURE2D(_DetailMap, sampler_DetailMap, detailUV).r * _DetailAlbedo;

### Detailed Smoothness
smoothness 跟 albedo 类似，代码大致如下：  

    _DetailSmoothness("Detail Smoothness", Range(0, 1)) = 1
    ...
    float smoothness = SAMPLE_TEXTURE2D(_MaskMap, sampler_BaseMap, baseUV).a * _Smoothness;
    float smoothnessDetail = SAMPLE_TEXTURE2D(_DetailMap, sampler_DetailMap, detailUV).b * _DetailSmoothness;
    float mask = SAMPLE_TEXTURE2D(_MaskMap, sampler_BaseMap, baseUV).b;
    smoothness = lerp(smoothness, smoothnessDetail < 0.0 ? 0.0 : 1.0, abs(smoothnessDetail) * mask);
    return smoothness;

### Detailed Normal
在 Shader 中采样 Normal Map 会使用到 core RP library 的 Packing.hlsl 中的 `UnpackNormalScale()` 或 `UnpackNormal()` 函数：  

    real3 UnpackNormalScale(real4 packedNormal, real bumpScale)
    {
        #if defined(UNITY_ASTC_NORMALMAP_ENCODING)
            return UnpackNormalAG(packedNormal, bumpScale);
        #elif defined(UNITY_NO_DXT5nm)
            return UnpackNormalRGB(packedNormal, bumpScale);
        #else
            return UnpackNormalmapRGorAG(packedNormal, bumpScale);
        #endif
    }

Unity 自动帮我们解码了 Normal Map 的压缩。DXT5nm 格式就是 DXT5 压缩格式 for normal，该格式将 normal 的 xy 轴存储到了 AG 通道。

因为 Detailed Normal 在 Detail Map 的 AG 通道，我们可以使用 `UnpackNormalAG()` 来获取 Detailed Normal，并使用 core RP library 的 CommonMaterial.hlsl 的函数 `BlendNormalRNM()`，对 Normal Map 的法线和 Detailed Normal 进行混合（法线混合的相关知识在下面的章节补充）：  

    float4 normalMap = SAMPLE_TEXTURE2D(_NormalMap, sampler_BaseMap, baseUV);
    float3 normal = UnpackNormalScale(normalMap, _NormalScale);

    float4 detailMap = SAMPLE_TEXTURE2D(_DetailMap, sampler_DetailMap, detailUV);
    float scale = _DetailNormalScale * SAMPLE_TEXTURE2D(_MaskMap, sampler_BaseMap, baseUV).b;
    float3 detailNormal = UnpackNormalAG(detailMap, scale);
    normal = BlendNormalRNM(normal, detailNormal);

    return normal;

如此一来，就可以将 Detail Map 的信息跟 Base Map、Normal Map 等贴图一起混合在材质上了。

# Normal Blending
法线混合相关内容是 catlikecoding 的 Custom SRP 教程没有涉及的，但在 Rendering 教程中有提及，我觉得有必要补充一下。因为法线贴图存储的不是颜色，直接线性混合，会减少法线带来的细节，让凹凸细节变得更加模糊，渲染结果变得更平整，最终效果是不能接受的。

【Linear Blending、Overlay Blending】

## Reoriented normal mapping

## UDN blending