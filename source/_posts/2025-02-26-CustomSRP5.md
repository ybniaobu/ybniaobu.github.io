---
title: Unity Custom SRP 基础（五）
date: 2025-02-26 11:30:46
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2025/02/26/m2zJbPd8aeGD14A.gif
mathjax: true
description: 本笔记的主要内容包含 PCSS 的实现；XXXXXXXXXXXXXXXXXXXXXXXXXX
---

> 本笔记是关于 Unity 的**自定义可编程渲染管线**的入门基础，即 **SRP (Scriptable Rendering Pipeline)**，主要参考了著名的教程 https://catlikecoding.com/ 的 Custom SRP Tutorial，以及知乎上各位图形学大神们的文章。  
>    
> 笔者使用的 Unity 版本是 6000.0.27f1，Core RP Library 的版本是 17.0.3。

# PCSS
本章节讲解**百分比接近软阴影 Percentage-Closer Soft Shadows (PCSS)** 的实现，这部分内容是 catlikecoding 教程中没有的。PCSS 解决了 PCF 均匀相同的半影宽度问题，能够根据**遮挡物 Blocker** 和**接受物 Receiver** 之间的距离产生可变宽度的软阴影效果。该技术最早由 NVIDIA 的一篇论文提出：[Percentage-Closer Soft Shadows](https://developer.download.nvidia.com/shaderlibrary/docs/shadow_PCSS.pdf) ，本章节也是基于这篇论文书写的。 

## 实现步骤
**第一步：Blocker Search**  
搜索阴影贴图的一定的区域，并记录下比采样点离光源更近的深度值做平均（注意这里只平均比采样点更近的遮挡物的深度值）。理论上来说，搜索区域的大小是由灯光大小和接受物离光源的距离共同决定的，如下图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/02/27/mw86o9gnCTrKx1e.jpg" width = "50%" height = "50%" alt="图78 - Blocker Search Step"/>
</div>

阴影贴图的位置是在灯光的近裁切平面，所以真实的搜索区域的大小需要通过相似三角形计算得到。但是在实际操作中，因为近裁切平面离光源非常近，我们可以假定近裁切平面就在灯光的位置。这样搜索区域的大小在世界空间下就是灯光的大小。根据灯光的大小和世界空间下阴影贴图的大小，可以计算出灯光所占阴影贴图的比例，就可以得到灯光所占阴影贴图像素的范围。

为什么要对遮挡距离进行平均计算，而不直接使用**着色点 shading point** 的采样深度。是因为一旦该点未被遮挡，遮挡距离会直接变为 0，这样就会形成一条具体的边缘，阴影将无法散开。

**第二步：Penumbra Estimation**  
根据计算得到的平均遮挡距离、着色点距离（接受物距离）和灯光大小，使用相似三角形计算半影宽度，如下图：  

$$ \omega_{Penumbra} = \cfrac {(d_{Receiver} - d_{Blocker}) \cdot \omega_{Light}} {d_{Blocker}} $$

<div  align="center">  
<img src="https://s2.loli.net/2025/02/27/6TaGFh1Hz54vmdU.jpg" width = "25%" height = "25%" alt="图79 - Penumbra Estimation Step "/>
</div>

**第三步：Filtering**  
这一步跟 PCF 是一样的，只不过滤波核大小使用的上一步估算出来的大小，可以使用一个参数乘以计算出来的半影宽度，来控制滤波核大小。

## 具体实现
下面为方向光、点光源、聚光灯的 PCSS 的具体实现，因为这三种光源的实现细节会有所不同，故一个一个讲，首先是最简单的聚光灯。

### 聚光灯
首先是需要从 CPU 传递到 GPU 的可控制的参数，CPU 中具体步骤这里就不摘抄了：  

    CBUFFER_START(LightParamsPerFrame)
        ...
        float4 _SpotLightShadowBias[MAX_SHADOWING_SPOT_LIGHT_COUNT]; // x: depth bias, y: slope scaled depth bias, z: normal bias, w: slope scaled normal bias
        float4 _SpotLightShadowParams[MAX_SHADOWING_SPOT_LIGHT_COUNT]; // x: light size, y: penumbra scale, z: blocker search sample number, w: filter sample number
        float4 _SpotLightDepthParams[MAX_SHADOWING_SPOT_LIGHT_COUNT]; // x: (f + n) / (f - n), y: -2 * f * n / (f - n); [if UNITY_REVERSED_Z] x: (f + n) / (n - f), y: -2 * f * n / (n - f)
        ...
    CBUFFER_END

原本我 Shadow Bias 的四个参数是全光源共用的，后来从效果体验上觉得对于点光源、聚光灯和方向光还是分开比较好。当然也可以选择所有点光源、所有聚光灯、所有方向光都分别共用一个 Shadow Bias，即三个 Shadow Bias 作为全局设置。但我最终还是选择了把 Shadow Bias 作为 per-light 设置，即一个灯光一个 Shadow Bias。`_SpotLightDepthParams` 里面的参数是用于将非线性深度转变为线性深度的，后面会提到。

#### Blocker Search

然后就是 PCSS 的第一步 Blocker Search 了，先确定搜索区域的范围，即灯光所占阴影贴图的比例，用灯光大小除以阴影贴图的大小，两者都是在世界空间下的大小。对于聚光灯来说，由于不是正交投影，阴影贴图的大小对于不同深度的着色点是不同的，这点在 PCF 中也提到过，就不再重复了：  

    float searchWidthWS = GetSpotLightSize(shadowingSpotLightIndex);
    float searchWidthPercent = searchWidthWS / (2.0 * ComputeTanHalfFOV(lightIndex) * linearDepth);
  
拿到了搜索区域的范围，就可以利用它偏移着色点并在阴影贴图上采样，获取遮挡物深度值最后平均。这里注意一下，我们要获取的是线性深度值，而直接在阴影贴图上采样得到的是非线性深度值，后面计算半影宽度需要的是线性深度。这里可以选择用非线性深度值做平均后再转换为线性深度值，但相对来说不太准确（其实影响也不大）。我这里选择的是，每次采样后都转换为线性深度，再做平均。

如何将非线性深度值转换为线性深度属于基础知识了，忘了回去看《Unity Shader入门精要》。我这里直接写结论了，首先传递投影矩阵的 m22 和 m23 分量，注意如果 Reversed Z 就传递 -m22 和 -m23，这样在 Shader 里就不用判断 Reversed Z 了，计算出来都是从近到远变大的线性深度值：  

``` C#
m_SpotLightDepthParams[i] = SystemInfo.usesReversedZBuffer
                            ? new Vector4(-projectionMatrix.m22, -projectionMatrix.m23)
                            : new Vector4(projectionMatrix.m22, projectionMatrix.m23);
```

在 Shader 中线性深度转换代码如下：  

    float NonLinearToLinearDepth(float4 depthParams, float nonLinearDepth)
    {
        return depthParams.y / (2.0 * nonLinearDepth - 1.0 + depthParams.x);
    }

注意，在计算平均遮挡深度时，仅在发生遮挡的时候才计入平均距离，故需要做判断。这里可以选择比较非线性深度值，也可以选择比较线性深度值，但是选择比较非线性深度值需要做 Reversed Z 判断并反转大于小于符号，代码如下：  

    float2 ComputeAverageBlockerDepth(float index, TEXTURE2D_ARRAY(shadowMap), float sampleNumber, float searchWidthPercent, float3 positionSS, float4 depthParams, uint hash1, uint hash2, float2x2 rotation)
    {
        float d_Shading = positionSS.z;
        float ld_Shading = NonLinearToLinearDepth(depthParams, d_Shading);
        float ald_Blocker = 0.0;
        float count = 1e-8; // avoid division by zero

        for (int i = 0; i < sampleNumber; i++)
        {
            float2 offset = mul(rotation, InverseSampleCircle(Sobol_Scrambled(i, hash1, hash2))) * 0.5;
            offset = offset * searchWidthPercent;
            float2 uv = positionSS.xy + offset;
            float d_Blocker = SampleShadowArray_Depth(uv, index, shadowMap, SHADOW_SAMPLER);
            float ld_Blocker = NonLinearToLinearDepth(depthParams, d_Blocker);
            
            if (ld_Blocker < ld_Shading)
            {
                ald_Blocker += ld_Blocker;
                count += 1.0;
            }
        }
        ald_Blocker = ald_Blocker / count;
        return float2(ald_Blocker, count);
    }

上述代码中变量的前缀 d、ld、ald 的含义分别为 depth、linear depth、average linear depth。offset 需要乘以 0.5 是因为 Sobol 序列随机数生成的范围是 [0, 1]，圆盘化后范围变为了 [-1, 1]，这样范围就从 1 变为 2 了。这样子就可以得到平均遮挡深度了，但是仍然存在问题。我将着色点深度减去平均遮挡深度所占着色点深度的比例（d_Shading - ald_Blocker）/ d_Shading 输出如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/02/27/Ufsjnp1zAcWv5m7.jpg" width = "40%" height = "40%" alt="图80 - 自遮挡现象"/>
</div>

为了方便观察，我将 Light Size 设置为了 0。可以看到有非常多的 alias，在“阴影”区域内的表现是正确的，立方体底部因为遮挡物和接受物非常接近，故输出值接近 0，接近黑色。但是在“阴影”区域外的 alias 区域从理论上来说都应该是白色，因为这些着色点并没有遮挡物，即 ald_Blocker 为 0，输出应该为 1。而且若将 Light Size 变大，alias 区域就会变为全黑色。这些 alias 产生的原因跟自阴影走样是一样的，那么解决方法自然就是 shadow bias，我选择第三步的 Filtering 和这里共用一个 shadow bias。为了节省计算，`ApplyShadowBias()` 函数我又改回了 (texelSize + penumbraWS) * shadowBias 的计算方法。

    float GetSpotLightShadowAttenuation_PCSS(int lightIndex, float3 positionWS, float3 normalWS, float3 L, float linearDepth)
    {
        float shadowingSpotLightIndex = GetShadowingSpotLightIndex(lightIndex);
        float texelSize = 2.0 * ComputeTanHalfFOV(lightIndex) * linearDepth / GetPunctualLightShadowArraySize();
        float searchWidthWS = GetSpotLightSize(shadowingSpotLightIndex);
        float searchWidthPercent = searchWidthWS / (2.0 * ComputeTanHalfFOV(lightIndex) * linearDepth);
        
        float3 positionWS_SearchBias = ApplyShadowBias(positionWS, GetSpotLightShadowBias(shadowingSpotLightIndex), texelSize, searchWidthWS, normalWS, L);
        float3 positionSS_Search = TransformWorldToSpotLightShadowCoord(positionWS_SearchBias, shadowingSpotLightIndex);
        
        uint hash1 = Hash_Jenkins(asuint(positionWS));
        uint hash2 = Hash_Jenkins(asuint(positionSS_Search));
        float random = floatConstruct(hash1);
        float randomRadian = random * TWO_PI;
        float2x2 rotation = float2x2(cos(randomRadian), -sin(randomRadian), sin(randomRadian), cos(randomRadian));
        float4 depthParams = GetSpotLightDepthParams(shadowingSpotLightIndex);
        float blockerSampleNumber = GetSpotLightBlockerSampleNumber(shadowingSpotLightIndex);

        float2 blocker = ComputeAverageBlockerDepth(shadowingSpotLightIndex, SPOT_LIGHT_SHADOW_MAP, blockerSampleNumber, searchWidthPercent, positionSS_Search, depthParams, hash1, hash2, rotation);
        float ald_Blocker = blocker.x;
        float blockerCount = blocker.y;
        
        ...
    }

#### Penumbra Estimation & Filtering
有了平均遮挡深度后，就可以计算半影宽度了，我增加了一个参数用于控制半影宽度的大小，即 penumbra scale：  

    float penumbraWS = GetSpotLightPenumbraScale(shadowingSpotLightIndex) * GetSpotLightSize(shadowingSpotLightIndex) * (linearDepth - ald_Blocker) / ald_Blocker;
    float penumbraPercent = penumbraWS / (2.0 * ComputeTanHalfFOV(lightIndex) * linearDepth);

后面就跟 PCF 一模一样了，没什么好讲的，只不过要注意的是，ald_Blocker 为 0 时，即没有遮挡物时，shadow attenuation 应该设置为 1.0，代码如下（为了节省计算，`ApplyPCF()` 函数里的 offset 改为了不除以 shadowArraySize，免得一乘一除重复计算）：  

    float GetSpotLightShadowAttenuation_PCSS(int lightIndex, float3 positionWS, float3 normalWS, float3 L, float linearDepth)
    {
        float shadowStrength = GetSpotLightShadowStrength(lightIndex);
        float distanceFade = ComputeDistanceFade(positionWS, GetMaxShadowDistance(), GetShadowDistanceFade());
        
        ... // Blocker Search
        
        if (blockerCount < 1.0) return 1.0;

        float penumbraWS = GetSpotLightPenumbraScale(shadowingSpotLightIndex) * GetSpotLightSize(shadowingSpotLightIndex) * (linearDepth - ald_Blocker) / ald_Blocker;
        float penumbraPercent = penumbraWS / (2.0 * ComputeTanHalfFOV(lightIndex) * linearDepth);
        float3 positionWS_FilterBias = ApplyShadowBias(positionWS, GetSpotLightShadowBias(shadowingSpotLightIndex), texelSize, penumbraWS, normalWS, L);
        float3 positionSS_Filter = TransformWorldToSpotLightShadowCoord(positionWS_FilterBias, shadowingSpotLightIndex);
        float filterSampleNumber = GetSpotLightFilterSampleNumber(shadowingSpotLightIndex);
        float shadowAttenuation = ApplyPCF_2DArray(shadowingSpotLightIndex, SPOT_LIGHT_SHADOW_MAP, filterSampleNumber, penumbraPercent, positionSS_Filter, hash1, hash2, rotation);
        
        return lerp(1.0, shadowAttenuation, shadowStrength * distanceFade);
    }

最终效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/02/28/9FBCeUQVX3IdlpE.jpg" width = "50%" height = "50%" alt="图81 - Spot Light PCSS（Blocker Search 和 Filtering 样本数量都为 8）"/>
</div>

### 点光源
点光源和聚光灯逻辑是一样的，只是采样的是 Cubemap，具体区别跟 PCF 那里其实差不多，没必要多讲了，这里就直接写代码了：  

    float2 ComputeAverageBlockerDepth_CubeArray(float index, float faceIndex, TEXTURECUBE_ARRAY(shadowMap), float sampleNumber,
        float searchWidthPercent, float3 sampleDir, float3 positionSS, float4 depthParams, uint hash1, uint hash2, float2x2 rotation)
    {
        float d_Shading = positionSS.z;
        float ld_Shading = NonLinearToLinearDepth(depthParams, d_Shading);
        float ald_Blocker = 0.0;
        float count = 1e-8; // avoid division by zero

        for (int i = 0; i < sampleNumber; i++)
        {
            float2 offset = mul(rotation, InverseSampleCircle(Sobol_Scrambled(i, hash1, hash2))); // don't need to divide 2, because cubemap is also [-1, 1]
            offset = offset * searchWidthPercent;
            float3 sampleDir_Offset = sampleDir + GetCubeMapOffset(faceIndex, offset);
            float d_Blocker = SampleShadowCubeArray_Depth(sampleDir_Offset, index, shadowMap, SHADOW_SAMPLER);
            float ld_Blocker = NonLinearToLinearDepth(depthParams, d_Blocker);
            
            if (ld_Blocker < ld_Shading)
            {
                ald_Blocker += ld_Blocker;
                count += 1.0;
            }
        }
        ald_Blocker = ald_Blocker / count;
        return float2(ald_Blocker, count);
    }

    float GetPointLightShadowAttenuation_PCSS(int lightIndex, float faceIndex, float3 positionWS, float3 normalWS, float3 L, float linearDepth)
    {
        float shadowStrength = GetPointLightShadowStrength(lightIndex);
        float distanceFade = ComputeDistanceFade(positionWS, GetMaxShadowDistance(), GetShadowDistanceFade());
        
        float shadowingPointLightIndex = GetShadowingPointLightIndex(lightIndex);
        float texelSize = 2.0 * linearDepth / GetPunctualLightShadowArraySize();
        float searchWidthWS = GetPointLightSize(shadowingPointLightIndex);
        float searchWidthPercent = searchWidthWS / (2.0 * linearDepth);
        
        float3 positionWS_SearchBias = ApplyShadowBias(positionWS, GetPointLightShadowBias(shadowingPointLightIndex), texelSize, searchWidthWS, normalWS, L);
        float3 sampleDir_Search = normalize(positionWS_SearchBias - GetPointLightPosition(lightIndex));
        float3 positionSS_Search = TransformWorldToPointLightShadowCoord(positionWS_SearchBias, shadowingPointLightIndex, faceIndex);
        
        uint hash1 = Hash_Jenkins(asuint(positionWS));
        uint hash2 = Hash_Jenkins(asuint(positionSS_Search));
        float random = floatConstruct(hash1);
        float randomRadian = random * TWO_PI;
        float2x2 rotation = float2x2(cos(randomRadian), -sin(randomRadian), sin(randomRadian), cos(randomRadian));
        
        float4 depthParams = GetPointLightDepthParams(shadowingPointLightIndex);
        float blockerSampleNumber = GetPointLightBlockerSampleNumber(shadowingPointLightIndex);
        float2 blocker = ComputeAverageBlockerDepth_CubeArray(shadowingPointLightIndex,faceIndex, POINT_LIGHT_SHADOW_MAP,
            blockerSampleNumber, searchWidthPercent, sampleDir_Search, positionSS_Search, depthParams, hash1, hash2, rotation);
        float ald_Blocker = blocker.x;
        float blockerCount = blocker.y;
        
        if (blockerCount < 1.0) return 1.0;

        float penumbraWS = GetPointLightPenumbraScale(shadowingPointLightIndex) * GetPointLightSize(shadowingPointLightIndex) * (linearDepth - ald_Blocker) / ald_Blocker;
        float penumbraPercent = penumbraWS / (2.0 * linearDepth);
        float3 positionWS_FilterBias = ApplyShadowBias(positionWS, GetPointLightShadowBias(shadowingPointLightIndex), texelSize, penumbraWS, normalWS, L);
        float3 sampleDir_Filter = normalize(positionWS_FilterBias - GetPointLightPosition(lightIndex));
        float3 positionSS_Filter = TransformWorldToPointLightShadowCoord(positionWS_FilterBias, shadowingPointLightIndex, faceIndex);
        float filterSampleNumber = GetPointLightFilterSampleNumber(shadowingPointLightIndex);
        float shadowAttenuation = ApplyPCF_CubeArray(shadowingPointLightIndex, faceIndex, POINT_LIGHT_SHADOW_MAP, filterSampleNumber,
            penumbraPercent, sampleDir_Filter, positionSS_Filter, hash1, hash2, rotation);
        
        return lerp(1.0, shadowAttenuation, shadowStrength * distanceFade);
    }

效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/02/28/IAYCaJtBgzTR9yh.jpg" width = "50%" height = "50%" alt="图82 - Point Light PCSS（Blocker Search 和 Filtering 样本数量都为 8）"/>
</div>

### 方向光
方向光跟精确光的主要区别就是，方向光是正交投影，并且我们不可能把摄像机放置在太阳的位置，这就对我们准确估算半影宽度产生了影响。我们回去看半影宽度估算的公式，首先采样得到的 $\,d_{Blocker}\,$、$\,d_{Receiver}\,$ 是不准的，因为我们的正交视锥体是包裹着视角范围的，而且正交视锥体是多个的，因为使用了级联阴影。我们通过采样拿到的 $\,d_{Blocker}\,$、$\,d_{Receiver}\,$ 仅仅是遮挡物离当前正交视锥体近裁切平面的距离。

但是 $\,d_{Receiver} - d_{Blocker}\,$ 却不会受到影响，无论当前正交视锥体近裁切平面有多远，因为这段距离是由接受物和遮挡物的位置决定的，跟光源位置无关。这样一来由于 $\,\omega_{Light}\,$，即太阳的大小，和真实的 $\,d_{Blocker}\,$ 的尺度过大，我们可以认为真实的 $\,d_{Blocker}\,$ 是不会变的，即太阳和地球的距离。那么 $\,\omega_{Light} / d_{Blocker} \,$ 是个常量，由此一来就可以估算半影宽度了。太阳的**角直径 angular diameter** 约为 0.5332°，太阳的直径约为 1,391,400 km，太阳距离地球的平均距离约为 149,600,000 km。那么 $\,\omega_{Light} / d_{Blocker} \,$ 约为 0.0093，我们可以四舍五入为 0.01。

遮挡物搜索区域的范围仍然用 light size 来控制，只不过这个 light size 没有什么具体意义（推荐还是太阳光和精确光的参数名字分开，我这里就偷懒了）。半影宽度还是可以通过 penumbra scale 控制大小。还有一点要注意的是，对于正交投影和透视投影，非线性深度值转换为线性深度的公式是不同的，代码如下：  

    float NonLinearToLinearDepth_Ortho(float4 depthParams, float nonLinearDepth)
    {
        return (depthParams.y - 2.0 * nonLinearDepth + 1.0) / depthParams.x;
    }

除了上述的不同之处之外，PCSS 实现逻辑基本和聚光灯也差不多，代码如下：  

    float3 ComputeAverageBlockerDepth_2DArray_Ortho(float index, TEXTURE2D_ARRAY(shadowMap), float sampleNumber,
        float searchWidthPercent, float3 positionSS, float4 depthParams, uint hash1, uint hash2, float2x2 rotation)
    {
        float d_Shading = positionSS.z;
        float ld_Shading = NonLinearToLinearDepth_Ortho(depthParams, d_Shading);
        float ald_Blocker = 0.0;
        float count = 1e-8; // avoid division by zero

        for (int i = 0; i < sampleNumber; i++)
        {
            float2 offset = mul(rotation, InverseSampleCircle(Sobol_Scrambled(i, hash1, hash2))) * 0.5;
            offset = offset * searchWidthPercent;
            float2 uv = positionSS.xy + offset;
            float d_Blocker = SampleShadowArray_Depth(uv, index, shadowMap, SHADOW_SAMPLER);
            float ld_Blocker = NonLinearToLinearDepth_Ortho(depthParams, d_Blocker);
            
            if (ld_Blocker < ld_Shading)
            {
                ald_Blocker += ld_Blocker;
                count += 1.0;
            }
        }
        ald_Blocker = ald_Blocker / count;
        return float3(ald_Blocker, count, ld_Shading);
    }

    float GetSunLightShadowAttenuation_PCSS(float3 positionWS, float3 normalWS, float3 L)
    {
        float cascadeIndex = ComputeCascadeIndex(positionWS);
        if (cascadeIndex >= GetSunLightCascadeCount()) return 1.0;
        float shadowStrength = GetSunLightShadowStrength();
        float shadowFade = 1.0;
        shadowFade *= ComputeDistanceFade(positionWS, GetMaxShadowDistance(), GetShadowDistanceFade());
        shadowFade *= ComputeCascadeEdgeFade(cascadeIndex, GetSunLightCascadeCount(), positionWS, GetCascadeEdgeFade(), GetCascadeCullingSphere(GetSunLightCascadeCount() - 1));

        float texelSize = GetCascadeCullingSphereRadius(cascadeIndex) * 2.0 / GetSunLightShadowArraySize();
        float searchWidthWS = GetSunLightSize();
        float searchWidthPercent = searchWidthWS / GetCascadeCullingSphereRadius(cascadeIndex) * 0.5;

        float3 positionWS_SearchBias = ApplyShadowBias(positionWS, GetSunLightShadowBias(), texelSize, searchWidthWS, normalWS, L);
        float3 positionSS_Search = TransformWorldToSunLightShadowCoord(positionWS_SearchBias, cascadeIndex);
        
        uint hash1 = Hash_Jenkins(asuint(positionWS));
        uint hash2 = Hash_Jenkins(asuint(positionSS_Search));
        float random = floatConstruct(hash1);
        float randomRadian = random * TWO_PI;
        float2x2 rotation = float2x2(cos(randomRadian), -sin(randomRadian), sin(randomRadian), cos(randomRadian));

        float4 depthParams = GetSunLightDepthParams(cascadeIndex);
        float blockerSampleNumber = GetSunLightBlockerSampleNumber();
        
        float3 blocker = ComputeAverageBlockerDepth_2DArray_Ortho(cascadeIndex, SUN_LIGHT_SHADOW_MAP, blockerSampleNumber, searchWidthPercent, positionSS_Search, depthParams, hash1, hash2, rotation);
        float ald_Blocker = blocker.x;
        float blockerCount = blocker.y;
        
        if (blockerCount < 1.0) return 1.0;

        float penumbraWS = GetSunLightPenumbraScale() * (blocker.z - ald_Blocker) * 0.01;
        float penumbraPercent = penumbraWS / GetCascadeCullingSphereRadius(cascadeIndex) * 0.5;
        
        float3 positionWS_FilterBias = ApplyShadowBias(positionWS, GetSunLightShadowBias(), texelSize, penumbraWS, normalWS, L);
        float3 positionSS_Filter = TransformWorldToSunLightShadowCoord(positionWS_FilterBias, cascadeIndex);
        float filterSampleNumber = GetSunLightFilterSampleNumber();
        float shadowAttenuation = ApplyPCF_2DArray(cascadeIndex, SUN_LIGHT_SHADOW_MAP, filterSampleNumber, penumbraPercent, positionSS_Filter, hash1, hash2, rotation);
        return lerp(1.0, shadowAttenuation, shadowStrength * shadowFade);
    }

实现效果如下（半影宽度实在太小了，故将 penumbra scale 设置为了 5）：  

<div  align="center">  
<img src="https://s2.loli.net/2025/02/28/ey3Xsu2o9gvrHmJ.jpg" width = "30%" height = "30%" alt="图83 - Directional（Sun） Light PCSS（Blocker Search 和 Filtering 样本数量都为 8）"/>
</div>

> 为了更好地控制半影宽度，最好也将 light size 乘上去：`float penumbraWS = GetSunLightPenumbraScale() * GetSunLightSize() * (blocker.z - ald_Blocker) * 0.01 * 10;`

## 其他说明
①循环采样问题：在动态循环或动态分支中使用 `Texture.Sample()` 会报错，解决方案是指定具体的 mipmap 去采样，即使用 `Texture.SampleLevel()`。原因是因为 sample 需要计算偏导数来决定采样哪个 level 的 mipmap，而由于偏导数的计算方式，其只有在可预测的控制流时才可以计算，即使用常量的控制流。  

②优化噪点问题：可以看到在上面展示的图片当中，噪点问题还是相当严重的。特别是在镜头运动的时候，噪点还会像电视机雪花一样闪烁。这个问题我打算在学习 **Temporal Anti-Aliasing TAA** 时处理。在上面的代码中，我已经使用低差异序列的 Sobol Sequence 做随机采样了，只不过 Sobol Sequence 是在 GPU 里计算的，要想进一步优化，可以直接用数组记录下 Sobol Sequence 的 4、8、16、32 个样本，然后根据关键字选择使用即可。至于随机旋转，可以使用一个分辨率比较小的 Blue Noise 贴图。  

③点光源的接缝问题：在点光源展示的图片中，能够较为明显的感受到接缝问题。但我感觉这个接缝问题并非由 cubemap 的接缝本身引起，而是由接缝两边不同 cubemap 面的半影宽度大小不均匀所产生的，而且接缝处若在面的中心位置，几乎看不到宽度差异，越接近边缘差异越明显。我推测原因是，透视投影导致的相同像素宽度但不同世界宽度的问题，之前没有考虑阴影接受平面和光源方向（阴影贴图方向）的角度问题。但目前没尝试出特别合适的解决方案，我在网上也查询了很久，只是有看到讨论和文章说传统阴影贴图技术存在这个问题，要尽量避免使用投射阴影的点光源，所以这个问题也只能先放着以后再处理了。  

④方向光（太阳光）的低顶点数的超长物体阴影宽度变化问题：这个问题只会在超长物体不在或部分不在摄像机视锥体的时候出现，因为部分顶点被裁切后，我们要解决 pancaking 问题，将顶点深度设置到了近裁切平面，导致了深度变化，也就导致了估算的阴影宽度也产生了变化。一个简单的解决方案就是超长物体尽量顶点多一些，顶点多了这个问题就不会很明显，其实顶点多了 pancaking 问题也不需要解决了。

# Post Processing
从这里就开始讲后处理了，后处理对于画面表现极为重要，个人认为重要性是比 PBR 还要高的。常见的后处理技术有：**泛光 Bloom**、**色调映射 Tone Mapping**、**色彩校正 Color Correction**、**色彩分级 Color Grading**、**景深 Depth of Field**、**运动模糊 Motion Blur**、**镜头光晕 Lens Flares** 等等。catlikecoding 教程中没有覆盖上述所有的后处理技术，强烈建议以后花足够的时间去学习并且优化效果。

另外，catlikecoding 教程没有使用 Unity 的专门用来处理后处理效果的 Volume 框架，只为后处理效果配置了一个全局的设置。而 Volume 框架不仅支持全局设置，也支持 per-scene 的设置和场景中一定范围内的局部设置。所以我后面也会使用这个 core RP library 里提供的 Volume 框架，本章节也主要讲如何在自定义管线中支持这个 Volume 框架，主要参考了 URP 的代码。

## Volume 框架
首先 Volume 框架相关 API 都在 Core RP Library -> Runtime -> Volume 里，有问题可以直接看里面的代码。首先后处理相关参数或属性都序列化在 Volume Profile 这个 ScriptableObject 里，可以通过 Assets -> Create -> Rendering -> Volume Profile 添加该资产。Volume Profile 又可以放置在 3 个地方，第一个叫做 the default volume for the whole project，第二个叫做 the global volume for the active quality level，第三个是 scene 里的 global 和 local volume。这三个的优先级是从低到高的，scene 里的 volume 覆盖 quality volume，quality volume 覆盖 default volume。

在 URP 中，default volume 是放置在 Project Settings > Graphics > URP > Default Volume Profile 里的。quality volume 放置在 URP Asset > Volumes > Volume Profile 里，而每个 Quality Level 又对应一个 URP Asset，实际上 SRP 也可以这么做。但是其实 default volume 和 quality volume 都是通过 `VolumeManager` 的 `Initialize(VolumeProfile globalDefaultVolumeProfile = null, VolumeProfile qualityDefaultVolumeProfile = null)` API 设置的。default volume 我觉得存在的意义不是很大，所以不打算传递给 `Initialize()` 函数。我在 RenderPipelineAsset 中新声明了 volumeProfile 字段，以便不同的 Quality Level 对应不同的全局后处理设置。`Initialize()` 这个函数必须在 RenderPipeline 的构造函数中调用：  

``` C#
public YRenderPipeline(YRenderPipelineAsset asset)
{
    ...
    VolumeManager.instance.Initialize(null, asset.volumeProfile);
    ...
}
```

### VolumeManager
**VolumeManager** 是一个对场景中 Volume 的设置进行跟踪管理的类。上面说过，场景中可以有 global volume 和 local volume，global volume 的影响范围是整个场景，local volume 的影响范围是它的 collider 组件的大小（触发器）。VolumeManager 会在运行时根据摄像机的位置对 global volume 和 local volume 的属性设置进行插值，保证不同 volume 之间的平滑过渡，而不是场景突然出现较大变化。

而插值的数据存储在 **VolumeStack** 里，这个 VolumeStack 我们一般不用去初始化，默认情况下 VolumeManager 里有一个 global volume stack，可以通过 `VolumeManager.instance.stack` 获取，它会自动管理并混合 volume 里的数据。只要调用过 `VolumeManager.instance.Initialize()`，这个 global volume stack 就会被自动创建。但是若想要对 VolumeStack 进行更多地控制，可以使用 `VolumeManager.instance.CreateStack()` 创建新的 VolumeStack，如何控制这里就不探讨了，下面还是使用默认的 global volume stack。

但是 VolumeStack 在我目前使用的版本中，即 Core RP Library 17.2.0，不会自动每帧更新，需要在 RenderPipeline 的 Render 函数中调用 `VolumeManager.instance.Update(Transform, LayerMask)` 方法，否则无法通过 VolumeStack 获取到序列化后的 Volume Profile 里的 VolumeComponent 的值，VolumeComponent 后面会讲。我不知道这个是不是版本的问题，因为我看网上有些老版本的自定义渲染管线中，并没有调用 Update 方法，所以一开始我以为 Unity 帮我们处理好了，然后就出现问题了，我老是获取不到序列化后的值，害得我找了老半天 bug。以后的版本中，这个 Update 方法需不需要自己调用，最好还是能留意一下。

``` C#
protected override void Render(ScriptableRenderContext context, List<Camera> cameras)
{
    ...
    foreach(Camera camera in cameras)
    {
        ...
        VolumeManager.instance.Update(camera.transform, 1);
        ...
    }
}
```

调用的时机自己注意一下，就不多说了。Update 方法的第一个参数 Transform 即通过位置来决定如何混合 volume 参数，所以传递进摄像机的位置属性即可，第二个 LayerMask 是用来控制 volume 的更新范围的，传递进去 1 即 Default Layer。

另外，别忘了在 RenderPipeline 的 `Dispose()` 函数中调用 `VolumeManager.instance.Deinitialize()` 来释放资源：  

``` C#
protected override void Dispose(bool disposing) 
{
    ...
    VolumeManager.instance.Deinitialize();
    ...
}
```

### VolumeComponent
**VolumeComponent** 就是我们在 Volume Profile 里通过 Add Override 添加的组件。我们可以自定义 VolumeComponent，以便存储后处理的相关属性。在 VolumeComponent 里的属性是通过 VolumeParameter 序列化的，一个基础的 VolumeComponent 如下：  

``` C#
[System.Serializable, VolumeComponentMenu("Post Processing/XXXX")]
[SupportedOnRenderPipeline(typeof(XXRenderPipelineAsset))]
public class XXXX : VolumeComponent, IPostProcessComponent
{
    public ClampedFloatParameter intensity = new ClampedFloatParameter(0f, 0f, 1f);
    public bool IsActive() => intensity.value > 0f;
}
```

SupportedOnRenderPipeline 特性和 IPostProcessComponent 接口不是必需的，IPostProcessComponent 只是提供了 IsActive() 方法。ClampedFloatParameter 就是 VolumeParameter 的一种，具体查看官方文档或直接查看源代码。VolumeComponent 内的属性我们可以通过 `VolumeManager.instance.stack.GetComponent<T>()` 获取，然后传递给具体的后处理 Shader。

## Copy
在正式进入后处理之前，我们先处理一个简单的 Copy Shader。因为我们要先将场景渲染至一个 render texture（自定义的 frame buffer）以便我们使用后处理 shader 对这个 render texture 进行后处理，最后再 blit 回 camera 的 frame buffer 以展现在屏幕上。所以可以把 Copy Shader 当作一个最简单的后处理效果，虽然 blit 就是 copy，但是 blit 效率相对低一点，后面会提到。

### Frame buffer
首先我们要创建一个新的 render texture，为了方便管理所有的 render texture ID，我创建了一个静态类，专门存放 ID，以便不同的渲染流程的类进行调用：

``` C#
public static class RenderTargetIDs
{
    public static readonly int k_FrameBufferId = Shader.PropertyToID("_CameraFrameBuffer");
    ...
}
```

然后在绘制几何体的类中（我的 SRP 在 ForwardGeometryNode 里）获取该 render texture 并设置为 render target：  

``` C#
data.context.SetupCameraProperties(data.camera);

data.buffer.GetTemporaryRT(RenderTargetIDs.k_FrameBufferId, data.camera.pixelWidth, data.camera.pixelHeight, 32, FilterMode.Bilinear, RenderTextureFormat.Default);
data.buffer.SetRenderTarget(new RenderTargetIdentifier(RenderTargetIDs.k_FrameBufferId), RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store);
data.buffer.ClearRenderTarget(true, true, Color.clear);
```

然后别忘了释放资源 `ReleaseTemporaryRT()`。可以看到上面的代码中直接 ClearRenderTarget 了，而不是根据第一篇文章说过的 CameraClearFlags 来判断是否清除 color buffer 和 depth buffer，这也导致了无法支持多摄像机渲染。如何同时支持多摄像机渲染和后处理，会在后面的章节中说明。然后就需要在处理后处理渲染的类中，将上述的 render texture 复制到 CameraTarget 中（目前还没有后处理相关 shader，先只复制），我新建了名为 PostProcessingNode 的类，专门处理后处理渲染，然后在 RenderPipeline 的 Render 函数中调用该类中处理渲染的方法。这里先直接使用 `CommandBuffer.Blit()` 函数：  

``` C#
public class PostProcessingNode : PipelineNode
{
    protected override void OnRender(YRenderPipelineAsset asset, ref PipelinePerFrameData data)
    {
        data.buffer.BeginSample("Post Processing");

        data.buffer.Blit(RenderTargetIDs.k_FrameBufferId, BuiltinRenderTextureType.CameraTarget);

        data.buffer.EndSample("Post Processing");
        data.context.ExecuteCommandBuffer(data.buffer);
        data.buffer.Clear();
        data.context.Submit();
    }
}
```

这样子就可以正常在 screen 看到画面了。

### Gizmos 和 Editor Preview
第一篇文章绘制 Gizmos 中提到过，Gizmo 有两个子集可以绘制，一个是 PreImageEffects，一个是 PostImageEffects。我们要分别在后处理前后调用：  

``` C#
protected override void OnRender(YRenderPipelineAsset asset, ref PipelinePerFrameData data)
{
#if UNITY_EDITOR
    if (Handles.ShouldRenderGizmos()) 
    {
        RendererList gizmosRendererList = data.context.CreateGizmoRendererList(data.camera, GizmoSubset.PreImageEffects);
        data.buffer.DrawRendererList(gizmosRendererList);
    }
#endif   

    data.buffer.BeginSample("Post Processing");
    ...
    data.buffer.EndSample("Post Processing");

#if UNITY_EDITOR
    if (Handles.ShouldRenderGizmos()) 
    {
        RendererList gizmosRendererList = data.context.CreateGizmoRendererList(data.camera, GizmoSubset.PostImageEffects);
        data.buffer.DrawRendererList(gizmosRendererList);
    }
#endif
    data.context.ExecuteCommandBuffer(data.buffer);
    data.buffer.Clear();
    data.context.Submit();
}
```

但是这样子所有的 gizmos 将不会被物体遮挡，如下图所示。这是因为 gizmos 的绘制依赖于原来的 frame buffer 中的深度信息，但是我们没有将其写入。这个问题也是后面的章节再处理。

<div  align="center">  
<img src="https://s2.loli.net/2025/03/11/w5PHXdvDeaEBAMR.png" width = "35%" height = "35%" alt="图84 - 左图：未被遮挡的 gizmos；右图：正确遮挡的 gizmos"/>
</div>

还有就是 Editor inspector 的 Preview 的绘制，比如 Material 的预览，以及 reflection probe 的绘制（refresh）也都是通过我们的管线绘制的。但是我们不能对它们也进行后处理，特别是 reflection probe。我们可以通过 [CameraType](https://docs.unity3d.com/ScriptReference/CameraType.html) 来判断是否跳过后处理阶段，`CameraType.Preview` 就是用于绘制 preview 的摄像机，`CameraType.Reflection` 就是用于绘制 reflection probe 的摄像机。

``` C#
protected override void OnRender(YRenderPipelineAsset asset, ref PipelinePerFrameData data)
{
    ... // GizmoSubset.PreImageEffects

#if UNITY_EDITOR
    if (data.camera.cameraType > CameraType.SceneView)
    {
        data.buffer.Blit(RenderTargetIDs.k_FrameBufferId, BuiltinRenderTextureType.CameraTarget);
        return;
    }
#endif

    ... // Post Processing
}
```

当然你也可以通过一个 IsActive 的 bool 值来控制是否激活后处理，未激活的情况下，就跳过创建一个新的 framebuffer render texture。或者单独为 preview 的绘制和 reflection probe 的绘制搞出一个渲染路径出来。

还有就是 scene 窗口有个按钮可以选择关闭后处理效果，如下图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/11/bJTf51z2qxkgtRU.jpg" width = "35%" height = "35%" alt="图85 - toggle post-processing in the scene window"/>
</div>

如果要支持这个功能，新增代码如下：  

``` C#
#if UNITY_EDITOR
    if (data.camera.cameraType > CameraType.SceneView)
    {
        data.buffer.Blit(RenderTargetIDs.k_FrameBufferId, BuiltinRenderTextureType.CameraTarget);
        return;
    }

    if (data.camera.cameraType == CameraType.SceneView && !SceneView.currentDrawingSceneView.sceneViewState.showImageEffects)
    {
        data.buffer.Blit(RenderTargetIDs.k_FrameBufferId, BuiltinRenderTextureType.CameraTarget);
        return;
    }
#endif
```

### Copy Shader
`CommandBuffer.Blit()` 函数之所以效率相对低一点，是因为它绘制了一个屏幕的长方形，即两个三角形。Blit 本质上是传递了一个长方形 mesh 给 GPU，并将源纹理作为贴图绘制出来。而我们可以调用 `CommandBuffer.DrawProcedural()` 只绘制一个三角形进行复制，并且不用 bind vertex 或者 index buffer，而是依赖系统值（System Value，SV），如 SV_VertexID，来生成顶点数据。`Blit()` 还有一个缺点就是会重复绘制长方形对角线的像素，因为绘制了两个三角形，其使用的 shader 是 built-in shader 里面的 Internal-BlitCopy。

我们需要为 DrawProcedural 传递一个自己的 copy material，首先创建一个 Copy Shader，引入 CopyPass.hlsl，如下。注意所有的 pass 应该都不做背面剔除以及深度写入：  

    Shader "Hidden/YPipeline/Copy"
    {
        SubShader
        {
            Tags { "RenderType" = "Opaque"}
            
            ZTest Always
            ZWrite Off
            Cull Off

            Pass
            {
                Name "Copy"
                
                HLSLPROGRAM
                #pragma target 3.5
                
                #pragma vertex CopyVert
                #pragma fragment CopyFrag

                #include "CopyPass.hlsl"
                ENDHLSL
            }
        }
    }

在 Pass 中我们要利用 `SV_VertexID` 语义来生成顶点坐标和 uv 坐标，让我们的三角形覆盖整个裁切空间（后面传入的 vp 矩阵是单位矩阵，所以顶点坐标和裁切坐标是一致的），具体如下图所示：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/12/hWa843I6lSZQVjL.png" width = "25%" height = "25%" alt="图86 - Triangle covering clip space"/>
</div>

三个顶点的坐标分别是 (-1, -1, z, 1)，(-1, 3, z, 1)，(3, -1, z, 1)。uv 坐标分别是 (0, 0)，(0, 2), (2, 0)。注意顶点的**绕序 winding order**，顺时针方向为正面。顶点坐标的 z 要根据是否 reversed-z 来判断是 0 还是 1。教程中是通过三元运算符计算坐标的，如下：  

    OUT.positionHCS = float4(vertexID <= 1 ? -1.0 : 3.0, vertexID == 1 ? 3.0 : -1.0, 0.0, 1.0);
    OUT.uv = float2(vertexID <= 1 ? 0.0 : 2.0, vertexID == 1 ? 2.0 : 0.0);

但是 URP 里的方式更好，通过位运算符计算出 uv 坐标，再映射回顶点坐标，整个 Pass 代码如下：  

    #ifndef YPIPELINE_SIMPLE_COPY_PASS_INCLUDED
    #define YPIPELINE_SIMPLE_COPY_PASS_INCLUDED

    #include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
    #include "../../ShaderLibrary/UnityInput.hlsl"

    TEXTURE2D(_BlitTexture);
    SAMPLER(sampler_LinearClamp);

    struct Varyings
    {
        float4 positionHCS  : SV_POSITION;
        float2 uv           : TEXCOORD0;
    };

    Varyings CopyVert(uint vertexID : SV_VertexID)
    {
        Varyings OUT;
        
        // OUT.positionHCS = float4(vertexID <= 1 ? -1.0 : 3.0, vertexID == 1 ? 3.0 : -1.0, 0.0, 1.0);
        // OUT.uv = float2(vertexID <= 1 ? 0.0 : 2.0, vertexID == 1 ? 2.0 : 0.0);

        OUT.uv = float2((vertexID << 1) & 2, vertexID & 2);
        OUT.positionHCS = float4(OUT.uv * 2.0 - 1.0, UNITY_NEAR_CLIP_VALUE, 1.0);
        
        if (_ProjectionParams.x < 0.0) OUT.uv.y = 1.0 - OUT.uv.y;

        // #if UNITY_UV_STARTS_AT_TOP
        //     OUT.uv.y = 1.0 - OUT.uv.y;
        // #endif
        
        return OUT;
    }

    float4 CopyFrag(Varyings IN) : SV_TARGET
    {
        return SAMPLE_TEXTURE2D_LOD(_BlitTexture, sampler_LinearClamp, IN.uv, 0);
    }

    #endif

上面代码中还要注意一个 uv 坐标的平台差异问题，在 DirectX 中 uv 的原点在屏幕的左上角。一般情况下，Unity 会在背后为我们处理了这个反转问题，但是对于 render texture 并没有处理。我们可以通过 `_ProjectionParams` 的 x 分量判断是否需要手动反转，并且该向量需要定义在 UnityInput.hlsl 文件里面。URP 中是通过 `UNITY_UV_STARTS_AT_TOP` 宏来进行判断的，但是我使用的时候不知道为什么，还是会出现反转问题，而且是复制给 `BuiltinRenderTextureType.CameraTarget` 的环节出现的问题。我感觉原因是 `_ProjectionParams` 会同时判断 Unity 是否在背后处理了反转问题，而 `UNITY_UV_STARTS_AT_TOP` 不会，而 URP 使用了 RTHandle API，可以绕过 Unity 对 `BuiltinRenderTextureType.CameraTarget` 的自动反转问题处理。我不确定上述原因是否正确，打算后面全面转为使用 RTHandle API 后再回来解决这个问题。故目前先使用 `_ProjectionParams` 的 x 分量进行判断。

---

有了 Copy Shader 后，就可以进行复制了，我们可以新建一个 BlitUtility 的静态类，专门用于存放这类函数：

``` C#
public static class BlitUtility
{
    private static readonly int k_BlitTextureId = Shader.PropertyToID("_BlitTexture");

    public static void BlitTexture(CommandBuffer cmd, int sourceID, int destinationID, Material material, int pass)
    {
        cmd.SetGlobalTexture(k_BlitTextureId, new RenderTargetIdentifier(sourceID));
        cmd.SetRenderTarget(new RenderTargetIdentifier(destinationID));
        cmd.DrawProcedural(Matrix4x4.identity, material, pass, MeshTopology.Triangles, 3);
    }

    public static void BlitTexture(CommandBuffer cmd, int sourceID, BuiltinRenderTextureType destination, Material material, int pass)
    {
        cmd.SetGlobalTexture(k_BlitTextureId, new RenderTargetIdentifier(sourceID));
        cmd.SetRenderTarget(new RenderTargetIdentifier(destination));
        cmd.DrawProcedural(Matrix4x4.identity, material, pass, MeshTopology.Triangles, 3);
    }
}
```

首先将 source texture 设置为 copy shader 里的 `_BlitTexture`，再将 destination texture 设置为 RenderTarget，最后使用 `DrawProcedural()` 将 source texture 绘制在 destination texture，传递的矩阵为单位矩阵，**图元拓扑 primitive topology** 为三角形，以及要绘制的顶点数量为 3。然后在 PostProcessingNode 的 `OnRender()` 函数中将 `Blit()` 函数改为我们的函数（Pass 传入 0）即可，就是要先初始化 copy material：  

``` C#
private const string k_Copy = "Hidden/YPipeline/Copy";
private Material m_CopyMaterial;

public override void Initialize()
{
    m_CopyMaterial = new Material(Shader.Find(k_Copy));
}

protected override void OnRender(YRenderPipelineAsset asset, ref PipelinePerFrameData data)
{
    ...
    data.buffer.BeginSample("Post Processing");
    // data.buffer.Blit(RenderTargetIDs.k_FrameBufferId, BuiltinRenderTextureType.CameraTarget);
    BlitUtility.BlitTexture(data.buffer, RenderTargetIDs.k_FrameBufferId, BuiltinRenderTextureType.CameraTarget, m_CopyMaterial, 0);
    data.buffer.EndSample("Post Processing");
    ...
}
```

# Bloom and HDR
在进入 HDR Bloom 之前，先讲解一下制作 Bloom 特效的基本原理，并实现一个 LDR Bloom。然后再从物理法则中深入理解 Bloom 产生的原因，并实现相对更加真实的 HDR Bloom。

## 制作 Bloom 基本原理
制作 Bloom 的流程总结起来就一句话，就是将一张图片的亮度高于一定阈值的区域拿出来，进行模糊，再叠加到原图上去，这样就可以实现 Bloom 了。这点其实[《Unity Shader入门精要》读书笔记（四）](https://ybniaobu.github.io/2023/12/19/2023-12-19-UnityShader4/#Bloom-%E6%95%88%E6%9E%9C) 里面也提到过，下面的做法跟入门精要里面也是类似的。

①将一张图片的亮度高于一定阈值的区域拿出来的过程，一般称为 **prefilter**，在该过程也可以将该区域放在一个分辨率更小的贴图当中，从而决定开始模糊的金字塔层级（详见下采样）。但是直接消除一定亮度以下的颜色会产生一个较为明显的边界，而我们希望一个逐渐变化的过渡。所以我们会给图片中的颜色乘上一个一个权重系数，这个权重系数由**阈值 Threshold** 与亮度计算而得：  

$$ \omega = \cfrac {max(0, b - t)} {max(b, 0.00001)} $$

b 即亮度 brightness，t 即阈值 threshold。我们一般会取 RGB 中最大的通道作为亮度值（我试了下 YUV 颜色空间的 Luminance 作为亮度，效果不是很好，特别是蓝色部分就不太会产生 bloom 效果）。上述公式的图像如下，因为形状像膝盖，称为 **knee curve**：

<div  align="center">  
<img src="https://s2.loli.net/2025/03/12/Ysc4O7XBV5PKCDR.png" width = "25%" height = "25%" alt="图87 - knee curve，从黄色到紫色 Thresholds 分别为 0.25, 0.5, 0.75, 1"/>
</div>

可以看到，亮度到达 threshold 后，权重就会逐渐变高。虽然这个曲线比直接去除 threshold 以下的颜色要平滑一些，但是仍然有一个明显的 cutoff point，故该曲线也被称为 **hard knee**。为了更好地平滑，需要引入另外一个参数 **thresholdKnee**，公式如下：  

$$ s = \cfrac {min( max(0, b - t + tk), 2tk)^2} {4tk + 0.00001} $$

其中 k 即 thresholdKnee，是一个 0 - 1 区间的值。图像如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/03/12/UJBoN3GjQEz8dCY.png" width = "25%" height = "25%" alt="图88 - Threshold 为 1 时，从黄色到黑色 Threshold Knee 分别为 0, 0.25, 0.5, 0.75, 1"/>
</div>

有了这两个参数，就可以更好地取出较亮的区域以便进行模糊处理。

②模糊的过程称为**下采样 Downsample**，常用于 Bloom 的模糊算法有**高斯模糊 Gaussian Blur**、**方框模糊 Box Blur**、**Kawase Blur** 以及 **Dual Kawase Blur**。我看有些文章说 Kawase Blur 或 Dual Kawase Blur 具有更加优秀的性能优势，但另外有些文章实现出来说有特别重的方块感。而高斯模糊更加常用，并且 UE 和 Unity 都采用高斯模糊，我觉得基本上实现基于高斯模糊的 Bloom 就够用了，其他模糊可以日后有空再尝试研究，下面的讲解也是基于高斯模糊。这个过程之所以称为下采样，是因为最简单的模糊方式就是将图片复制进其一半分辨率的图片（即 mipmap）当中，并采用 bilinear 的采样方式。我们会重复这个过程，迭代一定的次数，下采样到一定的级别，从而形成一个 texture 的金字塔，称为 **Bloom Pyramid**。当然 bilinear 的下采样的结果会导致严重的方块感，所以在下采样时会使用高斯模糊。

<div  align="center">  
<img src="https://s2.loli.net/2025/03/12/DPeiWHUuMYqCaoQ.png" width = "25%" height = "25%" alt="图89 - Bloom Pyramid"/>
</div>

③将所有 Bloom Pyramid 中模糊后的贴图叠加到原图上去的过程称为**上采样 Upsample**。

## LDR Bloom

## HDR

## HDR Bloom
