---
title: 屏幕空间环境光遮蔽（一）SSAO
date: 2025-08-01 12:54:23
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2025/08/01/287gfDiMYRFrmUs.gif
mathjax: true
description: 本文章主要内容有环境光遮蔽的理论基础；八面体法线压缩算法以及如何从深度重构法线；Normal-Oriented SSAO 算法的实现；使用 Spatial Filter 和 Temporal Filter 对 AO 降噪；在半分辨率下计算 AO 优化性能。
---

> 终于要进入**全局光照 Global Illumination** 的领域了，之前 YPipeline 能够使用的全局光照技术基本只有 **IBL** 和 **Light Map**。Unity 的 Progressive Lightmapper 在离线烘焙 Light Map 时理论上应该隐式地包含了因遮挡导致的环境光减弱的 AO 效果，所以若使用了 Light Map 理论上来说是不需要使用 SSAO 的，当然具体还得看美术需求。Progressive Lightmapper 也提供了 AO 的烘焙，可以额外开启加强 AO 效果。但是若只使用 IBL (Light Probe)，它只为场景添加了一个整体的颜色或者均匀的光照，但是没有考虑因遮挡导致的暗处或光线衰减，所以整体呈现的效果会让玩家在一定程度上感到不真实或不可信。而**环境光遮蔽 Ambient Occlusion (AO)** 通过描绘物体之间由于遮挡而产生的阴影可以更好地增强场景的可信度和立体感，是一个非常不错的补充 IBL 环境光的方式。IBL (Light Probe) + SSAO 的组合可以说是一种较为廉价的全局光照效果，一般对于小作坊的简陋的 3D 游戏来说肯定是足够了的。  
> 
> 其实我原来并不想先补充学习屏幕空间环境光遮蔽相关技术，想直接一步到位进入 SSGI 的学习，但是我后来看到 https://github.com/cdrinmatane/SSRT3 实现的 SSGI 借鉴了 GTAO 的思想，实现效果看得我非常心动，所以为了巩固基础，先实现 SSAO 相关算法。

# Ambient Occlusion 介绍
> 以下内容主要参考了《Real-Time Rendering, Fourth Edition》的 Chapter 11 Global Illumination。  

**环境光遮蔽 Ambient Occlusion** 最早是由**工业光魔**（又是你乔治卢卡斯）在电影《珍珠港》中为提高计算机生成飞机的环境光照质量所开发的。尽管环境光遮蔽是对光照物理效果进行了很大程度地简化，但是所呈现的效果往往能极大地增加可信度。当光源较为均匀，无法展现物体细节时，这种廉价方法可以提供对于物体形状的视觉暗示。

<div align="center">  
<img src="https://s2.loli.net/2025/08/24/GsMnljdwgZOqySr.png" width = "70%" height = "70%" alt="图1 - B25 bomber, Spinosaurus and Tyrannosaurus, Example Ambient Occlusion images. c©Lucas Digital Ltd. LLC."/>
</div>

## 理论基础
量化环境光遮蔽的理论背景就是反射方程，相关知识忘了详见[《GAMES101-图形学入门公开课笔记（二）》](https://ybniaobu.github.io/2024/04/23/2024-04-23-GAMES_101_2/)第十四课和第十五课。因为环境光遮蔽估计的是整个半球内的光照可见性，所以要基于表面 irradiance ($\,E\,$) 计算，表面 irradiance 是所有入射 radiance 的余弦加权积分。同时假设所有入射方向 $\,\omega_{i}\,$ 或 $\,l\,$ 上的 radiance 都是相同且恒定的，这样子计算表面 irradiance，即 $\,E(p)\,$ ，的方程和积分结果如下：  

$$ E(p) = \int_{\Omega} L_i (n \cdot l) dl = \pi L_i $$

上述方程没有考虑可见性问题，在着色点半球范围内的某些方向，可能会被自身物体的其他部分或者是场景中的其他物体所遮挡。为了简单起见，我们假设来自这些遮挡方向上的入射 radiance 为零，这个假设忽略了场景中可能会被其他物体反弹，并最终从这些遮挡方向到达点 p 的光线，但是它极大地简化了推理过程。基于上述假设，补充可见性后，可以得到下述方程：  

$$ E(p) = L_i \int_{\Omega} v(p,l) (n \cdot l) dl = k_{AO}(p) \pi L_i $$

$\,v(p,l)\,$ 是一个**可见性函数**，如果从点 p 向方向 l 投射的光线会被物体遮挡，则该函数值为 0，反之为 1。而 $\,k_{AO}(p)\,$ 则为**环境遮挡系数**，它的范围位于 \[0, 1\] 内，根据上述公式，环境遮挡系数可以被量化为：  

$$ k_{AO}(p) = \cfrac{1}{\pi} \int_{\Omega} v(p,l) (n \cdot l) dl $$

注意，表面的朝向会影响到 $\,k_{AO}(p)\,$ 的数值，因为可见性函数 $\,v(p,l)\,$ 会被余弦加权。若某表面大部分未遮挡区域都位于其表面法线的两端，因此该位置的余弦因子相对较小，$\,k_{AO}(p)\,$ 也会相对较小。还有一点，环境遮挡系数的名称可能会造成一定的误解，它本质上应该是可见性系数，因为它是对可见性函数（未遮挡为 1）的余弦加权积分。

### 环境法线 Bent Normal
除了环境遮挡系数，还有个概念叫做**环境法线 Bent Normal**，首先它是个方向向量，且是未遮挡方向的余弦加权平均值。在计算环境光着色时，可以使用它来代替几何法线，从而提供更加准确的全局光照结果，同时不需要额外的性能开销。注意，环境法线包含了遮挡信息，使用环境法线采样全局光照（IBL）后，理论上不应该再使用其他环境光遮蔽算法，当然实际上还得看美术需求。环境法线的计算公式如下：  

$$ n_{bent} = \cfrac { \int_{\Omega} l\cdot v(p,l) (n \cdot l) dl }{ ||\int_{\Omega} l\cdot v(p,l) (n \cdot l) dl|| } $$

其中符号 $\,||x||\,$ 是向量 x 的长度，积分的结果再除以它自身的长度，即归一化向量。

### Obscurance
环境遮挡系数的可见性函数方法对于封闭的几何体是不起作用的，因为对于封闭的几何体内部的物体来说，来自表面的所有射线都会击中该封闭的空间。所以有人提出了 Obscurance 思想，即通过**距离映射函数** $\,\rho(l)\,$ 来替代可见性函数 $\,v(l)\,$，从而对环境光遮蔽的计算进行了修改：  

$$ k_{AO}(p) = \cfrac{1}{\pi} \int_{\Omega} \rho(l) (n \cdot l) dl $$

距离映射函数是一个连续的函数，其返回值取决于射线与表面相交之前所传播的距离，当相交距离为 0 时，$\,\rho(l)\,$ 的值为 0；当相交距离大于设定的最大距离，或者没有相交时，$\,\rho(l)\,$ 的值为 1。Obscurance 在物理上是不正确的。然而，Obscurance 通常可以给出符合期望的合理结果。

### 相互反射
环境光遮蔽与完整全局光照之间的一个重要区别是**相互反射 interreflection**，所以相对于完整全局光照模拟产生的结果，会更暗一些。Occlusion 假设被遮挡方向上的 radiance 为零，但是实际上相互反射会为这些方向引入一个非零的 radiance。使用 Obscurance 距离映射函数来代替可见性函数也可以缓解这个问题，因为 Obscurance 函数的值通常会大于零。

以一种更加精确的方式来追踪相互反射是很昂贵的，因为它需要求解一个递归问题。想要给一个点进行着色，必须首先对其他点进行着色，以此类推。因此有人提出了一种廉价但较为准确的方法来近似相互反射，它基于在漫反射光照下对 Lambertian 场景的观察，即从一个给定位置能够看见的表面，往往会具有相似的 radiance。假设遮挡方向的 radiance $\,L_i\,$，等于当前着色点的出射 radiance $\,L_o\,$，从而打破递归，可以得到这样一个解析表达式：  

$$ E = \cfrac {\pi k_{AO}}{1 - \rho_{ss}(1 - k_{AO})} L_i $$

其中 $\,\rho_{ss}\,$ 是次表面反照率（漫反射率），上述表达式相当于使用了新的环境遮挡因子 $\,k'_{AO}\,$ 来替代之前的 $\,k_{AO}\,$ ：  

$$ k'_{AO} = \cfrac {k_{AO}}{1 - \rho_{ss}(1 - k_{AO})} $$

上式倾向于让环境遮挡因子变得更大（更亮），从而使得它在视觉上更加接近一个完整全局光照所产生的结果，它在一定程度上模拟了相互反射效应。

### 计算环境光遮蔽
环境遮挡因子的计算可能会很耗时，可以在渲染之前离线计算，预计算任何与光照相关的信息（包括环境光遮蔽），这个过程通常被称为**烘焙 Baking**，预计算环境光遮蔽最常见的方法就是蒙特卡罗方法。除了直接预计算环境光遮蔽的结果，对于静态场景还可以先预先计算环境遮挡因子 $\,k_{AO}\,$ 和环境法线 $\,n_{bent}\,$。遮挡数据对于物体上的每个顶点都是唯一的，它们通常会存储在纹理、体积或者网格顶点中。可以将环境遮挡因子和环境法线（可选）存储一个三维网格中，称为**环境光遮蔽体 ambient occlusion volume**。这类方法的计算成本较低，因为可以从纹理中直接读取出环境遮挡因子，不用实时计算。

但上述方法只适用于静态场景，对于动态场景，还是要实时计算这些参数，实际计算的方式，按照空间可以划分为两类：在**世界空间**中执行的方法和在**屏幕空间**中执行的方法。世界空间中执行的方法跟全局光照技术类似，比如符号距离场 SDF、稀疏体素八叉树 Sparse Voxel Octree 等等，世界空间的方法不是本篇文章的重点，而且对于现在来说，真的要实现，可以直接和全局光照技术一起实现。

在**屏幕空间**中执行的方法中，最出名的当属**屏幕空间环境光遮蔽 screen-space ambient occlusion (SSAO)** 算法，它最早是由 Crytek 在 SIGGRAPH 2007 的演讲 <a href="https://www.realtimerendering.com/advances/s2007/Mittring-Finding_NextGen_CryEngine2(Siggraph07).pdf">Finding Next Gen CryEngine2</a> 中提出，为它的孤岛危机 Crysis 所开发，最早该技术使用 z-buffer 作为唯一的输入，来估计环境遮挡因子 $\,k_{AO}\,$，该方法会将球形范围内的所有样本都考虑在内，而不是只考虑表面上半球范围内的样本，并且没有考虑余弦因子，所以该方法产生的 AO 效果相对来说不太正确，但尽管如此，最终产生结果在视觉上还是令人较为满意的。

<div align="center">  
<img src="https://s2.loli.net/2025/08/24/5PpT7VmQriSXh16.png" width = "50%" height = "50%" alt="图2 - Crytek 的 SSAO，球内样本所对应的深度，超过了 z-buffer 中对应位置的深度，则被认为被遮挡，即图中红色点。环境遮挡因子是通过测试的样本数与总样本数的加权比值。"/>
</div>

Crytek 的方法可以被解释为蒙特卡洛积分，计算出来的值可以称为 **Volumetric Obscurance**，可以被定义为：  

$$ \int_{\chi} d(x) o(x) dx $$

其中 $\,\chi\,$ 是围绕该像素点的一个三维球形邻域，$\,d(x)\,$ 是距离映射函数，$\,o(x)\,$ 是占用函数（occupancy function），即 0 和 1。另外，距离映射函数对于最终视觉质量的影响很小，因此可以使用常数。Szirmay-Kalos 等人据此提出了 [Volumetric Ambient Occlusion](https://cg.iit.bme.hu/~szirmay/ambient8.pdf)，它将积分转换到一个球面上，而不是在一个半球上；这个球体的半径为半球的一半，并且会沿着法线移动一个球体半径的距离，最终这个球体会与半球内接，被半球完全包裹。

之后 NVIDIA 的 Louis Bavoil 和 Miguel Sainz 提出了一个不同的方法，在 SIGGRAPH 2008 的演讲 [Image-Space Horizon-Based Ambient Occlusion](https://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf) 提出了基于视界的环境光遮蔽，即大名鼎鼎的 **Horizon-Based Ambient Occlusion (HBAO)**，它假设 z-buffer 中的数据表示了一个连续的高度场。通过确定**视界角 horizon angle**，可以对像素点的可见性进行估计，这里的视界角，指的是切面上方被邻域遮挡的最大角度。也就是说，对于某个点上的给定方向，我们会记录最高的可见物体所对应的角度。

再之后，受到上述算法的启发，动视 Activision 的 Jorge Jiménez 在 SIGGRAPH 2016 的演讲 [Practical Real-Time Strategies for Accurate Indirect Occlusion](https://blog.selfshadow.com/publications/s2016-shading-course/activision/s2016_pbs_activision_occlusion.pptx) 提出了 **Ground-truth Ambient Occlusion (GTAO)**，还可以看他们的论文：[链接一](https://www.activision.com/cdn/research/Practical_Real_Time_Strategies_for_Accurate_Indirect_Occlusion_NEW%20VERSION_COLOR.pdf)，[链接二](https://www.activision.com/cdn/research/PracticalRealtimeStrategiesTRfinal.pdf) 。HBAO 在计算遮挡的时候并不包括余弦项，并且它还增加了一个特殊的衰减，因此它的结果最多只能与光线追踪相接近，但是始终还是不一样的。GTAO 引入了缺失的余弦因子，去除了这个特殊的衰减函数，并在绕观察向量的参考系中给出了遮挡积分。

之后会详细讲解 **SSAO**、**HBAO**、**GTAO** 这三种算法，其实在屏幕空间实现的 AO 算法都可以称为 SSAO，所以在文章中提到的 SSAO 需要自行判断是特指 Crytek 的 SSAO 算法还是广义上的 SSAO。

> 其他关于 SSAO 相关算法的文章或演讲有：  
> ① A Comparative Study of Screen-Space Ambient Occlusion Methods ：https://www.gamedevs.org/uploads/comparative-study-of-ssao-methods.pdf ；  
> ② NIVDIA 的 Multi-Layer Dual-Resolution Screen-Space Ambient Occlusion（SIGGRAPH 2009）：https://developer.download.nvidia.com/presentations/2009/SIGGRAPH/Bavoil_MultiLayerDualResolutionSSAO.pdf ；  
> ③ NIVDIA 的 The Alchemy Screen-space Ambient Obscurance Algorithm ：https://research.nvidia.com/publication/2011-08_alchemy-screen-space-ambient-obscurance-algorithm ；  
> ④ NIVDIA 的 Scalable Ambient Obscurance ：https://research.nvidia.com/publication/2012-06_scalable-ambient-obscurance ；  
> ⑤ Efficient Screen-Space Approach to High-Quality Multi-Scale Ambient Occlusion：https://www.comp.nus.edu.sg/~lowkl/publications/mssao_visual_computer_2012.pdf ；  
> ⑥ Real-time Raytracing and Screen-space Ambient Occlusion：https://www.diva-portal.org/smash/get/diva2:1337203/FULLTEXT01.pdf ，这篇文章比较了很多 SSAO 算法，建议观看。


## 着色中的运用
### Diffuse Occlusion
我们计算出环境遮挡系数 $\,k_{AO}\,$ 之后，如何在着色光照计算中进行运用呢？首先只讨论**漫反射遮蔽 Diffuse Occlusion**，回顾反射方程，假设漫反射 Lambertian 表面，使用 Lambertian BRDF 作为反射方程的 $\,f_r(l, v)\,$，并补充上可见性函数 $\,v(l)\,$：  

$$ L_o = \int_{\Omega} \cfrac {\rho_{ss}}{\pi} L_i v(l) (n \cdot l) dl = \cfrac {\rho_{ss}}{\pi} \int_{\Omega} L_i v(l) (n \cdot l) dl $$

从中拆分出 $\,k_{AO}\,$ 项：  

$$ \begin{align*} L_o &= \cfrac {\rho_{ss}}{\pi} \int_{\Omega} L_i v(l) (n \cdot l) dl \\ &= \cfrac {\rho_{ss}}{\pi} \cfrac {\int_{\Omega} L_i v(l) (n \cdot l) dl}{\int_{\Omega} v(l) (n \cdot l) dl} \int_{\Omega} v(l) (n \cdot l) dl \\ &= \rho_{ss} k_{AO} \cfrac {\int_{\Omega} L_i v(l) (n \cdot l) dl}{\int_{\Omega} v(l) (n \cdot l) dl} \end{align*} $$

接下来做出了一个大胆的假设，就是忽略了可见性函数，忽略可见性是一个影响很大的近似操作，所产生的阴影没有任何预期的方向性，也就是说，它们看起来并不像是由特定光源产生的，比如精确光源。那么上面公式可以进一步简化为（余弦在单位半球上积分为 $\,\pi\,$）：  

$$ \begin{align*} L_o &\approx \rho_{ss} k_{AO} \cfrac {\int_{\Omega} L_i (n \cdot l) dl}{\int_{\Omega} (n \cdot l) dl} \\ &= \cfrac {\rho_{ss}}{\pi} k_{AO} \int_{\Omega} L_i (n \cdot l) dl \\ &= k_{AO} \int_{\Omega} \cfrac {\rho_{ss}}{\pi} L_i (n \cdot l) dl \end{align*} $$

$\,k_{AO}\,$ 后面的部分就是漫反射 BRDF 计算的 IBL 环境光积分，忘了回去看 [IBL 基于图像的光照（一）](https://ybniaobu.github.io/2024/07/09/2024-07-09-IBL_Basics1/#%E6%BC%AB%E5%8F%8D%E5%B0%84-BRDF-%E7%A7%AF%E5%88%86)。这意味着，可以直接通过计算 irradiance，即 IBL，并将其乘上环境光遮蔽值来完成环境光遮蔽的效果着色。

### Specular Occlusion
到目前为止，我们都是假设处理的是 Lambertian BRDF，对于**镜面反射遮蔽 Specular Occlusion** 来说，直接乘以 $\,k_{AO}\,$ 项可能无法产生可信的结果，因为此时除了可见性和法线，还取决于观察方向。同时，相比于漫反射的半球波瓣，镜面反射的 BRDF 波瓣也更窄。

寒霜引擎在它的 [Moving Frostbite to Physically Based Rendering 3.0](https://media.contentapi.ea.com/content/dam/eacom/frostbite/files/course-notes-moving-frostbite-to-pbr-v32.pdf) (SIGGRAPH 2014) 中提到过一个近似方法来模拟 GGX 波瓣的镜面反射遮蔽效果，对于粗糙表面来说，下述公式得到的 AO 值几乎是没有修改的，但对于光滑表面来说，观察方向越接近法线，ao 效果越低，即值越高，越接近掠射角，ao 效果越大，即值越低：  

    float computeSpecOcclusion ( float NdotV , float AO , float roughness )
    {
        return saturate (pow( NdoV + AO , exp2 ( -16.0 f * roughness - 1.0 f )) - 1.0 f + AO );
    }

具体如何应用，我在 [Custom Better PBR in Unity](https://ybniaobu.github.io/2024/10/22/2024-10-22-BetterPBR1/#Ambient-Occlusion) 文章中的 Ambient Occlusion 中有详细说明过，就不再赘述了。

而动视 Activision 在 GTAO 中还提出了 **Ground Truth Based Specular Occlusion (GTSO)**，使用了类似于分割求和近似 Split Sum Approximation（由 Epic Games 在 SIGGRAPH 2013 的 [Real Shading in Unreal Engine 4](https://cdn2-unrealengine-1251447533.file.myqcloud.com/Resources/files/2013SiggraphPresentationsNotes-26915738.pdf) 中提出）的方法，将 Visibility 项也拆分了出来，这个之后学习 GTAO 时会详细说明。

# Normal Buffer
因为 SSAO 算法需要使用到场景法线信息，在正式进入 SSAO 之前，先了解一下获取法线信息的方式，基本上有两张方式：①通过 Normal Buffer；②通过 Depth Buffer 重建法线信息。

我们先讲额外开 Normal Buffer 的方式，对于延迟管线，GBuffer 肯定是要存储法线信息的，不需要额外处理，对于前向管线，则需要额外开一张 Normal Buffer（和其他前向管线需要使用的场景信息一起又可称为 **Thin G-Buffer**），可以和 Depth Prepass 一起输出。我目前的前向管线参考了 Unity HDRP 前向管线的方案，开了格式为 R8G8B8A8_UNORM 的 Normal Buffer，使用**八面体法线编码 Octahedral Normal Vectors** 将压缩好的两个 float 打包进了 R8G8B8 三个通道，A 通道存储 Rougness 以便后续的 SSR 使用。

> 我看到 DOOM (2016) 的前向管线是 R16G16 + R8G8B8A8 的方式，R16G16 应该是八面体压缩法线后直接存储（猜测，不是很确定，总不至于是 xy only 的存储方式吧），R8G8B8A8 的前三个通道存储了 F0，A 通道存储了 Rougness。而 DOOM Eternal 的前向管线甚至没开 Thin G-Buffer，直接将 SSR 计算放在了 Forward Uber Shader 中，而不是使用 Compute Shader 计算，这点非常神奇。绝大多数的延迟管线游戏的 Normal Buffer，我看到是以 R10G10B10A2_UNORM 不压缩的形式存储前三个通道为主，少数 R8G8B8A8_UNORM，追求精度则是 R16G16_UNORM。从精度来看，应该是 Octahedron 32 > Octahedron 24 > UNORM 10×3 > Octahedron 20 的。关于各种压缩存储 Normal 精度问题可以详看这篇文章：[A Survey of Efficient Representations for Independent Unit Vectors](https://jcgt.org/published/0003/02/01/) 。

下面聊一下 Normal Buffer 的各种压缩存储方式。

## Normal Encoding
首先说明一下，采样 Normal Buffer 无论有没有压缩，使用 Linear 采样器，得到的插值结果都是不对的，会导致一定的渲染瑕疵。之前在讲法线混合的时候就提到过，即使先对 Normal Texture 解码，再线性混合，所得到的结果也是不对的，更不要说直接线性混合压缩过的法线。所以采样 Normal Buffer 最好使用 **Point/Nearest 采样器**。

可能有人会问，为什么 Shader 采样法线贴图的时候，一般使用 Linear 采样器，这是因为法线贴图总是代表某一模型上的法线信息，虽然线性插值会导致凹凸细节看起来变得更平滑，但是却可以因此避免一些锯齿状瑕疵，在视觉上往往也是可以接受的，而且我们会在着色器中再对采样结果进行 normalize 操作，去修正长度。而 Normal Buffer 不同，它代表着场景的世界法线信息，Linear 采样器很可能会混合到不同物体的法线，特别是物体的边缘，混合来自不同表面、不同朝向的法线是毫无意义的，并且会造成一些着色瑕疵。

而常见的对 Normal Buffer 的存储或压缩方式有：  

***①不压缩***：  
&emsp;&emsp; - 直接使用 R16G16B16A16_SNorm 存储，这样子精度是非常高的，虽然非常不常见。但是 Mafia: Definitive Edition 就是这么做的，它使用的还是 R16G16B16A16_SFloat，并且还在 A 通道存储了 Roughness。我看了一下 R16G16B16A16_SFloat 的误差应该是比输出 32 位的八面体编码要大的，但 R16G16B16A16_SNorm 的误差比 32 位八面体要小；  
&emsp;&emsp; - 映射至 0 - 1，并使用 R10G10B10A2_UNORM 的前三个通道存储，这种方式应该是 PC 端游戏最常见的方式了。另外，不推荐使用 R11G11B10_UFLOAT，因为 11-bit Float 精度分布是不均匀的，越接近零的数值，间隔越小（精度越高），越远离零的数值，间隔越大（精度越低）。在接近零的地方，11 位浮点数可以表示非常细小的变化，但在接近 1 的地方，其精度会迅速下降。具体来说，当数值大于 0.2 时，11 位浮点数的精度就已经不如 10 位 UNorm。而 10 位 UNorm 在 0 到 1 的范围内，这 1024 个值是均匀分布的。关于 R11G11B10 的精度问题，可以查看这篇文章：https://bartwronski.com/2017/04/02/small-float-formats-r11g11b10f-precision/ 。这个方式的编码和解码代码也很简单：  

    float3 encode(float3 n)
    {
        return n.xyz * 0.5 + 0.5;
    }

    float3 decode(float3 enc)
    {
        return enc.xyz * 2 - 1;
    }

***② XY Only***：  
这种方式看似美好，但是这种存储方式是极其不均匀的，看 z 的计算公式：  

    n.z = sqrt(1 - dot(n.xy, n.xy));

因为要开根号，所以当 xy 在 1 附近时，z 接近 0 时，x 或 y 分量上一个极小的量化误差或插值误差，都会导致计算出的 z 分量产生巨大的相对误差。

***③坐标映射/投影等压缩方案***：  
这里简单介绍一下常见的压缩方案，主要详细介绍八面体映射，其他方案详见这篇文章：[Compact Normal Storage for small G-Buffers](https://aras-p.info/texts/CompactNormalStorage.html#method04spheremap) 。  
&emsp;&emsp; - **球坐标 Spherical Coordinate** 或**经纬映射 Latitude-Longitude Mapping**：即使用球坐标的天顶角和方位角两个变量存储法线，经纬映射就是对球坐标映射至 \[0, 1\]；  
&emsp;&emsp; - **极射赤面投影法 Stereographic Projection**，具体详见维基百科：https://en.wikipedia.org/wiki/Stereographic_projection ，计算公式如下：  

$$ (X,Y) = \left(\cfrac{x}{1 - z} ,\cfrac{y}{1 - z} \right) $$
$$ (x,y,z) = \left( \cfrac{2X}{1 + X^2 + Y^2} , \cfrac{2Y}{1 + X^2 + Y^2}, \cfrac{-1 + X^2 + Y^2}{1 + X^2 + Y^2} \right) $$

&emsp;&emsp; - **兰勃特方位等积投影 Lambert Azimuthal Equal-Area projection**，还是详见维基百科：https://en.wikipedia.org/wiki/Lambert_azimuthal_equal-area_projection ，计算公式如下：  

$$ (X,Y) = \left( \sqrt{\cfrac{2}{1 - z}}x ,\sqrt{\cfrac{2}{1 - z}}y \right) $$
$$ (x,y,z) = \left( \sqrt{1 - \cfrac{X^2 + Y^2}{4}}X , \sqrt{1 - \cfrac{X^2 + Y^2}{4}}Y, -1 + \cfrac{X^2 + Y^2}{2} \right) $$

类似上述的制图投影应该还有很多，有兴趣可以额外去了解，目前的主流方案应该还是**八面体压缩 Octahedron Encoding**，并且从 A Survey of Efficient Representations for Independent Unit Vectors 的测试来看，在相同位数下，上述映射方式跟八面体压缩相比平均误差会大一点，当然也有比八面体压缩更高精度的压缩方式，但基本上都比八面体压缩更耗性能。

> 关于各种球面映射或者球函数，建议详细阅读这篇论文：[Spherical Function Representations: a Practical Survey](https://jojendersie.de/wp-content/uploads/2013/06/sfcsurvey.pdf) 。

### Octahedral Normal Vectors
**八面体映射 Octahedral Mapping** 就是将三维单位向量投影到正八面体的表面，然后将该八面体展开成二维平面，这种方式得到的分布是较为均匀的，具体如下图：  

<div align="center">  
<img src="https://s2.loli.net/2025/08/24/BbeEzY5ouLTFnQh.png" width = "50%" height = "50%" alt="图3 - Octahedral Mapping"/>
</div>

看起来映射方式较为复杂，但实际上映射方式非常简单，因为描述单位八面体的公式较为简单，即：  

$$ |x| + |y| + |z| = 1 $$

映射过程分为两步：①将球面投影到八面体表面，就是对 x, y, z 除以 $\,|x| + |y| + |z|\,$ 的值，这个过程相当于将球面“挤压”到八面体的表面上；②将八面体表面展开成一个平面，对于 z 大于 0 的情况来说，不需要任何转换，如果 z 小于 0，则将 x, y 反转即可。注意，八面体压缩后获得的两个坐标的范围是 \[-1, 1\]，若想输出 \[0, 1\] 范围值还需要再映射一次。代码如下：  

    float2 PackNormalOctQuadEncode(float3 n)
    {
        n *= rcp(max(dot(abs(n), 1.0), 1e-6));
        float t = saturate(-n.z);
        return n.xy + float2(n.x >= 0.0 ? t : -t, n.y >= 0.0 ? t : -t);
    }

    float3 UnpackNormalOctQuadEncode(float2 f)
    {
        float3 n = float3(f.x, f.y, 1.0 - (f.x < 0 ? -f.x : f.x) - (f.y < 0 ? -f.y : f.y));

        float t = max(-n.z, 0.0);
        n.xy += float2(n.x >= 0.0 ? -t : t, n.y >= 0.0 ? -t : t);

        return normalize(n);
    }

这段代码是 Unity 根据 A Survey of Efficient Representations for Independent Unit Vectors 上的代码而来的，并且根据推特上的[一个帖子](https://twitter.com/Stubbesaurus/status/937994790553227264)做出了优化，它巧妙得利用了 z 轴的数据，减少了一次判断。还有就是，`sign()` 是比三元操作符 `x >= 0 ? 1 : -1` 要昂贵的。

将法线的三个坐标值转换为八面体的两个坐标后，我们可以选用贴图的两个通道来存储，常见的存储方式有 Octahedron 20、Octahedron 24 以及 Octahedron 32。Octahedron 20 就是选用格式 R10G10B10A2_UNORM 的前两个通道，但据说 20 个 bit 的视觉瑕疵在拉近距离时是较为明显的，而视觉瑕疵的临界点大概在 22~23 个 bit 左右（道听途说的，不确定）。而 Octahedron 32 即格式 R16G16_UNORM，追求高精度的视觉效果可以选择这个。Octahedron 24 则相对麻烦点，需要将两个数据打包进 R8G8B8A8_UNORM 的前三个通道当中，打包的代码我就不摘抄了，详见 Unity 的实现代码。

另外提一下，还有一种对八面体映射的改进叫做**同心/轴八面体映射 Concentric Octahedral Map**，它结合了同心映射和八面体映射，具体详见这篇论文：[Fast Equal-Area Mapping of the (Hemi)Sphere using SIMD](https://fileadmin.cs.lth.se/graphics/research/papers/2008/simdmapping/clarberg_simdmapping08_preprint.pdf) ，这种方式比八面体映射更加均匀，当然也要更昂贵一些。

## Derived From Depth Buffer
除了额外开 Normal Buffer 的方法外，还可以直接通过 Depth Buffer 重建法线信息。但是通过 Depth Buffer 重建法线的方式，相比于直接采样 Normal Buffer 走样瑕疵会更多一点，所以 PC 端的前向管线建议还是开 Normal Buffer，移动端可以考虑使用 Depth Buffer 重建法线。

而通过 Depth Buffer 重建法线的方式有两种：  
**①**通过内置函数 `ddx()` / `ddy()` 重建法线；  
**②**通过计算当前像素点以及邻近像素点的世界坐标，然后计算法线；

### ddx/ddy 重建法线
HLSL 中叫 `ddx()` / `ddy()`，GLSL 中叫 `dFdx()` / `dFdy()`，在像素 (片元) 着色器中，用来快速估算水平方向和垂直方向上数值变化率（导数）的内建函数。在三角形光栅化阶段，GPU 会成批（每批 2×2 像素，称为 **Quad**）地并行运行多个片元着色器实例，如果你渲染一个三角形，并且它没有覆盖整个 2x2 Quad，那么空像素仍然需要运行，即使三角形只覆盖一个像素。而导数就是在 Quad 中进行计算的，只有栅格化时才执行 Quad，因此 ddx 和 ddy 只能在像素 (片元) 着色器中才能使用。

导数的计算就是在同一 Quad 内对像素值做差分，从下图可以看出来 ddx 就是右边的像素块的值减去左边像素块的值，而 ddy 就是下面像素块的值减去上面像素块的值。其中的 x，y 代表的是屏幕坐标。

<div align="center">  
<img src="https://s2.loli.net/2025/08/24/sjXd84ACEtBgKyh.jpg" width = "50%" height = "50%" alt="图4 - ddx/ddy"/>
</div>

然后 `ddx(f)` / `ddy(f)` 的参数 f 可以是标量和任意向量浮点元素值，是要被计算导数的变量。之所以可以计算数值变化率（导数），是因为光栅化阶段的顶点属性插值是线性插值。这个 `ddx(f)` / `ddy(f)` 也是在纹理采样过程中计算 **mipmap** 级别的依据，贴图 uv 偏导数过大的时候代表贴图离我们过远，就会选择低等级的 mipmap，在 Shader 中采样纹理时，采样的内部代码类似如下：  

    float lod = log2(max(length(ddx(uv)), length(ddy(uv))));
    tex.SampleLevel(sampler, uv, lod);


如果调用 `ddx(Pos)` / `ddy(Pos)` 就可以求出水平垂直坐标的两个差值，即两个向量，而这两个向量都在这个三角形的平面上，将这两个向量做叉乘，结果就是是垂直于这两条“边”所在平面的向量，即三角形的面法线：  

    normalize(cross(ddx(IN.worldPos), ddy(IN.worldPos)));

用这种方法重构出来的法线，在物体边缘处是有较多的 artifacts 的，并且该方法只能在片元着色器中使用，不能在 compute shader 中使用，因此不推荐使用。

### 邻近像素点重建法线
这个方法的原理其实和 ddx/ddy 是一样的，就是取邻近像素点的世界坐标的位置，计算出两个向量，再叉乘计算出法向量。关于这个方法可以参考这两篇文章：[Improved normal reconstruction from depth](https://wickedengine.net/2019/09/improved-normal-reconstruction-from-depth/) ，[Accurate Normal Reconstruction from Depth Buffer](https://atyuwen.github.io/posts/normal-reconstruction/#fn:2) 。

但是这个方法假设了邻近像素点属于同一个物体，所以当邻近像素点属于不同物体时（即物体边缘时），这个方法会出现瑕疵，当给 SSAO 使用时，物体边缘会出现黑边。所以在选择邻近像素点时，Improved normal reconstruction from depth 这篇文章推荐在上下左右 4 个像素点中，选择离中心像素点最近的两个像素点，计算向量并叉乘。代码大致如下（ue4 的代码）：  

    float DeviceZ = depth;
    float DeviceZLeft = sampleDepth(uv + lUV);
    float DeviceZTop = sampleDepth(uv + uUV);
    float DeviceZRight = sampleDepth(uv + rUV);
    float DeviceZBottom = sampleDepth(uv + dUV);

    float DeviceZDdx = TakeSmallerAbsDelta(DeviceZLeft, DeviceZ, DeviceZRight);
    float DeviceZDdy = TakeSmallerAbsDelta(DeviceZTop, DeviceZ, DeviceZBottom);

    float ZRight = (DeviceZ + DeviceZDdx);
    float ZDown = (DeviceZ + DeviceZDdy);

    float3 Right = GetViewPosition(uv + rUV, ZRight) - vPos;
    float3 Down = GetViewPosition(uv + dUV, ZDown) - vPos;
    return float3(normalize(cross(Right, Down)));

其中 `TakeSmallerAbsDelta`：  

    float TakeSmallerAbsDelta(float left, float mid, float right)
    {
        float a = mid - left;
        float b = right - mid;
        return (abs(a) < abs(b)) ? a : b;
    }

# SSAO
> SSAO 的相关内容主要参考了以下几篇文章：  
> ①Learn OpenGL 教程中的 SSAO 章节：https://learnopengl.com/Advanced-Lighting/SSAO ；  
> ②John Chapman 的博客文章 SSAO Tutorial：http://john-chapman-graphics.blogspot.com/2013/01/ssao-tutorial.html ；  
> ③Alex Tardif 的博客文章 SSAO ：https://alextardif.com/SSAO.html 。

前面有提到过，SSAO 最早由 Crytek 为孤岛危机 Crysis 开发，当时这个算法的思想非常简单，没有使用到场景的法线信息，只使用了 depth buffer，大致思想就是对于屏幕的每一个像素都计算一个 **Occlusion Factor**，步骤如下：  
①根据当前屏幕的像素位置，生成数个球形范围内的样本；  
②计算出这些样本的屏幕空间的坐标以及深度，同时根据屏幕空间坐标对 depth buffer 进行采样；  
③若这些样本的真实深度，比采样 depth buffer 得到的深度远，即被遮挡，则贡献于 Occlusion Factor。

<div align="center">  
<img src="https://s2.loli.net/2025/08/25/W1zMwuB9dIU7Svm.jpg" width = "30%" height = "30%" alt="图5 - Each of the gray depth samples that are inside geometry contribute to the total occlusion factor"/>
</div>

上述方法得到的结果其实是不太对的，但在视觉上还算可以接受。因为采样区域是球形，故平整的墙面也会是灰色，因为一半的样本是被遮挡的，而物体边缘因为只有少数样本会被遮挡，看起来会比其他区域白，如下图：  

<div align="center">  
<img src="https://s2.loli.net/2025/08/25/r7F6LxAdYIM24pz.png" width = "45%" height = "45%" alt="图6 - Crytek SSAO in 2007"/>
</div>

而接下来要实现的 SSAO 对上述方法进行了两个改进：①在以 normal 为朝向的半球内进行采样；②使用余弦权重，让算法相对来说更符合理论公式。虽然做出了改进，但使用 Normal-oriented Hemisphere 的 SSAO 与 Crysis 的 SSAO 的步骤基本上没有什么区别。

<div align="center">  
<img src="https://s2.loli.net/2025/08/25/uWYE81cQyZ2hjS3.jpg" width = "30%" height = "30%" alt="图7 - Normal-oriented Hemisphere"/>
</div>

## 基本实现
> 我的实现也会和著名的 SSAO 教程，即 [john-chapman](http://john-chapman-graphics.blogspot.com/2013/01/ssao-tutorial.html) 的教程，以及 LearnOpenGL 的教程有些许不同。

因为我们要考虑余弦权重，要先计算一下 SSAO 蒙特卡洛积分的公式，可以选择半球均匀采样，或者余弦半球采样。首先是半球均匀采样的公式，半球均匀采样的 PDF 是 $\,1 / 2\pi\,$ ：  

$$ k_{AO}(p) = \cfrac{1}{\pi} \int_{\Omega} \rho(l) (n \cdot l) dl \approx \cfrac{1}{\pi} \cdot \cfrac{1}{N} \sum_{i = 1}^N \cfrac {\rho(l) (n \cdot l)}{1 / 2\pi} = \cfrac{2}{N} \sum_{i = 1}^N \rho(l) (n \cdot l) $$

如果选用的是余弦半球采样，那么 PDF 是 $\, cos\theta / \pi\,$ ：  

$$ k_{AO}(p) = \cfrac{1}{\pi} \int_{\Omega} \rho(l) (n \cdot l) dl \approx \cfrac{1}{\pi} \cdot \cfrac{1}{N} \sum_{j = 1}^N \cfrac {\rho(l) (n \cdot l)}{cos\theta / \pi} = \cfrac{1}{N} \sum_{j = 1}^N \rho(l) $$

### 生成半球样本
在像素点的半球领域内生成样本有很多方法，john-chapman 的教程中是对 xyz 三个值取范围 \[0, 1\] 的随机值，再进行归一化确保每个样本在半球表面，再对每个样本乘以 0 - 1 的随机值进行缩放，让所有样本分布在半球内部。这样子生成的样本，首先在半球表面上应该是不均匀的（我没有验证过，但应该是对的），因为这样子其实生成的是立方体内的均匀样本，归一化到球形之后，在立方体四个角方向上的样本应该是偏多的。其次在半球内也是不均匀的，靠近圆心的样本会多。

于是我很快就想到了**逆变换采样**，但是要注意区分半球表面均匀采样和半球内均匀采样，若使用半球表面，并对每个样本乘以 0 - 1 的随机值进行缩放，靠近圆心的样本会多，如下面所示。半球内均匀采样的公式如下（假设在左手坐标系下，$\,\phi\,$ 是 xz 平面上离 x 轴的角度，范围在 $\,[-\pi, \pi]\,$ 之间，$\,\theta\,$ 是相对于 y 轴的角度，范围在 $\,[0, \pi]\,$ 之间，半球就是 $\,[0, \pi / 2]\,$ ）：

$$ \theta = arccos(1 - \xi_1) $$
$$ \phi = 2 \pi \xi_2 - \pi $$
$$ r = \sqrt[3]{\xi_3} $$

<div align="center">  
<img src="https://s2.loli.net/2025/08/25/E3G58YoQskVbF6C.png" width = "60%" height = "60%" alt="图8 - 左图：半球内均匀采样；右图：半球表面均匀采样，并且 r 为 0 - 1 的随机值"/>
</div>

然而半球内均匀采样的最终结果并不好，因为我们不可能在无限距离上生成样本，我们需要规定一个半径值，而规定了半径值会导致 SSAO 出现泾渭分明的边线，特别是墙壁拐角处。所以为了生成平滑的 SSAO 效果，我们会选择让样本更集中在圆心附近，而这种方法其实是一种**隐式的距离映射函数**，即上面理论介绍小节中讨论到的 **Obscurance** 的思想，即距离越远样本对 **Occlusion Factor** 的贡献越小。虽然 Obscurance 不符合物理准则，但是这样子效果确实会更好。根据实践，即使采用了 $\,r = \xi_3\,$ 随机值的半球样本，样本向球心的聚拢程度仍然不够，最好对 r 再乘上一次 $\,\xi_3\,$，即 $\,r = \xi_3 * \xi_3\,$ 。john-chapman 教程中也是这样做的，代码大致如下：  

    // john-chapman
    float scale = float(i) / float(kernelSize);
    scale = lerp(0.1f, 1.0f, scale * scale);
    kernel[i] *= scale;

    // 我的方案
    float scale = haltonSequence2[i];
    // float scale = (i + 0.5) * rcp(sampleCount);
    kernel[i] *= scale * scale;

另外，我们选用的其实是余弦半球表面采样，代码如下：  

    float4 CosineSampleHemisphere(float2 xi) // Left-handed Spherical and Cartesian Coordinate
    {
        float phi = PI * (2.0 * xi.x - 1.0);
        float cosTheta = sqrt(1 - xi.y);
        float sinTheta = sqrt(1 - cosTheta * cosTheta);

        float3 N = float3(sinTheta * cos(phi), cosTheta, sinTheta * sin(phi));
        float PDF = cosTheta * INV_PI;
        return float4(N, PDF);
    }

我使用的是低差异序列中的 Sobol 序列，随机旋转使用的是 Blue Noise。但是即使是低差异序列加随机旋转，在采样数特别少（差不多 8 - 16）的情况下，SSAO 还是会有一点点**带状伪影 Banding Artifacts** 的，毕竟采样数量太少了，而且还是三维空间下的样本。我也尝试过 Fibonacci Spiral 但效果感觉也没有差太多，我感觉可能还是手动布置采样点，同时确保随机旋转后，采样点能够尽可能均匀地分布到整个半球内，这样会好一点，但是我也没尝试过。毕竟 HBAO 和 GTAO 的效率和效果都比 SSAO 好，这里没必要特别纠结，写得比 Unity URP 的 SSAO 效果好就行了，而且后面的模糊过滤也可以减少噪点和带状伪影。

### TBN 矩阵
上面生成的半球是在切线空间下的，我们要使用 **TBN 矩阵**将其转换到世界空间（或观察空间，我选择的是世界空间）下的坐标，然后再转换到屏幕空间，以便采样 depth buffer 来计算 occlusion factor。我们首先要采样 Normal Buffer 以构建 TBN 矩阵，这其实算基础知识了，就不再赘述了，代码如下：  

    float3 normalWS = LoadAndDecodeNormal(pixelCoord); // N
    float3 up = abs(normalWS.y) > 0.999999 ? float3(0, 0, 1) : float3(0, 1, 0);
    float3 tangent = normalize(cross(up, normalWS)); // T
    float3 binormal = normalize(cross(tangent, normalWS)); // B

然后就是计算每个采样点的世界坐标，首先要获取半球原点（着色点）的世界坐标，通过 depth 重构世界坐标：  

    float originDepth = LoadDepth(pixelCoord);
    float4 originNDC = GetNDCFromUVAndDepth(screenUV, originDepth);
    float3 originWS = TransformNDCToWorld(originNDC, UNITY_MATRIX_I_VP);

准备好上述工作后，就可以在 SSAO 的循环中，生成采样点，并计算出采样点的世界坐标和屏幕坐标了，通过屏幕坐标采样 depth buffer 得到深度值：  

    // ------------------------- Get Origin Position -------------------------
    ...

    // ------------------------- Get Normal & Build Left-Handed TBN -------------------------
    ...

    // ------------------------- Random Rotation -------------------------
    float randomRadian = (LOAD_TEXTURE2D_LOD(_BlueNoise64, id.xy % _BlueNoise64_TexelSize.w, 0).r) * TWO_PI;
    float2x2 rotation = float2x2(cos(randomRadian), -sin(randomRadian), sin(randomRadian), cos(randomRadian));

    // ------------------------- SSAO Loop -------------------------
    
    float aoFactor = 0.0;
    for (int i = 0; i < sampleCount; i++)
    {
        // ------------------------- Generate Hemisphere Samples -------------------------
        float2 xi = k_Sobol[i + 1];
        float scale = k_Halton[i + 1].y;
        float3 dirTS = CosineSampleHemisphere(xi);
        dirTS *= scale * scale;
        dirTS.xz = mul(rotation, dirTS.xz);

        float3 dirWS = tangent * dirTS.x + normalWS * dirTS.y + binormal * dirTS.z;
        float3 sampleWS = originWS + dirWS * radius;
        float4 sampleHCS = TransformWorldToHClip(sampleWS);
        sampleHCS.xyz /= sampleHCS.w;

        float2 uv = sampleHCS.xy * 0.5 + 0.5;
        #if UNITY_UV_STARTS_AT_TOP
        uv.y = 1.0f - uv.y;
        #endif
        
        int2 pixelCoord = clamp(uv * _TextureSize.zw, 0, _TextureSize.zw - 1);
        float sampledDepth = LoadDepth(pixelCoord);
        float linearDepth = GetViewDepthFromDepthTexture(sampledDepth);
        ...
    }

`radius` 是用于控制 SSAO 范围的参数，样本的世界空间坐标就是原点坐标加上乘上 TBN 矩阵后的样本方向向量。

### 计算 Occlusion Factor
只要采样点在 depth buffer 中的深度，比它实际的深度（裁切空间坐标的 w）近，则认为该采样点被遮挡，然后就可以在循环后计算 AO 值。但是这样 AO 的范围还是无限距离的，我们需要增加一个范围检查：  

    for (int i = 0; i < sampleCount; i++)
    {
        ...
        bool rangeCheck = abs(sampleHCS.w - linearDepth) < radius;
        float occlusion = (linearDepth < sampleHCS.w) * rangeCheck;
        aoFactor += 1.0 - occlusion;
    }
    aoFactor = saturate(aoFactor / sampleCount);

<div align="center">  
<img src="https://s2.loli.net/2025/08/27/PTBIEuWKtYFxAVp.png" width = "60%" height = "60%" alt="图9 - Full Resolution，16 次采样，左图：无 Range Check；右图：有 Range Check"/>
</div>

可以从图中看出，没有 Range Check 会导致错误的 SSAO 现象，Range Check 确保了超过设定范围的被遮挡的样本不对 Occlusion Factor 做出贡献。

## Spatial Filter
上图中 AO 效果仍然可以看到很多噪点，我们可以增加采样数量以减少噪点，但是 SSAO 技术是性能敏感的，增加采样数量对于性能是一个很大的打击，所以我们往往会对 AO 进行模糊/过滤，过滤又可以分为**空间滤波 Spatial Filter** 以及**时间滤波 Temporal Filter**。最常见的空间滤波即高斯模糊，但是使用高斯过滤有个问题，就是 AO 的范围会扩散到物体边缘的上面，我们并不希望这样，所以我们会在过滤的权重上增加其他几何权重，比如深度或者法线，用于判断空间的**不连续性 Discontinuity**，从而阻止 AO 效果扩散到不该扩散的地方上去，而这种空间滤波即称为**双边滤波 Bilateral Filter**。但无论如何空间滤波的效果都比不过时间滤波，Temporal Filter 永远滴神！！！当然最好还是混合使用，下面首先讲解空间滤波。

> 我在网上还查阅到据说比双边滤波更好的边缘保持滤波：**Guided filter**，详见这篇文章 https://bartwronski.com/2019/09/22/local-linear-models-guided-filter/ ，它使用最小二乘法，建立了信号的线性关系，果然万事万物离不了统计学。我没有尝试过该方法，有兴趣可以试试。

### Bilateral Filter
首先回忆一下高斯模糊，我们常常会把二维高斯核拆为两个一维高斯核减少采样次数，而一维的高斯模糊的公式如下：  

$$ G(x) = \cfrac {1}{\sqrt{2 \pi \sigma^2}} e^{- \frac {x^2} {2 \sigma^2}} $$

其中 $\,x\,$ 是当前采样点到中心点的距离；$\,\sigma\,$ 决定了模糊的程度，该值越大，权重分布越均匀，模糊效果越强烈，即越接近 Box Filter。正如之前所说，使用高斯模糊有个显著的缺点，就是会导致 AO 效果“晕染”开来，导致 AO 效果出现在本不应该出现的物体边缘。为了更好的模糊效果，我们应该采用**边缘保持滤波 Edge-Preservation Filter / 几何感知滤波 Geometry-Aware Filter**。

而**双边过滤 Bilateral Filter** 正是一种边缘保持滤波，一般的高斯模糊的核权重是基于像素与像素之间的欧几里得距离计算而得，也称为**空域核 Spatial Kernel**，而双边过滤在考虑空间距离的同时，增加了**值域核 Range Kernel**。这个值域核考虑了像素与像素之间的相似性，可以基于颜色差异计算，也可以基于几何差异计算，取决于具体的需求。双边过滤可以基于高斯分布，也可以不基于，基于高斯分布的空域和值域核的权重公式如下：  

$$ w(i,j,k,l) = exp(- \cfrac {(i - k)^2 + (j - l)^2} {2 \sigma_s^2} - \cfrac {||I(i,j) - I(k,l)||^2} {2 \sigma_r^2}) $$

对于 AO 效果而言，我们更关心几何边缘的维持，所以过滤时需要额外考虑几何信息，比如 depth 和 normal 值，我自己实测下来，只考虑 depth 差异的效果比只考虑 normal 差异的效果要好很多，因为 normal 的差异无法区分两个平行的平面但有深度差异的情形。当然 depth 和 normal 同时考虑的效果肯定更好，但是为了减少采样次数，只考虑 depth 差异的效果已经基本够用了。需要注意的是，值域核的 sigma 是越小越能维持边缘，所以 $\,\sigma_s\,$ 需要大一点，$\,\sigma_r\,$ 需要小一点。

还有一点是，双边过滤理论上来说是**不可拆分的 not separable**，和高斯模糊不同，使用水平和垂直两次模糊在数学上不严格等于二维过滤。但是在实践中，将双边过滤拆分为水平和垂直两次所造成的 artifacts 在视觉上并不明显，所以往往还是会拆分以节省性能。拆分后的权重计算代码如下：  

    inline float BilateralWeight(float radiusDelta, float depthDelta, float2 sigma)
    {
        return exp(-radiusDelta * radiusDelta * rcp(2.0 * sigma.x * sigma.x) - depthDelta * depthDelta * rcp(2.0 * sigma.y * sigma.y));
    }

上述代码中，depthDelta 是在世界/观察空间下的线性深度差，基本上严格参照了双边过滤的公式。我也有看到其他代码（NVIDIA 的开源 [Direct3D SDK 11](https://github.com/LiveMirror/NVIDIA-Direct3D-SDK-11)）将深度测试更加简单化了，直接使用一个阈值进行测试，如下：  

    float CrossBilateralWeight(float r, float d, float d0)
    {
        // The exp2(-r*r*g_BlurFalloff) expression below is pre-computed by fxc.
        // On GF100, this ||d-d0||<threshold test is significantly faster than an exp weight.
        return exp2(-r*r*g_BlurFalloff) * (abs(d - d0) < g_BlurDepthThreshold);
    }

我实际测试下来，这两种代码的效果其实没啥区别，根据性能要求和喜好选择使用即可。

### Compute Shader 优化
如果使用像素/片元着色器进行模糊或过滤，即使拆分为水平和垂直两次，每次每个像素仍然都需要采样核长度的样本，假设核长度为 9，每个像素就都要采样 9 次。这样就会造成很多不必要的重复采样，而在 compute shader 中我们可以利用组共享内存 `groupshared` 变量来存储采样的结果，以节省采样次数。

还有一点就是，因为双边过滤需要 depth 信息，这样我们就需要采样 ao 贴图一次，采样 depth buffer 一次，为了节省采样次数，我们往往会在 SSAO 中输出 `float2(ao, z)`，这样在过滤时，就只需采样一次了。SSAO 的输出贴图的格式我选择的是 R16R16_UNorm，16 位的深度对于过滤来说基本上是足够了的，但是若将 depth 的 sigma 调得太小更容易出现伪边现象，所以这里就是在性能和效果做出妥协。

使用 Compute Shader 优化模糊算法的方法有两种，逻辑是相同的，就是细节有点不太一样，假设 1920 × 1080 的分辨率：  
**①**水平和垂直两个核，Horizontal Kernel 线程数量为 `[numthreads(64, 1, 1)]`，Vertical Kernel 线程数量为 `[numthreads(1, 64, 1)]`，这样子线程组大小分别是 120 × 1080 和 1920 × 68。每个线程组的组共享内存 groupshared 的大小需设置为 numthreads + 2 × MAX_KERNEL_RADIUS 个，即线程数量 64 加上设定的最大核半径乘 2，即左右各一次，这其实就是每个线程组需要采样的区域范围。然后，根据每个线程的 dispatchThreadID 采样一次 (ao, z)，且每个线程组的 groupIndex 若小于 MAX_KERNEL_RADIUS 或者大于等于线程数量 64 减去 MAX_KERNEL_RADIUS，则额外采样一次 (ao, z)，这样子利用 `GroupMemoryBarrierWithGroupSync();` 组内同步，就可以在组共享内存存储所有当前组需要采样的像素数据了；  
**②**这个方法和上面本质是一样的，方法一的线程还是可能需要采样两次，这个方法增加了每个组的线程数量，缺点就是线程数量乘以组数量，不匹配纹理分辨率，不是很直观，还有就是线程数量可能不正好是 32 或 64 的倍数。线程数量增加为 numthreads + 2 × MAX_KERNEL_RADIUS 个，组共享内存 groupshared 的大小不变。这样子线程数量是等于 groupshared 变量的大小的，每个线程采样一次 (ao, z) 即可，然后在模糊循环中判断范围，跳过多余的线程。我没有使用这个方法，代码还是参考 NVIDIA 的开源 [Direct3D SDK 11](https://github.com/LiveMirror/NVIDIA-Direct3D-SDK-11) 里的 SSAO 的 Blur。

方法一的代码如下，以 Horizontal Kernel 为例，Vertical Kernel 基本是一样的，就不重复摘录了：  

    [numthreads(64, 1, 1)]
    void SpatialBlurHorizontalKernel(uint3 id : SV_DispatchThreadID, uint groupIndex : SV_GroupIndex)
    {
        // ------------------------- Fetch AO & Depth -------------------------

        int2 pixelCoord = clamp(id.xy, 0, _TextureSize.zw - 1);
        _AOAndDepth[groupIndex + MAX_FILTER_RADIUS] = LoadAOandDepth(pixelCoord);

        if (groupIndex < MAX_FILTER_RADIUS)
        {
            int2 extraCoord = pixelCoord - int2(MAX_FILTER_RADIUS, 0);
            extraCoord = clamp(extraCoord, 0, _TextureSize.zw - 1);
            _AOAndDepth[groupIndex] = LoadAOandDepth(extraCoord);
        }

        if (groupIndex >= 64 - MAX_FILTER_RADIUS)
        {
            int2 extraCoord = pixelCoord + int2(MAX_FILTER_RADIUS, 0);
            extraCoord = clamp(extraCoord, 0, _TextureSize.zw - 1);
            _AOAndDepth[groupIndex + 2 * MAX_FILTER_RADIUS] = LoadAOandDepth(extraCoord);
        }
        
        GroupMemoryBarrierWithGroupSync();

        // ------------------------- Bilateral Blur -------------------------

        float2 middle = _AOAndDepth[groupIndex + MAX_FILTER_RADIUS];
        float middleDepth = GetViewDepthFromDepthTexture(middle.g);
        float weightSum = 0.0;
        float aoFactor = 0.0;
            
        int radius = int(GetSpatialBlurKernelRadius());
        for (int i = -radius; i <= radius; i++)
        {
            float2 sample = _AOAndDepth[groupIndex + MAX_FILTER_RADIUS + i];
            float sampleDepth = GetViewDepthFromDepthTexture(sample.g);
            float depthDelta = abs(sampleDepth - middleDepth);
            float weight = BilateralWeight(i, depthDelta, GetSpatialBlurSigma());
            aoFactor += sample.r * weight;
            weightSum += weight;
        }
        aoFactor /= weightSum;
        _OutputTexture[id.xy] = float2(aoFactor, middle.g);
    }

双边过滤后，SSAO 的效果如下图：  

<div align="center">  
<img src="https://s2.loli.net/2025/09/05/WFJ6rkVeU8dNMYp.png" width = "100%" height = "100%" alt="图10 - Full Resolution 1080P，12 次采样，上图：无过滤；左图：高斯过滤，Kernel Radius 为 4，即核大小为 9，Spatial Sigma 为 2；右图：双边过滤，Kernel Radius 为 4，即核大小为 9，Spatial Sigma 为 2，Range Sigma 为 0.25"/>
</div>

可以从图中看出，双边过滤可以较好地维持物体的边缘，特别是桌子和椅子的边缘处。空间滤波的缺点其实非常明显，就是会感觉特别模糊，这点是不如时间滤波的。

## Temporal Filter
我在网上主要查阅到这几篇文章提及 SSAO 的 Temporal Filter：①[Stable SSAO in Battlefield 3 with Selective Temporal Filtering](https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/gamedev/files/gdc12/GDC12_Bavoil_Stable_SSAO_In_BF3_With_STF.pdf)；②[Tech Feature: SSAO and Temporal Blur](https://frictionalgames.com/2014-01-tech-feature-ssao-and-temporal-blur/)；③[Temporal supersampling pt. 2 – SSAO demonstration](https://bartwronski.com/2014/04/27/temporal-supersampling-pt-2-ssao-demonstration/)；④《GPU Pro 2》Chapter III-1：Temporal Screen-Space Ambient Occlusion。

这几篇文章都有使用 Depth Rejection，因为 SSAO 的输出刚好有 (AO, Z) 信息，AO History 也可以存储 (AO, Z)，这样子做 Depth Rejection 几乎没有额外的消耗。但只使用 Depth Rejection 会有**拖影现象 Trailing Artifact**，这个拖影现象产生的原因和阴影鬼影的逻辑一样，因为 ao 效果是在物体的外围的，所以动态物体移动时 ao 区域的深度不会发生变化，导致 Depth Rejection 无法拒绝错误历史。文章一和文章四的解决方案基本都是使用一个 mask 标记出场景中空间不连续的区域及其周围，然后取消该区域的历史混合。文章三中也提到了这一点，而且文章三认为 ao 的拖影现象在场景中其实是很难察觉的。

而文章二中，在 Depth Rejection 的基础上还对比了历史帧和当前帧的 AO 值大小，若差异过大，则减少历史帧的混合比例，以减少拖影现象。这个方法其实是一种另类的 Color Rejection，但这个方法需要让 Temporal Filter 在 Spatial Filter 之后进行，否则会导致 Temporal Filter 效果很差，很难兼顾 ao 拖影和降噪效果。

我不是很喜欢 Depth Rejection + Mask 的方案，所以我最终还是选择了 **Color Rejection + Depth Rejection** 的方案，Color Rejection 的缺点就是需要采样周围 9 个样本，并且需要在降噪效果和解决拖影问题之间做出平衡。虽然采样次数可以通过 Compute Shader 的 `groupshared` 变量来优化。

### History Rejection
AO 的 Color Rejection 无需像 TAA 中一样如此复杂，因为 AO 几乎无需关心闪烁问题。首先需要对 SSAO 的半球随机样本添加 jitter，我对随机旋转添加了每帧变化的随机数：  

    float randomRadian = (LOAD_TEXTURE2D_LOD(_BlueNoise64, id.xy % _BlueNoise64_TexelSize.zw, 0).r + _Jitter.z * IsTemporalBlurEnabled()) * TWO_PI;

然后使用了 TAA 的 Variance Clip 的方法建立了一个最大最小值，并且直接将 History AO 钳制到了该最大最小值，critical value（gamma）作为一个可以调控的值（范围在 0.5 - 1.5）：  

    float2 VarianceMinMax(in float2 samples[9], float gamma)
    {
        float m1 = 0;
        float m2 = 0;

        UNITY_UNROLL
        for (int i = 0; i < 9; i++)
        {
            float sampleColor = samples[i].r;
            m1 += sampleColor;
            m2 += sampleColor * sampleColor;
        }

        const int sampleCount = 9;
        m1 *= rcp(sampleCount);
        m2 *= rcp(sampleCount);

        float sigma = sqrt(abs(m2 - m1 * m1)); // standard deviation
        float neighborMin = m1 - gamma * sigma;
        float neighborMax = m1 + gamma * sigma;

        return float2(neighborMin, neighborMax);
    }

    [numthreads(8, 8, 1)]
    void TemporalBlurKernel(uint3 id : SV_DispatchThreadID)
    {
        ...
        float2 minMax = VarianceMinMax(neighbours, GetTemporalVarianceCriticalValue());
        history.r = clamp(history.r, minMax.x, minMax.y);
        ...
    }

想要最大程度消除拖影现象，需要将 critical value（gamma）设置为一个很小的值，但是这样会导致 Temporal Filter 的降噪效果变差，从而导致噪点的微弱雪花闪烁，虽然大部分情况看不到。所以最好还是同时顺带对当前颜色进行过滤，以减少降低 critical value 的副作用：  

    float FilterMiddleColor(in float2 samples[9], float middleDepth)
    {
        const float weights[9] = { 4.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0 };
        
        float weightSum = 4.0;
        float filtered = weightSum * samples[0].r;

        UNITY_UNROLL
        for (int i = 0; i < 8; i++)
        {
            float sampleDepth = GetViewDepthFromDepthTexture(samples[i + 1].g);
            bool occlusionTest = abs(1 - sampleDepth / middleDepth) < 0.1;
            float weight = weights[i + 1] * occlusionTest;
            weightSum += weight;
            filtered += weight * samples[i + 1].r;
        }
        
        filtered *= rcp(weightSum);
        return filtered;
    }

但是实际上上述 3 × 3 的过滤的效果也有限。总之 critical value 越小，拖影越少，但降噪效果降低；critical value 越大，降噪效果越好，但拖影就会越明显。所以最好还是在 Temporal Filter 之前做一次 Spatial Sigma 较小（大概 0.5 到 1 之间）的 Spatial Filter，提前做一次 Spatial Filter 能够增加 Temporal Filter 的降噪效果，拖影现象也可以得到一定的缓解，缺点就是噪点的雪花闪烁也会相对更加明显一点。好在 AO 是低频信息，噪点闪烁和拖影现象不仔细观察，其实几乎看不太见，也无需过于纠结。

最后我使用了 Depth Rejection，同时当 historyUV 离屏时，将 blendFactor 设置为 0，以解决镜头移动时屏幕边缘的拖影问题：  

    bool depthTest = abs(1 - middleDepth / historyDepth) < 0.1;
    float blendFactor = lerp(0, 0.9, depthTest);

    blendFactor = lerp(blendFactor, 0, any(abs(historyUV - 0.5) > 0.5));

### Compute Shader 优化
采样当前像素点周围 3 × 3 个样本，也可以跟 Spatial Filter 中使用的方法一样，利用 Compute Shader 的组共享内存 `groupshared` 变量来存储采样的结果，就是方法略微改变了一下，并且没有了水平和垂直两次 pass，只有一次 pass。首先我们设置线程数量为 8 × 8：  

    static const uint THREAD_NUM = 8;

    [numthreads(THREAD_NUM, THREAD_NUM, 1)]
    void TemporalBlurKernel(uint3 id : SV_DispatchThreadID)
    {
        ...
    }

一个线程组代表纹理上 8 × 8 的区域，那么这个 8 × 8 的区域最大的采样范围为 8 + 1 + 1，即 10 × 10：  

    static const uint TILE_BORDER = 1;
    static const uint TILE_SIZE = THREAD_NUM + 2 * TILE_BORDER;
    groupshared float2 _AOZ[TILE_SIZE * TILE_SIZE];

然后就是使用个循环采样这个 10 × 10 区域的所有样本：  

    int2 tileTopLeftCoord = groupId.xy * THREAD_NUM - TILE_BORDER;

    UNITY_UNROLL
    for (uint i = groupIndex; i < TILE_SIZE * TILE_SIZE; i += THREAD_NUM * THREAD_NUM)
    {
        int2 coord = tileTopLeftCoord + int2(i % TILE_SIZE, i / TILE_SIZE);
        coord = clamp(coord, 0, _TextureSize.zw - 1);
        _AOZ[i] = LoadAOandDepth(coord);
    }

    GroupMemoryBarrierWithGroupSync();

每个当前像素点对应周围 3 × 3 个样本在 `groupshared` 变量中的索引可以通过下述方法计算获取：  

    uint2 middleTileID = groupThreadId.xy + TILE_BORDER;
    uint tileIDs[9];
    tileIDs[0] = (middleTileID.y     ) * TILE_SIZE + (middleTileID.x     );
    tileIDs[1] = (middleTileID.y +  0) * TILE_SIZE + (middleTileID.x +  1);
    tileIDs[2] = (middleTileID.y +  1) * TILE_SIZE + (middleTileID.x +  0);
    tileIDs[3] = (middleTileID.y +  0) * TILE_SIZE + (middleTileID.x + -1);
    tileIDs[4] = (middleTileID.y + -1) * TILE_SIZE + (middleTileID.x +  0);
    tileIDs[5] = (middleTileID.y +  1) * TILE_SIZE + (middleTileID.x + -1);
    tileIDs[6] = (middleTileID.y +  1) * TILE_SIZE + (middleTileID.x +  1);
    tileIDs[7] = (middleTileID.y + -1) * TILE_SIZE + (middleTileID.x + -1);
    tileIDs[8] = (middleTileID.y + -1) * TILE_SIZE + (middleTileID.x +  1);

拿到索引就可以获取到当前像素点周围 9 个样本的 (ao, z) 的值了，从而进行 History Rejection。最终的 Temporal Filter 效果如下：  

<div align="center">  
<img src="https://s2.loli.net/2025/09/07/of3qIljMrFDt5hz.png" width = "100%" height = "100%" alt="图11 - Full Resolution 1080P，12 次采样，上图：无过滤；左图：仅 Temporal Filter，Ciritical Value(gamma) 为 1；右图：Spatial + Temporal Filter，Kernel Radius 为 4，即核大小为 9，Spatial Sigma 为 0.6，Range Sigma 为 0.25"/>
</div>

对比 Spatial Filter，可以看出 Temporal Filter 能在不增加整体模糊程度的情况下进行降噪，效果比 Spatial Filter 要优秀很多，缺点就是之前所说的，会增加一定的 artifact。

## Half Resolution
SSAO 是一个特别耗时的算法，我显卡 4080 在 4k 下，12 次采样，Radius 为 2 的情况下，基础耗时 0.7 ~ 0.8 ms，Spatial Filter 两个 pass 耗时总共大约 0.5 ms，Temporal Filter 耗时大约 0.2 ms，总耗时约 1.5 ms，测试较为随意，可能不是很准，在复杂场景下耗时应该会增加。由于 Ambient Occlusion 是低频信息，所以一个常见的优化方案就是在**一半分辨率 Half Resolution** 的 RT 上渲染 AO，这样做可以显著增加性能，但缺点就是噪点会更加明显，也会增加其他视觉瑕疵或 Artifacts。

Half Resolution 渲染 AO 常见的步骤是，①先 Depth PrePass 渲染场景深度，②然后对 Depth Buffer、Normal Buffer 进行**下采样 DownSample** 至一半分辨率，③再在一半分辨率上渲染 AO，④之后进行 Spatial/Temporal Filter，⑤最后对 AO 进行**上采样 UpSample** 再应用到场景当中。我也有看到 Spatial/Temporal Filter 在上采样之后做的，即在 **Full Resolution** 过滤，这样子效果肯定会更好一点。

> 其实这一块内容也称为 **Mixed Resolution Rendering**，若要了解这块内容建议搜索这个关键词。推荐的文章有：  
> ① Mixed Resolution Rendering (GDC 2009)：https://gdcvault.com/play/1667/Mixed-Resolution ；  
> ② Mixed Resolution Rendering in Skylanders Superchargers (GDC 2016)：https://www.gdcvault.com/play/1022982/Mixed-Resolution-Rendering-in-Skylanders 。

### Downsample
之所以存在下采样这一步骤，有一个重要原因是**缓存命中率 Cache Hit Rate** 的问题，如果在半分辨率上采样全分辨率 Depth Buffer，当处理 AO 效果时，半分辨率相邻像素在全分辨率深度纹理中对应的采样点相距 2 个 texel，这意味着你的采样访问模式是跳跃式的，而不是连续的，因为不在一个 2 x 2 的 Quad 内。这种非连续的、稀疏的内存访问模式会导致大量的**缓存未命中 Cache Miss**。每一次缓存未命中都需要从显存中重新提取数据，而显存访问的延迟和带宽消耗是巨大的。

> 增加 SSAO 的 Radius 也会导致缓存命中率下降，从而降低性能。强烈建议观看这篇文章：[Real-time Raytracing and Screen-space Ambient Occlusion](https://www.diva-portal.org/smash/get/diva2:1337203/FULLTEXT01.pdf) 。

还有一个原因是，如何采样 Depth Buffer 和 Normal Buffer 的问题，Normal Buffer 之前提到过无法使用 Linear 采样器，只能直接 Load 或 Point 采样器再解码。Depth Buffer 的问题跟 Normal Buffer 类似，使用 Linear 采样器的话，会造成一定的 AO 瑕疵，比如黑边，因为 Linear 采样器会创造出新的深度值，而这个深度值并不能完全代表全分辨率中的像素的深度。

根据我自己的实践，在半分辨率下 `GatherRed()` 全分辨率 Depth Buffer 的四个像素，无论使用最近深度（Max Depth）、最远深度（Min Depth）还是使用平均值甚至是中位数，这样得到的 AO 效果在物体边缘都是会有瑕疵的，这点其实 [Mixed Resolution Rendering in Skylanders Superchargers](https://www.gdcvault.com/play/1022982/Mixed-Resolution-Rendering-in-Skylanders) 文章中也有提及，该文章最终选取的方法是使用一个 Checkerboard Pattern 选择使用最近深度或最远深度的纹素，据说这样可以减少边缘的错误。

但是我实践之后，感觉 Checkerboard  Pattern 选择使用最近深度或最远深度的方法也不是很好，故我没有采用这个方案，可能这种方式不太适用于 AO 效果。总之最近深度会引入黑边、最远深度会引入白边（halo）、Checkerboard Pattern 会间隔引入黑边白边（汗 ￢ω￢），我猜测原因是计算 AO 效果的 depth 不能使用 Checkerboard Pattern，否则就会引入 artifact，而上采样使用的 depth 最好是 Checkerboard Pattern，但这么搞太麻烦了。所以我最终选取的方案就是最普遍的方案，即使用 Load 或 Point 采样器进行下采样，然后通过 **Depth-Aware Upsample** 减少因为下采样引入的 artifacts，详情见 Upsample 小节。

    [numthreads(8, 8, 1)]
    void DepthDownsampleKernel(uint3 id : SV_DispatchThreadID)
    {
        int2 pixelCoord = clamp(id.xy, 0, _TextureSize.zw - 1);
        _OutputTexture[id.xy] = LOAD_TEXTURE2D_LOD(_CameraDepthTexture, pixelCoord * 2, 0).r;
    }

我目前没有对 Normal Buffer 进行下采样，只对 Depth Buffer 进行了下采样，因为 Normal Buffer 只需在 SSAO 中采样一次，而采样 Depth Buffer 则需要数次或十几次，其次我确实也没有感受出 Load 或 Point 采样器采样 Normal Buffer 所带来的 AO 瑕疵问题，所以额外下采样感觉意义不大，除非这个半分辨率的 Normal Buffer 之后还需要使用。但是我看到死亡搁浅 Death Stranding 的管线中处理 SSAO 时，对 Normal Buffer 进行了下采样，但不清楚下采样过程中是不是只是使用 Load 或 Point 采样器，还是有一些其他的奇技淫巧，我没有查询到更多的资料，所以这一部分可能还有待额外了解。

### Upsample
由于使用了 Half Resolution，那么下采样得到的 depth 则代表了全分辨率下的 4 个像素，但一个像素永远代表不了 4 个像素，所以会给 AO 效果引入 artifact。具体表现就是若对 AO Texture 直接使用 linear 上采样，在物体边缘可能会出现白边或 ao 晕染现象（下采样为 Load 或 Point 的情形下），产生的原因就是因为深度信息的缺失。这也是为什么需要上采样的原因，我们可以通过采样全分辨率下的 depth 来判断并选择使用 Half Resolution AO 中的像素，从而减少 artifact，这类上采样可以称为 **Depth-Aware Upsample**。Depth-Aware Upsample 主要有以下两种方式：  

**①Bilateral Upsample**：具体可以参考 [Mixed Resolution Rendering](https://gdcvault.com/play/1667/Mixed-Resolution) 这篇文章。首先当前全分辨率 Full Resolution 贴图中的每个像素都有对应 Half Resolution 贴图中 2 × 2 区域的 4 个像素，我们先计算这 4 个像素的空间权重，即 bilinear weight，如下图：  

<div align="center">  
<img src="https://s2.loli.net/2025/09/08/HlJLyAdw9Yh2sj5.png" width = "20%" height = "20%" alt="图12 - Bilateral Upsample Bilinear Weights"/>
</div>

    float4 vBilinearWeights[4] = 
    {
                // 0     1     2     3
        float4( 9/16, 3/16, 3/16, 1/16), // 0
        float4( 3/16, 9/16, 1/16, 3/16), // 1
        float4( 3/16, 1/16, 9/16, 3/16), // 2
        float4( 1/16, 3/16, 3/16, 9/16), // 3
    }

然后就是计算 Half Resolution 中 4 个像素的 Depth Weight，采样 Full Resolution 的当前像素 Depth，并通过 `Gather()` 获取到 Half Resolution 中对应的 4 个 Depth，利用差值的倒数作为 Depth Weight：  

    float4 fDepthsCoarse[4] = {...};
    float fDepthHiRes = ...;

    for (int i = 0; i < 4; i++)
    {
        float fDepthDiff = fDepthHiRes - fDepthsCoarse[i];
        vDepthWeights[i] = 1.0 / (EPSILON + abs(fDepthDiff));
    }

若想要更好的上采样效果还可以增加 Normal Weight，这里就不考虑 Normal 了，最后计算上采样结果：  

    for (int nSample = 0; nSample < 4; nSample++)
    {
        float fWeight = vDepthWeights[nSample] * vBilinearWeights[nTexel][nSample];
        fTotalWeight += fWeight;
        vUpsampledShading += vShadingCoarse[nSample];
    }
    vUpsampledShading /= fTotalWeight;

**②Nearest-Depth Upsample**：这个方法来源于 NVIDIA 的文章 [Fast rendering of opacitymapped particles using DirectX 11 tessellation and mixed resolutions](https://developer.download.nvidia.com/assets/gamedev/files/sdk/11/OpacityMappingSDKWhitePaper.pdf) 。原理和上面其实差不多，还是获取 Full Resolution 中当前像素的深度值，以及其对应的 Half Resolution 的 4 个像素深度值（可以使用 `GatherRed()`）。选择这 4 个深度值中离 Full Resolution 深度值最近的像素点，并返回该像素点的颜色值。该方法有个问题是，若只使用了 2 × 2 像素中的一个像素，有时候会在非边缘像素中造成块状 artifacts。为了解决这个问题，可以对边缘像素使用 Nearest-Depth Upsample，对非边缘像素使用 Bilinear Upsample。至于如何区分边缘和非边缘，还是利用深度差，代码大致如下：  

    if (abs(Z00 - ZFull) < g_DepthThreshold && abs(Z10 - ZFull) < g_DepthThreshold &&
        abs(Z01 - ZFull) < g_DepthThreshold && abs(Z11 - ZFull) < g_DepthThreshold)
    {
        return g_LoResColor.Sample(g_SamplerBilinear, LoResUV);
    }
    else
    {
        return g_LoResColor.Sample(g_SamplerNearest, NearestUV);
    }

我最终使用了 **Bilateral Upsample** 的方案，但是奇怪的是，在实践中我发现，Bilateral Upsample 不考虑 BilinearWeights 效果会更好，边缘的 artifact 还会更少，不知道是何原因。最终代码如下：  

    [numthreads(8, 8, 1)]
    void UpsampleKernel(uint3 id : SV_DispatchThreadID)
    {
        float2 screenUV = (float2(id.xy) + float2(0.5, 0.5)) * _CameraBufferSize.xy;
        uint2 pixelCoord = clamp(id.xy, 0, _CameraBufferSize.zw - 1);

        // ------------------------- Fetch Full Resolution Depth & 4 Half Resolution Depths -------------------------
        
        float fullDepth = LOAD_TEXTURE2D_LOD(_CameraDepthTexture, pixelCoord, 0).r;
        fullDepth = GetViewDepthFromDepthTexture(fullDepth);
        
        // float4 halfDepths = GATHER_RED_TEXTURE2D(_HalfDepthTexture, sampler_PointClamp, screenUV);
        float4 halfDepths = GATHER_GREEN_TEXTURE2D(_InputTexture, sampler_PointClamp, screenUV);
        halfDepths.x = GetViewDepthFromDepthTexture(halfDepths.x);
        halfDepths.y = GetViewDepthFromDepthTexture(halfDepths.y);
        halfDepths.z = GetViewDepthFromDepthTexture(halfDepths.z);
        halfDepths.w = GetViewDepthFromDepthTexture(halfDepths.w);
        
        float4 aos = GATHER_RED_TEXTURE2D(_InputTexture, sampler_PointClamp, screenUV);

        // ------------------------- Depth-Aware Upsample -------------------------
        
        bool4 weights = abs(1.0 - halfDepths / fullDepth) < 0.05;
        float weightSum = weights.x + weights.y + weights.z + weights.w + HALF_MIN;
        float ao = aos.x * weights.x + aos.y * weights.y + aos.z * weights.z + aos.w * weights.w;
        ao = lerp(ao / weightSum, 1, all(depthTest == 0));
        
        _OutputTexture[id.xy] = float2(ao, 0);
    }

使用 Half Resolution 进行性能优化后的 AO 效果如下：  

<div align="center">  
<img src="https://s2.loli.net/2025/09/09/qykcd24RAxHCfim.png" width = "100%" height = "100%" alt="图13 - 1080P 下 Half Resolution 渲染 AO，即 AO 在 540 P 下渲染，12 次采样，最终上采样到 1080P。上图：无过滤；左图：双边过滤，Kernel Radius 为 4，即核大小为 9，Spatial Sigma 为 2，Range Sigma 为 0.25；右图：Spatial + Temporal Filter，Kernel Radius 为 4，即核大小为 9，Spatial Sigma 为 0.6，Range Sigma 为 0.25，Temporal Filter 的 Ciritical Value(gamma) 为 1。"/>
</div>

可以看出 Half Resolution 下渲染 AO，比 Full Resolution 要模糊不少，但性能却可以提升至 4 倍多，之前说过显卡 4080 在 4k 下 Full Resolution 下 SSAO + Spatial + Temporal Filter 总耗时大约 1.5 ms，而 Half Resolution 下多出下采样和上采样阶段总耗时大约在 0.3 ~ 0.4 ms，性能提升非常明显。

# 其他说明
## SSDO
这里简单介绍一下经常能看到的名词 SSDO，即 **屏幕空间定向遮蔽 Screen-Space Directional Occlusion (SSDO)**，它对 SSAO 技术的一种改良。从它的名字中带有 Occlusion 以及其与 SSAO 的关系，很容易让人误解它是 Ambient Occlusion 技术之一，但实际上 SSDO 应该更偏向于 **Diffuse Global Illumination**，可以说是 **SSGI** 的一种。它在 2009 年由 Tobias Ritschel 提出，可以在《GPU Pro 1》中的 IV Global Illumination - 2 Screen-SpaceDirectionalOcclusion 看到该算法的详细内容。SSAO 只考虑一定范围内的遮挡关系，而 SSDO 在计算遮挡关系的同时还会考虑光照信息，所以 SSDO 可以认为是一定范围内 Diffuse Global Illumination。其思想大致如下，使用类似 SSAO 的方法，在一个半球空间中均匀散布一些采样点，若这些采样点被遮挡，则采样遮挡位置的颜色，若这些采样点未被遮挡，则采样 Irradiance Environment Map。其实我觉得这个技术作为一种廉价的 Diffuse Global Illumination 技术应该挺好用的，虽然我没有尝试过，讲道理独立 3d 游戏真的可以使用该技术实现一个简单的全局光照。