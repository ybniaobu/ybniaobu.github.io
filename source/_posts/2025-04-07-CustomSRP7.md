---
title: Unity Custom SRP 基础（七）
date: 2025-04-07 20:57:36
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2025/04/07/2zi6pj3XtAPHvmy.gif
mathjax: true
description: 本笔记的主要内容包含XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX。
---

> 本笔记是关于 Unity 的**自定义可编程渲染管线**的入门基础，即 **SRP (Scriptable Rendering Pipeline)**，主要参考了著名的教程 https://catlikecoding.com/ 的 Custom SRP Tutorial，以及知乎上各位图形学大神们的文章。  
>    
> 笔者使用的 Unity 版本是 6000.0.27f1，Core RP Library 的版本是 17.0.3。

# Particles
Unity 自带的**内置 Particle System** 大部分工作都是 CPU 计算的，而实现项目 CPU 瓶颈居多，所以它只适用于一些只使用少量粒子（数千以内）的简单特效，比如烟雾、火焰、爆炸等。而 **Visual Effect Graph** 是基于 GPU 计算，适合需要高质量、复杂粒子效果的场景。教程中只涉及内置 Particle System，没有涉及如何接入 Visual Effect Graph，需要自行研究如何接入。

其实什么都不用做就已经可以使用 Particle System 了，通过 GameObject / Effects / Particle System 创建即可。Particle System 可以使用任意材质，只不过使用并非专门为内置粒子系统而写的任意材质会带有一定的限制。教程中也只涉及 Particle System 的 Unlit Shader，不涉及 Particle Lit Shader ，但 Lit 跟 Unlit 类似，只是多了光照计算等，可以查阅 URP 或 HDRP。

## Unlit Particles Shader
我们先将自己的 Unlit Shader 全部复制出来，新建一个 ParticlesUnlit.Shader 然后粘贴进去，将 Shader 菜单路径改一下就行：  

    Shader "Custom RP/Particles/Unlit"
    {
        ...
    }

顺便提一下，对于内置粒子系统，GPU instancing 是不起作用的。然后将自己的 ParticlesUnlit.Shader 附给 Particle System 的 Renderer 就可以渲染粒子了：  

> 教程里说 Particle System 不支持 GPU instancing，但是我看官方文档时，发现是可以支持的，详见：https://docs.unity3d.com/Manual/gpu-instancing-particle-systems.html 。只是需要将 Particle System 的 Renderer 模式改为 Mesh，点击 Enable GPU Instancing，并且需要一个支持 Particle GPU instancing 的 Shader。

<div  align="center">  
<img src="https://s2.loli.net/2025/04/09/1dkv7G9OBUXMqi4.jpg" width = "60%" height = "60%" alt="图115 - 使用了一张圆形渐变贴图的 billboard particles"/>
</div>

## Vertex Color
Particle System 里面的很多颜色属性都是通过顶点色来传递的，比如 Start Color、Color over Lifetime、Color by Speed 等等，所以我们要为 Particle Unlit Shader 增加顶点色以使用这些属性：  

    struct Attributes
    {
        ...
        float4 color : COLOR;
        ...
    };

    struct Varyings
    {
        ...
        float4 color : COLOR;
        ...
    };

    Varyings ParticlesUnlitVert(Attributes IN)
    {
        Varyings OUT;
        ...
        OUT.color = IN.color;
        ...
        return OUT;
    }

    float4 ParticlesUnlitFrag(Varyings IN) : SV_Target
    {
        ...
        return float4(albedo.rgb * IN.color.rgb + emission * IN.color.rgb, albedo.a * IN.color.a);
    }

当然你也可以为顶点色和 base color 的混合做正片叠底 Multiply 以外的混合方式。

<div  align="center">  
<img src="https://s2.loli.net/2025/04/10/4jfHxRgLF5AUvzl.jpg" width = "60%" height = "60%" alt="图116 - Start Color 选择 Random Color 设置从黑到白渐变的粒子效果"/>
</div>

还有一点就是，当粒子有不同的颜色时，排序上就会出现问题，这点跟透明物体的排序问题是一致的。所以我们最好在 Particle System 的 Renderer 中将 Sort Mode 改为 By Distance。

## Flipbooks
Flipbooks 就是粒子的序列帧动画，让每个粒子播放一段动画。其实不用做任何事情就已经可以使用 Flipbooks 了，我们点开 Particle System 的 Texture Sheet Animation 就可以，并将 Shader 赋予如下贴图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/10/bhZ3VTRavrOuzmI.jpg" width = "25%" height = "25%" alt="图117 - flipbook texture"/>
</div>

这张贴图是 4 × 4 的，故 Texture Sheet Animation 里的 Tiles 属性也要设置为 4 × 4。然后就会出现类似一下效果，我特意把循环速度调慢了以降低动画播放速度方便看清：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/10/CyU4bZnsNmKXvf9.gif" width = "60%" height = "60%" alt="图118 - flipbook particle 效果"/>
</div>

可以看到每个粒子的动画播放速度非常得慢，并且帧与帧之间衔接不是很顺畅，这就需要对帧与帧之间做混合。

### Flipbook Blending
为了方便混合，我们需要传递第二套 uv 以及一个 animation blend 参数给 Shader，同时我们需要在 Particle System 的 Renderer 里面勾选上 Custom Vertex Streams 并添加 UV2 和 AnimBlend，如下图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/10/2ASenfjD4R59pVs.jpg" width = "40%" height = "40%" alt="图119 - Custom Vertex Streams"/>
</div>

然后我们就要去 Shader 里添加这些属性，首先添加一个 Flipbook Blending 的开关，如下：  

    Shader "YPipeline/Particles/Unlit"
    {
        Properties
        {
            ...
            [Toggle(_FLIPBOOK_BLENDING)] _FlipbookBlending ("Flipbook Blending", Float) = 0.0
        }

        SubShader
        {
            Pass
            {
                ...
                #pragma shader_feature_local _FLIPBOOK_BLENDING
                ...
            }
        }
    }

之后 Pass 里的修改如下：  

    struct Attributes
    {
        ...
        #if defined(_FLIPBOOK_BLENDING)
            float4 uv : TEXCOORD0;
            float uvBlend : TEXCOORD1;
        #else
            float2 uv : TEXCOORD0;
        #endif
    }

    struct Varyings
    {
        ...
        float2 uv : TEXCOORD0;

        #if defined(_FLIPBOOK_BLENDING)
            float3 uv2AndBlend : TEXCOORD1;
        #endif
    };

    Varyings ParticlesUnlitVert(Attributes IN)
    {
        ...
        float4 baseST = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseTex_ST);
        OUT.uv = IN.uv.xy * baseST.xy + baseST.zw;
        #if defined(_FLIPBOOK_BLENDING)
            OUT.uv2AndBlend.xy = IN.uv.zw * baseST.xy + baseST.zw;
            OUT.uv2AndBlend.z = IN.uvBlend;
        #endif
        return OUT;
    }

    float4 ParticlesUnlitFrag(Varyings IN) : SV_Target
    {
        ...
        #if defined(_FLIPBOOK_BLENDING)
            albedo = lerp(albedo, SAMPLE_TEXTURE2D(_BaseTex, sampler_Trilinear_Repeat_BaseTex, IN.uv2AndBlend.xy), IN.uv2AndBlend.z);
        #endif
        ...
    }

效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/10/6FSR5ng9CrQo43j.gif" width = "60%" height = "60%" alt="图120 - Flipbook Blending"/>
</div>

可以看到粒子动画帧与帧之间的衔接变得舒服很多。

## Fading Near Camera
当摄像机在粒子发射的区域内部，由于粒子会离摄像机很近，从而占据画面的大部分并影响到玩家观察场景。Particle System 中 Renderer 的 Max Particle Size 可以限制粒子最大能占据画面的百分比，但是该属性会导致，当粒子达到最大大小并逐渐接近近裁切平面时，粒子看起来在变小，所以效果并不是很好。

另外一个方案就是，我们可以让粒子根据深度进行渐变，越接近屏幕越透明，这样子当模拟大气效果时，会有个更好的效果。首先我们为 Particle Unlit Shader 添加 `_CAMERA_NEAR_FADE` 关键字开关属性，以及一个距离和距离范围属性：  

    [Toggle(_CAMERA_NEAR_FADE)] _CameraFading ("Camera Fading", Float) = 0.0
    _NearFadeDistance ("Near Fade Distance", Range(0.0, 10.0)) = 1
    _NearFadeRange ("Near Fade Range", Range(0.0, 10.0)) = 1

别忘了加上 shader_feature：  

    #pragma shader_feature _NEAR_FADE

然后在 Pass 里，根据深度值和 Distance 以及 Range 属性进行 lerp 改变 alpha 的值，深度值可以根据 SV_POSITION 语义来获取，如何获取就不解释了，忘了看之前相关文章，就是正交投影需要注意一下，代码如下（`unity_OrthoParams` 和 `_ProjectionParams` 都是 UnityInput）：  

    UNITY_INSTANCING_BUFFER_START(UnityPerMaterial)
        ...
        UNITY_DEFINE_INSTANCED_PROP(float, _NearFadeDistance)
        UNITY_DEFINE_INSTANCED_PROP(float, _NearFadeRange)
        ...
    UNITY_INSTANCING_BUFFER_END(UnityPerMaterial)

    float GetViewDepthFromSVPosition(float4 positionHCS)
    {
        if (unity_OrthoParams.w < 0.5f) // Perspective
        {
            return positionHCS.w;
        }
        else // Orthographic
        {
            float normalizedDepth = positionHCS.z;

            #if UNITY_REVERSED_Z
                normalizedDepth = 1.0 - normalizedDepth;
            #endif

            return (_ProjectionParams.z - _ProjectionParams.y) * normalizedDepth + _ProjectionParams.y;
        }
    }

    ...

    float4 ParticlesUnlitFrag(Varyings IN) : SV_Target
    {
        ...
        #if defined(_CAMERA_NEAR_FADE)
            float depth = GetViewDepthFromSVPosition(IN.positionHCS);
            float nearFadeDistance = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _NearFadeDistance);
            float nearFadeRange = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _NearFadeRange);
            float nearAttenuation = (depth - nearFadeDistance) / nearFadeRange;
            albedo.a *= saturate(nearAttenuation);
        #endif
        ...
    }

开启 _CAMERA_NEAR_FADE 后，改变 NearFadeDistance 效果如下：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/11/2ZClqOErxANXIR7.gif" width = "60%" height = "60%" alt="图121 - NearFadeDistance 逐渐变大的 Camera Near Fade 效果"/>
</div>

## Soft Particles
当粒子和非透明物体重合交叠时，部分粒子在物体前部分粒子在物体后，就会显得特别突兀。为了解决这个问题，我们需要对比粒子和物体的深度，当物体在粒子背后很近时，将粒子 fade out，于是就需要采样 depth texture，depth texture 的相关前置内容见下一章节的 Copy Depth 小节。

### 重构 View Space Depth
这部分内容的逻辑 [《Unity Shader入门精要》读书笔记（四）](https://ybniaobu.github.io/2023/12/19/2023-12-19-UnityShader4/#%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0-%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%92%8C%E6%B3%95%E7%BA%BF%E7%BA%B9%E7%90%86) 里面已经讲得很详细了，就不再赘述了。下面就简单记录下步骤：  

因为我们要用屏幕空间的 uv 坐标对 `_CameraDepthTexture` 进行采样，最简单的计算屏幕 uv 的方式是使用带 SV_POSITION 语义的 PositionHCS 的 xy 分量除以屏幕分辨率，屏幕分辨率可以通过 UnityInput 的 `_ScreenParams` 获取，我们需要添加在 UnityInput 里。与此同时，我们重构 View Space Depth 需要用到远近裁切平面的相关参数，需要使用到 `_ZBufferParams`，也要添加在 UnityInput 里：  

    ...
    float4 _ProjectionParams;
    // x = width
    // y = height
    // z = 1 + 1.0/width
    // w = 1 + 1.0/height
    float4 _ScreenParams;
    // x = 1-far/near
    // y = far/near
    // z = x/far
    // w = y/far
    // or in case of a reversed depth buffer (UNITY_REVERSED_Z is 1)
    // x = -1+far/near
    // y = 1
    // z = x/far
    // w = 1/far
    float4 _ZBufferParams;
    float4 unity_OrthoParams;
    ...

然后因为 Unity 内置的使用 _ZBufferParams 的 `LinearEyeDepth()` 函数不适用于正交投影的情形下，需要区分：  

    float GetViewDepthFromDepthTexture(float sampledDepth)
    {
        if (unity_OrthoParams.w < 0.5f) // Perspective
        {
            return LinearEyeDepth(sampledDepth, _ZBufferParams);
        }
        else // Orthographic
        {
            #if UNITY_REVERSED_Z
            sampledDepth = 1.0 - sampledDepth;
            #endif
            return (_ProjectionParams.z - _ProjectionParams.y) * sampledDepth + _ProjectionParams.y;
        }
    }

### Soft Particles Fading
可以获取到深度了，就可以实现 Soft Particles 的效果了。首先为 Soft Particles 添加几个 Shader 参数：  

    [Toggle(_SOFT_PARTICLES)] _SoftParticles ("Soft Particles", Float) = 0
    _SoftParticlesDistance ("Soft Particles Distance", Range(0.0, 10.0)) = 0
    _SoftParticlesRange ("Soft Particles Range", Range(0.01, 10.0)) = 1

别忘了添加 shader feature：  

    #pragma shader_feature _SOFT_PARTICLES

然后在 UnityPerMaterial 里加上上面说的参数变量，片元着色器里的计算跟 Camera Near Fade 很类似：  

    float viewDepth = GetViewDepthFromSVPosition(IN.positionHCS);
    #if defined(_SOFT_PARTICLES)
        float2 screenUV = IN.positionHCS.xy / _ScreenParams.xy;
        float sampledDepth = SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_CameraDepthTexture, screenUV, 0);
        float viewSampledDepth = GetViewDepthFromDepthTexture(sampledDepth);
        float depthDelta = abs(viewSampledDepth - viewDepth);
        float softParticlesDistance = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _SoftParticlesDistance);
        float softParticlesRange = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _SoftParticlesRange);
        float softParticlesAttenuation = (depthDelta - softParticlesDistance) / softParticlesRange;
        albedo.a *= saturate(softParticlesAttenuation);
    #endif

从不开启 Soft Particles 到开启后的效果如下： 

<div  align="center">  
<img src="https://s2.loli.net/2025/04/17/JRPLleGKdHscoDT.gif" width = "60%" height = "60%" alt="图122 - 前半为关闭 Soft Particles 的效果，后半为开启 Soft Particles 的效果"/>
</div>

## Distortion
跟 Soft Particles 类似，Soft Particles 需要 Depth 信息，Distortion 这种透明效果则需要场景颜色信息。所以需要采样 Color Texture，Color Texture 的前置知识详见下一章节的 Copy Color 小节。

### Distortion Map
Distortion Map 是一个类似于法线贴图的贴图，它可以用于随机化采样场景颜色的 uv 从而达到一种扭曲的效果，类似如下贴图：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/17/guX4UYb5sGRljdp.png" width = "15%" height = "15%" alt="图123 - Round Particle Distortion Map"/>
</div>

接下来就是为 Distortion 添加 Shader 参数：  

    [Toggle(_DISTORTION)] _Distortion ("Distortion", Float) = 0.0
    [NoScaleOffset] _DistortionTex ("Distortion Texture", 2D) = "bump" {}
    _DistortionStrength ("Distortion Strength", Range(0.0, 2.0)) = 0.1
    
别忘了添加 shader feature：

    #pragma shader_feature _DISTORTION

然后为这些参数添加变量。使用 Distortion Map 跟 Normal Map 类似，需要解码，解码后我们只需要 xy 分量对 uv 进行偏移，并使用偏移后的 uv 对 `_CameraColorTexture` 进行采样，先直接输出采样的颜色：  

    #if defined(_DISTORTION)
        float4 packedDistortion = SAMPLE_TEXTURE2D(_DistortionTex, sampler_Trilinear_Repeat_BaseTex, IN.uv);
        #if defined(_FLIPBOOK_BLENDING)
            float4 packedDistortion2 = SAMPLE_TEXTURE2D(_DistortionTex, sampler_Trilinear_Repeat_BaseTex, IN.uv2AndBlend.xy);
            packedDistortion = lerp(packedDistortion, packedDistortion2, IN.uv2AndBlend.z);
        #endif
        float distortionStrength = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _DistortionStrength);
        float3 distortion = UnpackNormalScale(packedDistortion, distortionStrength);
    
        float3 sampledColor = SAMPLE_TEXTURE2D_LOD(_CameraColorTexture, sampler_LinearClamp, screenUV + distortion * albedo.a, 0).rgb;
        albedo.rgb = sampledColor;
    #endif

这样就可以有扭曲空间的效果了：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/17/sroxIVtXEFcHyeB.gif" width = "60%" height = "60%" alt="图124 - Round Particle Distortion Effect"/>
</div>

### Distortion Blend
之前因为我们直接输出了采样的 `_CameraColorTexture` 的颜色，但是有时候我们想要粒子原来的颜色，这时候我们可以使用 `_DistortionBlend` 在两者之间做混合：  

    _DistortionBlend("Distortion Blend", Range(0.0, 1.0)) = 1

当 `_DistortionBlend` 为 1 时，只能看到 Distortion 的效果；当 `_DistortionBlend` 为 0 时，能看到 Distortion 和粒子原来颜色混合的效果。并且我们希望粒子原来的 alpha 越小，即越透明 Distortion 效果越弱：  

    #if defined(_DISTORTION)
        ...
        float3 sampledColor = SAMPLE_TEXTURE2D_LOD(_CameraColorTexture, sampler_LinearClamp, screenUV + distortion * albedo.a, 0).rgb;
        float distortionBlend = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _DistortionBlend);
        albedo.rgb = lerp(sampledColor, albedo.rgb, saturate(albedo.a - distortionBlend));
    #endif

下面的效果是上面讲 Flipbook 时的那张 Flipbook Map 和它对应的 Flipbook Distortion Map 共同产生的，Flipbook Distortion Map 在教程里有，我这里就不放出来了：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/17/yGLZNih6I3c4nbf.gif" width = "60%" height = "60%" alt="图125 - Filpbook Particle Distortion Blend Effect（关闭了 Camera Near Fade）"/>
</div>

# Depth & Color Texture
我们之前只使用了一张 frame buffer，同时包含了颜色和深度信息。但其实深度模板缓冲区和目标视图缓冲区（color buffer）的资源是分开的，即**颜色附件 Color Attachment** 和**深度/模板附件 Depth/Stencil Attachment**，我们可以根据需求分别创建。

首先替换之前的 `_CameraFrameBuffer` 相关代码：  

``` C#
public static readonly int k_ColorBufferId = Shader.PropertyToID("_CameraColorBuffer");
public static readonly int k_DepthBufferId = Shader.PropertyToID("_CameraDepthBuffer");

...
{
    data.buffer.GetTemporaryRT(RenderTargetIDs.k_ColorBufferId, data.camera.pixelWidth, data.camera.pixelHeight, 0, FilterMode.Bilinear, asset.enableHDRFrameBufferFormat ? RenderTextureFormat.DefaultHDR : RenderTextureFormat.Default);
    data.buffer.GetTemporaryRT(RenderTargetIDs.k_DepthBufferId, data.camera.pixelWidth, data.camera.pixelHeight, 32, FilterMode.Point, RenderTextureFormat.Depth);
    data.buffer.SetRenderTarget(new RenderTargetIdentifier(RenderTargetIDs.k_ColorBufferId), 
                                RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store,
                                new RenderTargetIdentifier(RenderTargetIDs.k_DepthBufferId),
                                RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store);
}
```

然后别忘了 ReleaseTemporaryRT。

## Depth Prepass
教程中没有提到 Depth Prepass，这个技术是一个较为简单的优化技术，在复杂场景中能有效地提升性能，故顺便在这里实现一下。

### 基本原理
首先需要注意区分 **Early-Z** 和 **Depth Prepass**（Z-Prepass）：  
**①Early-Z** 是一个移动设备 GPU 的 TBR/TBDR 架构下的硬件技术（但是好像现在大部分 GPU 都支持，网上的资料都有点老了），它在硬件层面上修改了渲染管线，在光栅化阶段和片元着色器阶段中间，加入了一个 Early-Z 阶段，这个阶段进行的操作和原本逐片元操作/输出合并阶段的 z-test（为了与 early-z 区别，这个阶段也会被成为 **Late-Z**）操作完全一样。这样做的目的是为了提前剔除掉被遮挡的片元，减少 over-draw 从而节省 GPU 性能。Early-Z 的问题在于，若开启了 alpha test ，或者有丢弃像素的操作，又或者在片元着色器开启了深度写入，就会使 Early-Z 失效，所以一般情况下会配合软件层面的 Depth Prepass 使用。  
**②Depth Prepass** 是一个软件技术，其做法就是使用两个 pass，第一个 pass 渲染物体的深度（即 Depth Prepass），第二个 pass 渲染该物体的颜色。在渲染深度时，只开启 Zwrite 和 ZTest 渲染物体的深度；在渲染颜色时，关闭 Zwrite，并将 ZTest 设置为 **Equal**，注意这里深度比较函数必须是 Equal 不能是 LessEqual，Equal 下性能是最优的。Depth Prepass 可以有效地减少片元着色器的工作，特别适用于复杂的片元着色器（比如 PBR）以及复杂的场景（比如场景中有较多的花草树木，以及头发渲染）。

### 实现
***①第一步 Depth Prepass：***  
首先为 Opaque 和 AlphaTest 的物体准备一个 Depth Prepass，Shader 里类似如下：  

    Pass
    {
        Name "Depth"
        
        Tags { "LightMode" = "Depth" }
        
        ZWrite On
        ColorMask 0
        Cull [_Cull]
        
        HLSLPROGRAM
        #pragma target 4.5

        #pragma vertex DepthVert
        #pragma fragment DepthFrag

        #pragma shader_feature_local_fragment _CLIPPING

        #pragma multi_compile _ LOD_FADE_CROSSFADE

        #include "StandardForwardDepthPass.hlsl"
        ENDHLSL
    }

然后只需要在片元着色器中做裁切就行，大致如下：  

    struct Attributes
    {
        float4 positionOS   : POSITION;
        float2 uv           : TEXCOORD0;
    };

    struct Varyings
    {
        float4 positionHCS  : SV_POSITION;
        float2 uv           : TEXCOORD0;
    };

    Varyings DepthVert(Attributes IN)
    {
        Varyings OUT;
        OUT.uv = TRANSFORM_TEX(IN.uv, _BaseTex);
        OUT.positionHCS = TransformObjectToHClip(IN.positionOS.xyz);
        return OUT;
    }

    float DepthFrag(Varyings IN) : SV_DEPTH
    {
        float baseTexAlpha = SAMPLE_TEXTURE2D(_BaseTex, sampler_Trilinear_Repeat_BaseTex, IN.uv).a;
        float opacityTexAlpha = SAMPLE_TEXTURE2D(_OpacityTex, sampler_Trilinear_Repeat_BaseTex, IN.uv).r;
        float alpha = baseTexAlpha * opacityTexAlpha * _BaseColor.a;

        #if defined(_CLIPPING)
            clip(alpha - _Cutoff);
        #endif
            
        #if defined(LOD_FADE_CROSSFADE)
            float dither = InterleavedGradientNoise(IN.positionHCS.xy, 0);
            float isNextLodLevel = step(unity_LODFade.x, 0);
            dither = lerp(-dither, dither, isNextLodLevel);
            clip(unity_LODFade.x + dither);
        #endif

        return IN.positionHCS.z;
    }

***②第二步修改渲染颜色的 Pass：***  
这一步只需要将原先渲染颜色的 Pass 的 ZWrite 关闭，ZTest 设置为 **Equal** 即可：  

    Pass 
    {
        ...
        ZWrite Off
        ZTest Equal
        Cull [_Cull]
        ...
    }


***③第三步在 SRP 中提交绘制指令：***  
首先为 Depth Prepass 创建一个 ShaderTagId：  

``` C#
public static ShaderTagId k_DepthShaderTagId = new ShaderTagId("Depth");
```

然后就是绘制，Filtering & Sorting Settings 可以和 Draw Opaque、alphaTest 流程中的局部参数共用，流程就不重复说明了，详见第一篇文章。这里要注意一下的是绘制时无需传递 perObjectData，还有就是因为只写入深度，ClearRenderTarget 时只需要清除深度：  

``` C#
// Filtering & Sorting
FilteringSettings opaqueFiltering = 
    new FilteringSettings(new RenderQueueRange(2000, 2449));

FilteringSettings alphaTestFiltering =
    new FilteringSettings(new RenderQueueRange(2450, 2499));

SortingSettings opaqueSorting = new SortingSettings(data.camera)
{
    criteria = SortingCriteria.CommonOpaque
};

SortingSettings alphaTestSorting = new SortingSettings(data.camera)
{
    criteria = SortingCriteria.OptimizeStateChanges
};

// Depth PrePass
data.buffer.BeginSample("Depth PrePass");
data.buffer.SetRenderTarget(new RenderTargetIdentifier(RenderTargetIDs.k_DepthBufferId), RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store);
data.buffer.ClearRenderTarget(true, false, data.camera.backgroundColor.linear);

DrawingSettings depthOpaqueDrawing = new DrawingSettings(k_DepthShaderTagId, opaqueSorting)
{
    enableInstancing = asset.enableGPUInstancing,
    perObjectData = PerObjectData.None
};

DrawingSettings depthAlphaTestDrawing = new DrawingSettings(k_DepthShaderTagId, alphaTestSorting)
{
    enableInstancing = asset.enableGPUInstancing,
    perObjectData = PerObjectData.None
};

RendererListParams depthOpaqueRendererListParams =
    new RendererListParams(data.cullingResults, depthOpaqueDrawing, opaqueFiltering);

RendererListParams depthAlphaTestRendererListParams =
    new RendererListParams(data.cullingResults, depthAlphaTestDrawing, alphaTestFiltering);

RendererList depthOpaqueRendererList = data.context.CreateRendererList(ref depthOpaqueRendererListParams);
RendererList depthAlphaTestRendererList = data.context.CreateRendererList(ref depthAlphaTestRendererListParams);

data.buffer.DrawRendererList(depthOpaqueRendererList);
data.buffer.DrawRendererList(depthAlphaTestRendererList);

data.buffer.EndSample("Depth PrePass");
```

最后在渲染颜色阶段，即 Draw Opaque、alphaTest 时，将渲染好的 DepthBuffer 与 ColorBuffer 同时设置为 RenderTarget，并且 Depth Buffer 的 RenderBufferLoadAction 是 Load，然后 ClearRenderTarget 只需要清除颜色即可：  

``` C#
data.buffer.SetRenderTarget(new RenderTargetIdentifier(RenderTargetIDs.k_ColorBufferId), 
                RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store,
                new RenderTargetIdentifier(RenderTargetIDs.k_DepthBufferId),
                RenderBufferLoadAction.Load, RenderBufferStoreAction.Store);
            
data.buffer.ClearRenderTarget(false, true, data.camera.backgroundColor.linear);
```

大致测试了下面渲染场景的帧率变化（在一个点光源下，开了后处理）：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/17/LzruBWN9wt6mKRc.png" width = "100%" height = "100%" alt="图126 - 左图：无 Depth PrePass 时总消耗 5.3 ms；右图：Depth PrePass 后总消耗 3.5 ms"/>
</div>

## Copy Depth
我们在渲染的时候，有时候需要采样 depth buffer，比如之前的 Soft Particles 效果、SSAO 等等。但是当我们以 depth buffer 作为 render target 的时候，不能同时对 depth buffer 进行采样，所以我们需要将 depth buffer 复制到 depth texture 里，Unity URP 的 CopyDepth 阶段就是在做这件事情。我们新创建一个 `_CameraDepthTexture` 的 RT 资源，创建的过程就不再赘述了，跟上面 `_CameraDepthBuffer` 一样。

我们之前后处理中提到的 Copy Shader 复制的是整个 Color Buffer，我们现在要创建一个复制 Depth Buffer 的版本，即 Copy Depth Shader，Shader 代码大致如下，将 ColorMask 设置为 0，开启深度写入：  

    Shader "Hidden/YPipeline/CopyDepth"
    {
        SubShader
        {
            Tags
            {
                "RenderType" = "Opaque"
            }
            
            ZTest Always
            ZWrite On
            ColorMask 0
            Cull Off

            Pass
            {
                Name "CopyDepth"
                
                HLSLPROGRAM
                #pragma target 3.5
                
                #pragma vertex CopyDepthVert
                #pragma fragment CopyDepthFrag

                #include "CopyDepthPass.hlsl"
                ENDHLSL
            }
        }
    }

然后顶点着色器跟之前的 Copy Shader 一样，片元着色器目标是 SV_DEPTH，而不是 SV_TARGET：  

    TEXTURE2D(_CameraDepthBuffer);
    SAMPLER(sampler_CameraDepthBuffer);

    struct Varyings
    {
        float4 positionHCS  : SV_POSITION;
        float2 uv           : TEXCOORD0;
    };

    Varyings CopyDepthVert(uint vertexID : SV_VertexID)
    {
        Varyings OUT;
        OUT.uv = float2((vertexID << 1) & 2, vertexID & 2);
        OUT.positionHCS = float4(OUT.uv * 2.0 - 1.0, UNITY_NEAR_CLIP_VALUE, 1.0);
        
        if (_ProjectionParams.x < 0.0) OUT.uv.y = 1.0 - OUT.uv.y;
        
        // #if UNITY_UV_STARTS_AT_TOP
        //     OUT.uv.y = 1.0 - OUT.uv.y;
        // #endif
        
        return OUT;
    }

    float CopyDepthFrag(Varyings IN) : SV_DEPTH
    {
        return SAMPLE_TEXTURE2D_LOD(_CameraDepthBuffer, sampler_CameraDepthBuffer, IN.uv, 0).r;
    }

具体什么时候 Copy Depth 我是参考了 URP 放在 Depth Prepass 和 Draw Opaque & AlphaTest 之间。


## Gizmos and Depth
之前在 Post Processing 时提到过，Gizmo 的绘制依赖于原来的 CameraTarget 中的深度信息。若没有深度信息，gizmos 就不会被物体遮挡。现在我们可以将深度信息复制进 CameraTarget 了：  

``` C#
if (Handles.ShouldRenderGizmos())
{
    BlitUtility.CopyDepth(data.buffer, RenderTargetIDs.k_DepthBufferId, BuiltinRenderTextureType.CameraTarget);
    RendererList gizmosRendererList = data.context.CreateGizmoRendererList(data.camera, GizmoSubset.PreImageEffects);
    data.buffer.DrawRendererList(gizmosRendererList);
}
```

这样子就可以看到 Gizmo 被正确地遮挡了：  

<div  align="center">  
<img src="https://s2.loli.net/2025/04/17/WGaoFULMK1l9JjR.jpg" width = "15%" height = "15%" alt="图127 - Gizmos recognizing depth"/>
</div>

## Copy Color
很多透明物体的效果都需要用到场景颜色信息，比如透明物体的折射、透视、扭曲等等，简单使用透明度混合的透明效果其实不是很好，不能很好地做到玻璃、水面等效果。此时我们可以在 Opaque、AlphaTest 和 Skybox 渲染完成后，将 Color Buffer 复制到 `_CameraColorTexture` 纹理当中，然后在实现透明物体的效果时，采样它。创建 `_CameraColorTexture` 纹理跟上面 `_CameraDepthTexture` 类似，然后在 Skybox 渲染后进行复制即可：  

``` C#
data.buffer.BeginSample("Copy Color");
data.buffer.GetTemporaryRT(RenderTargetIDs.k_ColorTextureId, data.camera.pixelWidth, data.camera.pixelHeight, 0, FilterMode.Bilinear, 
    asset.enableHDRFrameBufferFormat ? RenderTextureFormat.DefaultHDR : RenderTextureFormat.Default);
BlitUtility.BlitTexture(data.buffer, RenderTargetIDs.k_ColorBufferId, RenderTargetIDs.k_ColorTextureId);

data.buffer.EndSample("Copy Color");
```

还是别忘了 ReleaseTemporaryRT。这里顺便提一下，教程中类似于 URP 在 PipelineAsset 里提供了一个选项用于是否开启 Copy Color（Copy Depth 也有一个选项，即 URP 中的 Depth Texture 和 Opaque Texture 选项），若不开启则将 `_CameraColorTexture` 设置为纯灰贴图。因为我觉得 URP 是通用渲染管线，所以它给了这个选项，而我们的渲染管线完全可以根据项目需求选择是否 Copy Color，所以我没有添加这个选项，若不需要 Copy Color，完全可以注释掉这些代码。


# Render Scale
