---
title: Temporal Anti-Aliasing (TAA)
date: 2025-06-21 19:43:03
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2025/06/23/kEPO3zg8IRXwUFC.gif
mathjax: true
description: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
---

> 本来是想先实现 SSGI 或者 SSR 的，但是研究着研究着，就发现 SSGI 或 SSR 的思想都有些许依赖于 SSAO 相关技术算法（SSAO、SSDO、HBAO、GTAO 等等），然后就想着要不先实现 SSAO，但是研究着研究着，又发现 SSAO 等相关技术跟 PCSS 一样较为依赖于降噪技术，降噪技术又可以分为**空间滤波 Spatial Filter** 以及**时间滤波 Temporal Filter**。所以为了打好基础，最终决定先实现 **Temporal Anti-Aliasing (TAA)**，对**抖动 jitter** 有个更深入的理解后，再进入全局光照的实现，顺便也能优化一下 PCSS。
>   
> 本篇文章具体实现是在 Unity 当中，主要参考了以下几篇文章或演讲或 PPT（加粗的文章更综合地介绍了 TAA 技术，建议优先观看）：  
> ① High Quality Temporal Supersampling（SIGGRAPH 2014）：https://advances.realtimerendering.com/s2014/#_HIGH-QUALITY_TEMPORAL_SUPERSAMPLING or https://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf ；  
> ② **A Survey of Temporal Antialiasing Techniques** ：https://research.nvidia.com/labs/rtr/publication/yang2020survey/ ；  
> ③ **Temporal AA and the Quest for the Holy Trail** ：https://www.elopezr.com/temporal-aa-and-the-quest-for-the-holy-trail/ ；  
> ④ Temporal Reprojection Anti-Aliasing in INSIDE（GDC 2016）：https://www.gdcvault.com/play/1022970/Temporal-Reprojection-Anti-Aliasing-in ；  
> ⑤ An Excursion in Temporal Supersampling（GDC 2016）：https://developer.download.nvidia.com/gameworks/events/GDC2016/msalvi_temporal_supersampling.pdf ；  
> ⑥ Temporal Antialiasing in Uncharted 4（SIGGRAPH 2016）：https://advances.realtimerendering.com/s2016/ ；  
> ⑦ Temporal Antialiasing Starter Pack ：https://alextardif.com/TAA.html ；  
> ⑧ Temporal Anti Aliasing – Step by Step ：https://ziyadbarakat.wordpress.com/2020/07/28/temporal-anti-aliasing-step-by-step/ ；  
> ⑨ Adaptive Temporal Antialiasing ：https://research.nvidia.com/publication/2018-08_adaptive-temporal-antialiasing ；  
> ⑩ Dynamic Temporal Antialiasing and Upsampling in Call of Duty（SIGGRAPH 2017）：https://www.activision.com/cdn/research/Dynamic_Temporal_Antialiasing_and_Upsampling_in_Call_of_Duty_v4.pdf 。  


# TAA 原理及流程简单介绍
**时域抗锯齿 Temporal Anti-Aliasing (TAA)** 用一句话概括可以说是使用了历史帧中的**子/次/亚像素 subpixel** 进行累计从而更有效地实现**超采样抗锯齿 Supersampling Anti-Aliasing (SSAA)**。相对于 SSAA 或 MSAA 来说，TAA 有效地将一个像素多次采样分摊到了连续的几帧当中，即以一个像素一次采样的代价实现了超采样抗锯齿。与此同时，单帧光栅化单像素采样，实际上会“损失”一些亚像素信息的，而 TAA 通过抖动在历史帧中解析出单帧光栅化以及单像素采样中丢失的亚像素信息，从而能够更好地解决**几何锯齿 Geometric Aliasing**（几何光栅化锯齿） 和**着色锯齿 Shading Aliasing**（渲染锯齿），也相对来说增加了基于时序的稳定性。特别是当物体或相机移动时，图元边缘相对于采样点的位置不断变化，导致边缘像素的覆盖率计算结果不断变化，使得边缘像素的颜色在帧与帧之间发生微妙的、高频的亮度变化，而历史帧的混合可以减少这种不稳定性。虽然由于每帧的抖动，会让我们误以为更加不稳定，但实际上可以通过些许 trick 来减少感知上的闪烁问题，从而达到其他后处理抗锯齿技术无法达到的基于时序的稳定性。这也引出了 TAA 的两大难题：**鬼影 ghost** 问题和**闪烁 flicker** 问题，我们在后面重点解决这两个问题。

TAA 大致流程如下所示：  
①首先对摄像机进行**抖动 jitter**，这样子每一帧采样的是一个基于中心位置的被抖动过后的随机位置采样点，将当前帧采样颜色和历史颜色进行混合得到 TAA Target 贴图，后处理后输出。与此同时，将 TAA Target 复制为 TAA History/Accumulation 贴图，将累计的历史颜色信息用于下一帧进行混合；  
②由于相机或物体会进行移动，所以采样历史帧（TAA History）时，不能使用当前的 uv 值，否则采样到的颜色会不是当前像素点或物体的历史颜色信息。这就需要使用上一帧的矩阵进行**重投影 Reprojection** 计算出**运动向量 Motion Vector**，从而还原出当前像素在上一帧的屏幕位置。Motion Vector 又分为 **Camera Motion Vector** 和 **Object Motion Vector**，后面会详细说明；  
③由于运动时遮挡的变化，或者超出屏幕范围，又或者是灯光的变化，会让我们采样到跟当前帧不匹配的历史颜色信息。因此我们需要**验证 Validate** 或**矫正 rectify** 历史信息，以防止鬼影以及闪烁现象的产生。

<div align="center">  
<img src="https://s2.loli.net/2025/07/06/HCp3vjaQ4tswEUq.png" width = "60%" height = "60%" alt="图1 - TAA 基本流程"/>
</div>

下面就开始介绍具体的实现方式，我们先处理静态场景的情形，即摄像机和物体都不运动：

# 累计历史样本
## 抖动 Jitter
由于我们要对一个像素内的多个**子像素 subpixel** 进行采样，故我们需要对采样点的位置进行偏移，即**抖动 Jitter**，通常情况下会使用低差异序列中的 Halton 序列，从而实现更好的抗锯齿效果。UE4 默认使用了 Halton 序列的前 8 个样本，Playdead Studios 工作室（《INSIDE》、《地狱边境》的制作厂商）在 GDC 2016 的分享中有提到使用前 16 个样本可以产生更好的效果，我也采用了这个方式。Halton 序列等低差异序列的生成就不在这里赘述了，详见《Physically Based Rendering: From Theory To Implementation》中的第八章 Sampling and Reconstruction 的第六节 [Halton Sampler](https://www.pbr-book.org/4ed/Sampling_and_Reconstruction/Halton_Sampler) 。

<div align="center">  
<img src="https://s2.loli.net/2025/07/06/Fr3pSCQuLM2fJWl.jpg" width = "50%" height = "50%" alt="图2 - Halton Squence"/>
</div>

对采样点进行偏移的方式通常是修改相机的投影矩阵，只需修改矩阵中的两个变量即可：  

    ProjectionMatrix[0][2] += ( OffsetX * 2.0f – 1.0f ) / FrameBufferSize.Width;
    ProjectionMatrix[1][2] += ( OffsetY * 2.0f – 1.0f ) / FrameBufferSize.Height;

至于为什么要对 offset 乘 2 减 1 的原因是，Halton 序列即 offset 的范围是在 (0, 1)，我们希望采样点偏移的范围是在一个像素内，即在 (-0.5, 0.5) 之间，需要对 Halton 序列减去 0.5。又因为齐次除法后得到的 NDC 坐标的 x、y 分量都在 \[-1, 1\] 之间，而得到 uv 值在 (0, 1) 之间，故需要乘以 2 消除缩放影响。具体推导如下，假设 jitter 在 (-0.5, 0.5) 之间：  

$$ P'_{clip} = M_{persp}P_{view} = \begin{bmatrix} A & 0 & 2 \times jitter.x / width & 0 \\ 0 & B & 2 \times jitter.y / height & 0 \\ 0 & 0 & C & D \\ 0 & 0 & 1\,or\, -1 & 0 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix} = \begin{bmatrix} Ax + 2 \times jitter.x / width \times z \\ By + 2 \times jitter.y / height \times z \\ ... \\ ... \end{bmatrix} $$

$$ P'_{NDC} = \left[ \cfrac {A}{z}x + \cfrac {2 \times jitter.x}{width} , \cfrac {B}{z}y + \cfrac {2 \times jitter.y}{height} , ..., ... \right] $$

$$ P'_{ScreenUV} = \left[ \cfrac {A}{2z}x + \cfrac {jitter.x}{width} + 0.5 , \cfrac {B}{2z}y + \cfrac {jitter.y}{height} + 0.5 \right] $$

要注意，原 ScreenUV 为 $\, \left[ \cfrac {A}{2z}x + 0.5, \cfrac {B}{2z}y + 0.5 \right] \,$，故偏移了 $\, \left[ \cfrac {jitter.x}{width}, \cfrac {jitter.y}{height} \right] \,$，符合我们的要求。注意上面推导的是透视投影的情况，正交投影则需改变第一行第四位，以及第二行第四位，即 \[0\]\[3\] 和 \[1\]\[3\] ：

$$ P'_{clip} = M_{ortho}P_{view} = \begin{bmatrix} A & 0 & 0 & 2 \times jitter.x / width \\ 0 & B & 0 & 2 \times jitter.y / height \\ 0 & 0 & C & D \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix} = \begin{bmatrix} Ax + 2 \times jitter.x / width \\ By + 2 \times jitter.y / height \\ ... \\ ... \end{bmatrix} $$

$$ P'_{ScreenUV} = \left[ \cfrac {A}{2}x + \cfrac {jitter.x}{width} + 0.5 , \cfrac {B}{2}y + \cfrac {jitter.y}{height} + 0.5 \right] $$

为了方便控制偏移距离，可以给 jitter 乘以一个 jitterScale 参数，用于控制偏移的范围。得到修改的矩阵后，只需调用 `CommandBuffer.SetViewProjectionMatrices()` 即可实现抖动了，我用了一张金属度比较高的图片，方便观察闪烁现象：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/08/Hl3VvTNDFPCQh7n.gif" width = "512" height = "512" alt="图3 - Jitter"/>
</div>

## Exponential Blending
接下来就是将当前帧与历史帧进行混合了，直接混合所有历史帧肯定是不现实的，因为我们没法存储所有历史数据。绝大多数 TAA 的实现采用了类似递归的方式，将所有历史帧的累加结果存储到一张贴图当作，即 TAA History/Accumulation Texture，并采用了以下公式进行混合：  

$$ f_n(p) = \alpha \cdot s_n(p) + (1 - \alpha) \cdot f_{n-1} (\pi(p)) $$

其中 $\,f_n(p)\,$ 是第 n 帧的输出颜色，$\,\alpha\,$ 是混合系数，$\,s_n(p)\,$ 是当前帧颜色，$\,f_{n-1} (\pi(p))\,$ 是经过重投影后的累计历史帧。重投影后面再考虑，这里先考虑静态场景。在这个公式下，历史帧会被不断累计，当然随着时间的流逝，单一历史帧的影响会被无限缩小。

<div align="center">  
<img src="https://s2.loli.net/2025/07/08/ANXvRKOwUabtjDl.jpg" width = "45%" height = "45%" alt="图4 - 单一历史帧随着帧数增加所占的比例变化"/>
</div>

越老的历史帧所占的比例越来越小，在大部分情况下是很好的选择，因为场景肯定会变化，大概率老的历史帧的颜色已经不在屏幕上了，但是从最小化方差的角度来看，上述选择只能算次优解。下面的表格揭示了不同帧数下，不同 $\,\alpha\,$ 对应的有效累计样本数：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/08/YNvzT4HPkb29eK6.jpg" width = "45%" height = "45%" alt="图5 - 在 α = 0.1 的情况下，经过 5 帧相当于 1 个像素采样了 2 个样本；经过 10 帧，相当于 5 个样本；经过 15 帧，相当于 10 个样本；经过无限帧，相当于 19 个有效样本"/>
</div>

$\,\alpha\,$ 的值，通常的选择是 0.1。可以看到对于无限帧的情况，相当于 19x SSAA，效果还是相当不错的。在 Unity RenderGraph 创建持久化的 RT，即 TAA History，以及临时资源 TAA Target 并绘制的 C# 代码，这里就不展示了，就说一下大致流程，创建好了 TAA History 和 TAA Target 后，将 Color Attachment 作为当前帧的输入纹理，将 TAA History 作为历史帧的输入纹理，使用 Shader 或 Compute Shader 绘制出 TAA Target 后，作为 post processing 的输入纹理。与此同时，将 TAA Target 复制给 TAA History 以便下一帧使用。TAA Shader 目前的混合代码如下（我的 blend factor 是乘在 history 上的，故是 0.9）：  

    TEXTURE2D(_TAAHistory);
    float4 _TAAParams; // x: history blend factor

    float4 TAAFrag(Varyings IN) : SV_TARGET
    {
        float3 history = LOAD_TEXTURE2D_LOD(_TAAHistory, IN.positionHCS.xy, 0).xyz;
        float3 current = LOAD_TEXTURE2D_LOD(_BlitTexture, IN.positionHCS.xy, 0).xyz;
        float3 color = lerp(current, history, _TAAParams.x);
        return float4(color, 1.0);
    }

混合效果如下，为了方便观察高光闪烁问题，我又在机器人旁边加了盏红灯：  

<table><tr>
<td><img src='https://s2.loli.net/2025/07/08/JTbiFsuX9qG4Eeh.gif' width="512" alt="图6 - TAA (After Simple Exponential Blending)"></td>
<td><img src='https://s2.loli.net/2025/07/08/BonNJtUQaAZOPx9.gif' width="512" alt="图7 - NoAA"></td>
</tr></table>

可以很明显地感受到抗锯齿的效果，但也能明显地感受到闪烁问题。还有一点是，上图中可能会感觉到在 TAA 下会损失一些贴图细节，这是因为上图分辨率较小，只有 512 × 512，分辨率越高，这些现象越能得到缓解，对于现在普遍的 2k 与 4k 屏幕，这个问题不明显。

图 6 中的闪烁问题我们暂时先放着，后面再详细讨论，我们先来看看若将摄像机进行移动，图 6 会变为什么样子：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/09/AktR52mKGpsdqIQ.gif" width = "512" height = "512" alt="图8 - 鬼影 Ghost 现象"/>
</div>

上图只移动了摄像机，没移动物体，可以看到传说中的鬼影问题了。这下就集齐了 TAA 的两大问题：闪烁和鬼影。闪烁问题无论静态动态都存在，动态情况下会加剧闪烁问题，而鬼影只在动态场景存在，下面我们先来处理鬼影现象。

## 重投影
鬼影现象的产生原因很简单，相机或物体的移动导致了颜色信息的位置发生了变化，而我们还在采样原来的位置。所以我们很容易可以想到，通过计算屏幕像素每帧的坐标变化，即计算**运动向量 Motion Vector**，来找到像素移动前的屏幕坐标进行采样就可以解决鬼影问题。之前有提到过 Motion Vector 分为 **Camera Motion Vector** 和 **Object Motion Vector**，因为 Object Motion Vector 的实现在工程上（Unity 有一些历史遗留问题）具有一定的困难，我们先来解决只有摄像机移动的动态情形，即 Camera Motion Vector 的情形，至于物体的移动，等闪烁和鬼影解决得差不多了之后会专门讲解。

计算 Camera Motion Vector 的方法也不难，步骤如下：  
①通过 Camera Depth Texture 获取当前像素点的深度，通过深度还原出像素点的 clip space 坐标；  
②将 clip space 通过 view-projection 的逆矩阵，反向投影至世界坐标；  
③使用上一帧的 view-projection 矩阵，投影至上一帧的屏幕 uv 坐标，与当前帧的屏幕 uv 坐标相减得到 Camera Motion Vector。  
（注意上述方法只能计算 Camera Motion Vector，Object Motion Vector 还涉及到 MVP 矩阵的 M 的变化。）

Camera Motion Vector 可以直接在 TAA 的 Shader 里计算，也可以存储在一张 RT 里并在 TAA 里采样，我这里是开了一张 RT 存储的。如果后面没有使用 Camera Motion Vector 的后处理效果，比如动态模糊，并且 TAA 只打算使用 Camera Motion Vector，不使用 Object Motion Vector，那么就没必要多开张 RT。

这里顺便提一下 Unity 工程上的问题，以下的 Shader 参数，Unity 的 Built-in Engine 不会自动上传：  

    float4x4 unity_MatrixInvP;
    float4x4 unity_MatrixInvVP;
    float4x4 _PrevViewProjMatrix; // non-jittered.
    float4x4 _NonJitteredViewProjMatrix; // non-jittered.

因为我看 URP 将上述参数放在了 UnityInput.hlsl 里面，我还以为 Unity 的 Built-in Engine 会自动上传，但是使用这些参数，会发现它们都是单位矩阵。所以 UnityInput.hlsl 里的参数，哪些会被 Built-in Engine 自动上传，哪些不会，还得自己测试一下。最后上述这些参数，还得我们自己上传，首先找个地方保留住这一帧和上一帧的各种矩阵，然后根据它们计算相关参数，代码大致如下：  

``` C#
bool isProjectionMatrixFlipped = SystemInfo.graphicsUVStartsAtTop;

Matrix4x4 viewMatrix = yCamera.perCameraData.viewMatrix;
Matrix4x4 inverseViewMatrix = viewMatrix.inverse;
Matrix4x4 gpuProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.jitteredProjectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 inverseProjectionMatrix = gpuProjectionMatrix.inverse;
Matrix4x4 gpuNonJitterProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.projectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 nonJitterInverseProjectionMatrix = gpuNonJitterProjectionMatrix.inverse;

Matrix4x4 inverseViewProjectionMatrix = inverseViewMatrix * inverseProjectionMatrix;
Matrix4x4 nonJitterViewProjectionMatrix = gpuNonJitterProjectionMatrix * viewMatrix;
Matrix4x4 nonJitterInverseViewProjectionMatrix = inverseViewMatrix * nonJitterInverseProjectionMatrix;

Matrix4x4 previousViewMatrix = yCamera.perCameraData.previousViewMatrix;
Matrix4x4 previousInverseViewMatrix = previousViewMatrix.inverse;
Matrix4x4 previousGPUProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.previousJitteredProjectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 previousInverseProjectionMatrix = previousGPUProjectionMatrix.inverse;
Matrix4x4 previousGPUNonJitterProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.previousProjectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 previousNonJitterInverseProjectionMatrix = previousGPUNonJitterProjectionMatrix.inverse;

Matrix4x4 previousViewProjectionMatrix = previousGPUProjectionMatrix * previousViewMatrix;
Matrix4x4 previousInverseViewProjectionMatrix = previousInverseViewMatrix * previousInverseProjectionMatrix;
Matrix4x4 previousNonJitterViewProjectionMatrix = previousGPUNonJitterProjectionMatrix * previousViewMatrix;
Matrix4x4 previousNonJitterInverseViewProjectionMatrix = previousInverseViewMatrix * previousNonJitterInverseProjectionMatrix;

cmd.SetGlobalMatrix(YPipelineShaderIDs.k_InverseProjectionMatrixID, inverseProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_InverseViewProjectionMatrixID, inverseViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredViewProjectionMatrixID, nonJitterViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredInverseViewProjectionMatrixID, nonJitterInverseViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_PreviousViewProjectionMatrixID, previousViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_PreviousInverseViewProjectionMatrixID, previousInverseViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredPreviousViewProjectionMatrixID, previousNonJitterViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredPreviousInverseViewProjectionMatrixID, previousNonJitterInverseViewProjectionMatrix);
```

上面代码中要注意一下的是 `GL.GetGPUProjectionMatrix` 这个 API，从 `camera.projectionMatrix` 获取到投影矩阵是 OpenGL 习惯下的矩阵，我们需要根据不同平台转换成不同的习惯下的投影矩阵，所幸 `GL.GetGPUProjectionMatrix` 可以帮我们完成这件事情。另外一点要注意的是矩阵的乘法顺序，就不再赘述了。拿到矩阵后，就可以计算 Camera Motion Vector 了，Shader 代码如下：  

> 在 DirectX 平台下，只要将 `GL.GetGPUProjectionMatrix()` 设置为 true，Unity 会将 Projection Matrix 的 y 轴翻转，这样子经过视口变换（uv 的 v 再次翻转），就统一了 OpenGL 下和 DirectX 下的 uv 了（即原点在左下角），这也是普通的 Unity Shader 中我们不用关心 uv 原点的位置的原因。但是直接绘制 RT 的 Shader 就不同了，因为此时的 uv 和 positionHCS 是我们生成的，而不是通过 Projection Matrix 计算而得，所以之前在顶点着色器中，将 uv 的 v 轴手动翻转了，positionHCS 无需手动翻转是因为视口变换会翻转。所以下面计算 Camera Motion Vector 要特别注意 uv 的方向。

    float4 GetNDCFromUVAndDepth(float2 uv, float depth)
    {
        #if UNITY_UV_STARTS_AT_TOP
            uv.y = 1.0f - uv.y;
        #else
            depth = 2.0 * depth - 1.0;
        #endif
        
        return float4(2.0 * uv - 1.0, depth, 1.0);
    }

    float3 TransformNDCToWorld(float4 NDC, float4x4 invViewProjMatrix)
    {
        float4 positionHWS = mul(invViewProjMatrix, NDC);
        return positionHWS.xyz / positionHWS.w;
    }

    float4 CameraMotionVectorFrag(Varyings IN) : SV_TARGET
    {
        float depth = LOAD_TEXTURE2D_LOD(_CameraDepthTexture, IN.positionHCS.xy, 0).r;
        float4 NDC = GetNDCFromUVAndDepth(IN.uv, depth);
        float3 currentPositionWS = TransformNDCToWorld(NDC, UNITY_MATRIX_I_VP);

        float4 currentPositionCS = mul(UNITY_MATRIX_NONJITTERED_VP, float4(currentPositionWS.xyz, 1.0));
        float4 previousPositionCS = mul(UNITY_PREV_MATRIX_NONJITTERED_VP, float4(currentPositionWS.xyz, 1.0));
        
        float2 currentPositionNDC = currentPositionCS.xy * rcp(currentPositionCS.w);
        float2 previousPositionNDC = previousPositionCS.xy * rcp(previousPositionCS.w);
        
        float2 velocity = currentPositionNDC - previousPositionNDC;
        
        #if UNITY_UV_STARTS_AT_TOP
        velocity.y = -velocity.y;
        #endif
        
        velocity *= 0.5;

        return float4(velocity, 0,0);
    }

另外要注意的是，计算 Camera Motion Vector 的时候要去除掉 jitter 的影响，否则得到的 Motion Vector 是不对的，使用了会导致画面糊，所以上面计算时，使用了 `NONJITTERED_VP` 矩阵。然后在 TAA Shader 采样时，减去 Motion Vector：  

    float4 TAAFrag(Varyings IN) : SV_TARGET
    {
        float2 velocity = LOAD_TEXTURE2D_LOD(_CameraMotionVectorTexture, IN.positionHCS.xy, 0).rg;
        
        float3 history = SAMPLE_TEXTURE2D_LOD(_TAAHistory, sampler_LinearClamp, IN.uv - velocity, 0).xyz;
        float3 current = LOAD_TEXTURE2D_LOD(_BlitTexture, IN.positionHCS.xy, 0).xyz;

        float3 color = lerp(current, history, _TAAParams.x);
        return float4(color, 1.0);
    }

注意，采样 history 时，因为要减去 velocity，得到的新 uv 值不会正好在像素中心，此时若使用 load 或者 point 采样不合理，效果也不好，会出现涂抹 smear 感，建议使用 linear 采样。但是使用 linear 采样，又会造成模糊感，这也是一个较为重要的优化地方，在其他优化技术的 History Filter 小节中会详细介绍，目前为止的效果如下：  

> 在后处理中，因为我们是绘制全屏三角形，设置了 3 个顶点 uv 值为 (0, 0)、(2, 0)、(0, 2)，当 uv 通过光栅化插值时，uv 会天然满足精确对应到纹素中心，此时和 SV_POSITION 语义的 xy 分量是对齐的，都代表纹素中心，只不过 SV_POSITION 是像素坐标。如果采样在纹素中心，此时 linear 和 point 采样得到的结果会是一样的。

<div align="center">  
<img src="https://s2.loli.net/2025/07/10/Xgvw4oALqMxWShn.gif" width = "512" height = "512" alt="图9 - After Camera Motion Vector"/>
</div>

可以看到，虽然可以看清物体了，但是还是有鬼影现象，这是由于场景中的遮挡关系发生了变化。比如上图中，在这一帧中，机器人的位置可以拿到 Motion Vector 得到上一帧中机器人的颜色进行混合，但是机器人右上位置的像素，在上一帧中被机器人遮挡，在这一帧中没被机器人遮挡，同时 Motion Vector 也为 0，那么这些像素就会混合到上一帧中机器人的颜色，从而导致鬼影。所以 Camera Motion Vector 只能消除一部分的鬼影。当然鬼影现象还会因为其他因素引起，比如灯光的变化等等，这就需要我们去验证历史数据，拒绝不能使用的历史数据。

# 验证历史样本
那么如何验证历史样本呢？一般来说，有两类验证历史样本可信度的信息，即**几何信息 geometry data** 和**颜色信息 color data**。几何信息包括物体深度、速度以及 object ID 等等。使用几何信息拒绝历史样本相对于颜色信息来说没有那么 Robust，所以这里主要详细介绍 Color Rejection，Color Rejection 可以说是 TAA 离开不了的一环。Geometry Rejection 的相关方法基本上只大致介绍一下思路，但 Velocity Rejection 我会详细说明。

## Color Rejection
### Color Clamping
Color Clamping 假设采样点周围样本对于 TAA 累计过程是有效的，历史样本如果跟当前帧样本出现较大偏差，那么历史样本就应该被拒绝。但相较于直接拒绝历史样本，Color Clamping 选择将历史样本钳制到当前帧样本周围 5 个样本或 9 个样本组成的 AABB 包围盒中：  

    float4 TAAFrag(Varyings IN) : SV_TARGET
    {
        float2 velocity = LOAD_TEXTURE2D_LOD(_CameraMotionVectorTexture, IN.positionHCS.xy, 0).rg;
        float3 history = SAMPLE_TEXTURE2D_LOD(_TAAHistory, sampler_LinearClamp, IN.uv - velocity, 0).xyz;

        float3 current = LOAD_TEXTURE2D_LOD(_BlitTexture, IN.positionHCS.xy, 0).xyz;

        float3 N = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(0, 1)).xyz;
        float3 E = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(1, 0)).xyz;
        float3 S = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(0, -1)).xyz;
        float3 W = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(-1, 0)).xyz;
        #if _TAA_SAMPLE_3X3
        float3 NW = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(-1, 1)).xyz;
        float3 NE = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(1, 1)).xyz;
        float3 SW = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(-1, -1)).xyz;
        float3 SE = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(1, -1)).xyz;
        #endif

		float3 min = min(current, min(N, min(E, min(S, W))));
		float3 max = max(current, max(N, max(E, max(S, W))));
        #if _TAA_SAMPLE_3X3
        min = min(min, min(NW, min(NE, min(SW, SE))));
        max = max(max, max(NW, max(NE, max(SW, SE))));
        #endif
        
        history = clamp(history, min, max);
        float3 color = lerp(current, history, _TAAParams.x);
        return float4(color, 1.0);
    }

由于临近采样点的亮度变化可能会很大，这会导致 AABB 包围盒很大，从而重现鬼影现象。Epic Games 的 Karis 在 SIGGRAPH 2014 的演讲 [High Quality Temporal Supersampling](https://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf) 有提到使用 **YCoCg** 色彩空间可以使 AABB 包围盒更加紧致，因为它将颜色的色度 Chroma（即 Co, Cg 通道）从亮度（Y 通道）分离了出来，而亮度往往占据颜色差值的主导地位。

> YCoCg 色彩空间包含 Y、Co、Cg 三个通道，分别对应亮度、绿色色度（chrominance green）和橙色色度（chrominance orange）。  

YCoCg 到 RGB 的转换是线性变换，代码如下：  

    float3 RGB2YCoCg(float3 rgb) {
        return float3(
                rgb.x/4.0 + rgb.y/2.0 + rgb.z/4.0,
                rgb.x/2.0 - rgb.z/2.0,
                -rgb.x/4.0 + rgb.y/2.0 - rgb.z/4.0);
    }

    float3 YCoCg2RGB(float3 YCoCg) {
        return float3(
                YCoCg.x + YCoCg.y - YCoCg.z,
                YCoCg.x + YCoCg.z,
                YCoCg.x - YCoCg.y - YCoCg.z);
    }

最后 YCoCg 色彩空间下的 3X3 的 9 个样本的 Color Clamping 的结果如下：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/11/3IAFVrQbg6X4ZtE.gif" width = "512" height = "512" alt="图10 - After Color Clamping"/>
</div>

其实这样子 TAA 已经勉强成形了，后面的技术的目的都是进一步优化 TAA 以减少鬼影和闪烁问题。

### Color Clipping
Color Clipping 可以说是 Color Clamping 的进阶版本，用一张图可以概括它们的区别：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/11/Nq6mJQIWw2z3clg.jpg" width = "30%" height = "30%" alt="图11 - AABB clipping and clamping"/>
</div>

我在网上找到有两种 Color Clipping 的实现方法，一种是 UE4 中的实现，一种是 [Playdead](https://github.com/playdeadgames/temporal/blob/4795aa0007d464371abe60b7b28a1cf893a4e349/Assets/Shaders/TemporalReprojection.shader) 的实现（它在 GDC 2016 的演讲：[Temporal Reprojection Antialiasing in INSIDE](https://www.gdcvault.com/play/1022970/Temporal-Reprojection-Anti-Aliasing-in)）。UE4 的实现我没在网上找到具体的来源，虽然是开源的，但是登录 Github 还要 Epic 账号才能查看，比较麻烦，而且网上也有很多人已经将这些代码摘抄下来了。Unity 的 [HDRP](https://github.com/Unity-Technologies/Graphics/blob/master/Packages/com.unity.render-pipelines.high-definition/Runtime/PostProcessing/Shaders/TemporalAntialiasing.hlsl) 里面这两种实现也都有，可以直接查看。

#### Playdead 的 Clip to AABB Center
Playdead 分享的库中的源代码如下：  

    float4 clip_aabb(float3 aabb_min, float3 aabb_max, float4 p, float4 q)
    {
        // note: only clips towards aabb center (but fast!)
        float3 p_clip = 0.5 * (aabb_max + aabb_min);
        float3 e_clip = 0.5 * (aabb_max - aabb_min) + FLT_EPS;

        float4 v_clip = q - float4(p_clip, p.w);
        float3 v_unit = v_clip.xyz / e_clip;
        float3 a_unit = abs(v_unit);
        float ma_unit = max(a_unit.x, max(a_unit.y, a_unit.z));

        if (ma_unit > 1.0)
            return float4(p_clip, p.w) + v_clip / ma_unit;
        else
            return q;// point inside aabb
    }

其中 q 是 history，这个 p 不用管它，影响不到颜色。Playdead 的命名很难看懂，p_clip 就是 AABB center，e_clip 就是 AABB extent，FLT_EPS 是最小浮点数，防止除 0 的。v_clip 就是 history 到 AABB center 的距离（或向量），v_unit 就是 history 到 AABB center 的距离除以 AABB 边缘到 AABB center 的距离，它是一个距离倍数。只要这个距离倍数有一个分量大于 1 了，则认为需要 Clip，重新计算 history，计算方式为 AABB center 加上使用最大距离倍数缩放后的 v_clip。可以看到该方法是从 AABB center 出发的 Clip，因此我觉得从逻辑上来说，效果应该是不如 UE4 的实现的（虽然我也看不出来哪个效果更好，笑 ^_^），效果如下：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/11/1FzUWbOJwC7LRrd.gif" width = "512" height = "512" alt="图12 - Clip to AABB Center"/>
</div>

反正我是没看出来和 Color Clamping 的区别，不知道其他运动剧烈或者边缘亮度差距较大的物体的效果区别明不明显。

#### UE4 的 Clip to Filtered Color
我在网上找到不同人摘抄的这段 UE4 的代码，计算方式可能略有不同，但逻辑都是一样的，跟 Unity HDRP 里的也差不多：  

    float3 ClipToFiltered(float3 NeighborMin, float3 NeighborMax, float3 Filtered, float3 History)
    {
        float3 Center  = 0.5 * (NeighborMax + NeighborMin);
        float3 Extents = 0.5 * (NeighborMax - NeighborMin);

        float3 RayDir = Filtered - History;
        float3 RayPos = History - Center;
        RayDir = abs(RayDir) < (1.0/65536.0) ? (1.0/65536.0) : RayDir;
        float3 InvDir = rcp(RayDir);
        float3 MaxIntersect = (Extents - RayPos)  * InvDir;
        float3 MinIntersect = -(Extents + RayPos) * InvDir;
        float3 EnterIntersect = min(MinIntersect, MaxIntersect);
        float ClipBlend = max3(EnterIntersect.x, EnterIntersect.y, EnterIntersect.z);
        ClipBlend = saturate(ClipBlend);
        return lerp(history, filtered, ClipBlend);
    }

TODO：写一下上面代码逻辑

因为要使用预过滤的当前帧中间颜色值，我先用没过滤的中间颜色值替代了，预过滤在其他优化技术中详细说明，现在效果如下：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/11/YgpwKOy23VFnm5i.gif" width = "512" height = "512" alt="图13 - Clip to Filtered Color"/>
</div>

还是没看出来区别，但是从理论上来说，应该是 Clip to Filtered Color 是最好的。

### Variance Clipping

Convex Hull 凸包

## Geometry Rejection
①**Depth Rejection**：这个方法假设每帧之间的深度不会发生显著变化，深度变化较大的不属于同一像素点。要实现该方法，需要存储上一帧的深度纹理。这个方法可能比较适用于特殊的游戏，例如《这是我的战争》（This War of Mine）这样的 3d 横板游戏，因为镜头以平移为主，深度不会发生较大变化。但可想而知，这个方法对于一般的 3d 游戏相对来说没有这么好用。  
②**Stencil Rejection**：顽皮狗在 SIGGRAPH 2016 的演讲 [Temporal Antialiasing in Uncharted 4](https://advances.realtimerendering.com/s2016/) 有介绍这一方法的使用，大致意思就是对于一些主要渲染物体，比如人物，使用 Stencil Buffer 存储模板值，也要保留上一帧的 Stencil Buffer，进行比较，不同模板值的不是同一物体，此时只采样当前帧颜色，不混合历史帧。这个方法我觉得比较适用于屏幕长期占用着主要渲染物体的游戏，比如第三人称游戏或者赛车游戏等等。  
③**Velocity Rejection**：

### Velocity Rejection

# 其他优化技术
## 闪烁优化

### Prefilter

### Luma Weighted Exponential Blending
这里顺便讨论一下 TAA 在 Pipeline 的位置问题，上面将 TAA 放置在了所有后处理之前，即是在 HDR 线性空间下进行的。在这样的情况下，物体或相机移动，会导致物体的高光计算的剧烈变化，从而导致高光的闪烁问题。可能有人会说为什么不把 TAA 放在 Tone Mapping 之后，这样子就能解决闪烁问题。因为 bloom 或者 lens flare 效果可能会增大因高亮度颜色产生的 alias 问题，TAA 放在 Tone Mapping 之后的话 bloom 闪烁问题也会比较严重。还有一点是在 Tone Mapping 之前，相对来说更物理正确，因为 Tone Mapping 之前是线性空间，之后是非线性空间。但是的确 TAA 在 LDR 下的效果会比 HDR 下要好，这就产生了一个妥协的办法，即类似于 Bloom 中的解决方案，使用 Karis Average 来混合当前帧与历史帧从而缓解这个运动状态下的 bloom 闪烁现象，如下：

$$ w(c) = \cfrac {1} {1 + Luminance(c)} $$

代码如下：  

    float3 LumaExponentialAccumulation(float3 history, float3 current, float blendFactor)
    {
        float historyLuma = Luminance(history);
        float currentLuma = Luminance(current);
        float historyLumaWeight = rcp(historyLuma + 1.0);
        float currentLumaWeight = rcp(currentLuma + 1.0);
        float weightSum = lerp(currentLumaWeight, historyLumaWeight, blendFactor);
        float3 blendColor = lerp(current * currentLumaWeight, history * historyLumaWeight, blendFactor);
        return blendColor / weightSum;
    }

上述方法在静态场景中几乎看不出差别，就是能感觉到高光变暗了，并不能解决图 6 中的闪烁问题。但是在运动场景中，确实能减少一部分的高光闪烁问题，至于需不需要使用看项目需求吧。

### Dynamic/Adaptive Blending

### Closest Depth Velocity

### History Filter

# Object Motion Vector

# Upsampling / Super-Resolution