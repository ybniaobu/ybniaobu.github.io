---
title: Temporal Anti-Aliasing (TAA)
date: 2025-06-21 19:43:03
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2025/06/23/kEPO3zg8IRXwUFC.gif
mathjax: true
description: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
---

> 本来是想先实现 SSGI 或者 SSR 的，但是研究着研究着，就发现 SSGI 或 SSR 的思想都有些许依赖于 SSAO 相关技术算法（SSAO、SSDO、HBAO、GTAO 等等），然后就想着要不先实现 SSAO，但是研究着研究着，又发现 SSAO 等相关技术跟 PCSS 一样较为依赖于降噪技术，降噪技术又可以分为**空间滤波 Spatial Filter** 以及**时间滤波 Temporal Filter**。所以为了打好基础，最终决定先实现 **Temporal Anti-Aliasing (TAA)**，对**抖动 jitter** 有个更深入的理解后，再进入全局光照的实现，顺便也能优化一下 PCSS。
>   
> 本篇文章具体实现是在 Unity 当中，主要参考了以下几篇文章或演讲或 PPT（加粗的文章更综合地介绍了 TAA 技术，建议优先观看）：  
> ① High Quality Temporal Supersampling（SIGGRAPH 2014）：https://advances.realtimerendering.com/s2014/#_HIGH-QUALITY_TEMPORAL_SUPERSAMPLING or https://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf ；  
> ② **A Survey of Temporal Antialiasing Techniques** ：https://research.nvidia.com/labs/rtr/publication/yang2020survey/ ；  
> ③ **Temporal AA and the Quest for the Holy Trail** ：https://www.elopezr.com/temporal-aa-and-the-quest-for-the-holy-trail/ ；  
> ④ Temporal Reprojection Anti-Aliasing in INSIDE（GDC 2016）：https://www.gdcvault.com/play/1022970/Temporal-Reprojection-Anti-Aliasing-in ；  
> ⑤ An Excursion in Temporal Supersampling（GDC 2016）：https://developer.download.nvidia.com/gameworks/events/GDC2016/msalvi_temporal_supersampling.pdf ；  
> ⑥ Temporal Antialiasing in Uncharted 4（SIGGRAPH 2016）：https://advances.realtimerendering.com/s2016/ ；  
> ⑦ Temporal Antialiasing Starter Pack ：https://alextardif.com/TAA.html ；  
> ⑧ Temporal Anti Aliasing – Step by Step ：https://ziyadbarakat.wordpress.com/2020/07/28/temporal-anti-aliasing-step-by-step/ ；  
> ⑨ Adaptive Temporal Antialiasing ：https://research.nvidia.com/publication/2018-08_adaptive-temporal-antialiasing ；  
> ⑩ Dynamic Temporal Antialiasing and Upsampling in Call of Duty（SIGGRAPH 2017）：https://www.activision.com/cdn/research/Dynamic_Temporal_Antialiasing_and_Upsampling_in_Call_of_Duty_v4.pdf 。  


# TAA 原理及流程简单介绍
**时域抗锯齿 Temporal Anti-Aliasing (TAA)** 用一句话概括可以说是使用了历史帧中的**子/次/亚像素 subpixel** 进行累计从而更有效地实现**超采样抗锯齿 Supersampling Anti-Aliasing (SSAA)**。相对于 SSAA 或 MSAA 来说，TAA 有效地将一个像素多次采样分摊到了连续的几帧当中，即以一个像素一次采样的代价实现了超采样抗锯齿。与此同时，单帧光栅化单像素采样，实际上会“损失”一些亚像素信息的，而 TAA 通过抖动在历史帧中解析出单帧光栅化以及单像素采样中丢失的亚像素信息，从而能够更好地解决**几何锯齿 Geometric Aliasing**（几何光栅化锯齿） 和**着色锯齿 Shading Aliasing**（渲染锯齿），也相对来说增加了基于时序的稳定性。特别是当物体或相机移动时，图元边缘相对于采样点的位置不断变化，导致边缘像素的覆盖率计算结果不断变化，使得边缘像素的颜色在帧与帧之间发生微妙的、高频的亮度变化，而历史帧的混合可以减少这种不稳定性。虽然由于每帧的抖动，会让我们误以为更加不稳定，但实际上可以通过些许 trick 来减少感知上的闪烁问题，从而达到其他后处理抗锯齿技术（诸如形态抗锯齿）无法达到的基于时序的稳定性。这也引出了 TAA 的两大难题：**鬼影 ghost** 问题和**闪烁 flicker** 问题，我们在后面重点解决这两个问题。

TAA 大致流程如下所示：  
①首先对摄像机进行**抖动 jitter**，这样子每一帧采样的是一个基于中心位置的被抖动过后的随机位置采样点，将当前帧采样颜色和历史颜色进行混合得到 TAA Target 贴图，后处理后输出。与此同时，将 TAA Target 复制为 TAA History/Accumulation 贴图，将累计的历史颜色信息用于下一帧进行混合；  
②由于相机或物体会进行移动，所以采样历史帧（TAA History）时，不能使用当前的 uv 值，否则采样到的颜色会不是当前像素点或物体的历史颜色信息。这就需要使用上一帧的矩阵进行**重投影 Reprojection** 计算出**运动向量 Motion Vector**，从而还原出当前像素在上一帧的屏幕位置。Motion Vector 又分为 **Camera Motion Vector** 和 **Object Motion Vector**，后面会详细说明；  
③由于运动时遮挡的变化，或者超出屏幕范围，又或者是灯光的变化，会让我们采样到跟当前帧不匹配的历史颜色信息，从而产生鬼影现象。因此我们需要**验证 Validate** 或**矫正 rectify** 历史信息，以防止鬼影现象的产生；  
④最后是解决闪烁问题。鬼影和闪烁问题是 TAA 的两大顽疾，而且这两顽疾几乎很难同时解决。防止鬼影现象产生，往往会加剧闪烁现象，解决了闪烁问题，鬼影可能又会回来。

<div align="center">  
<img src="https://s2.loli.net/2025/07/06/HCp3vjaQ4tswEUq.png" width = "60%" height = "60%" alt="图1 - TAA 基本流程"/>
</div>

下面就开始介绍具体的实现方式，我们先处理静态场景的情形，即摄像机和物体都不运动：

# 累计历史样本
## 抖动 Jitter
由于我们要对一个像素内的多个**子像素 subpixel** 进行采样，故我们需要对采样点的位置进行偏移，即**抖动 Jitter**，通常情况下会使用低差异序列中的 Halton 序列，从而实现更好的抗锯齿效果。UE4 默认使用了 Halton 序列的前 8 个样本，Playdead Studios 工作室（《INSIDE》、《地狱边境》的制作厂商）在 GDC 2016 的分享中有提到使用前 16 个样本可以产生更好的效果，我也采用了这个方式。Halton 序列等低差异序列的生成就不在这里赘述了，详见《Physically Based Rendering: From Theory To Implementation》中的第八章 Sampling and Reconstruction 的第六节 [Halton Sampler](https://www.pbr-book.org/4ed/Sampling_and_Reconstruction/Halton_Sampler) 。

<div align="center">  
<img src="https://s2.loli.net/2025/07/06/Fr3pSCQuLM2fJWl.jpg" width = "50%" height = "50%" alt="图2 - Halton Squence"/>
</div>

对采样点进行偏移的方式通常是修改相机的投影矩阵，只需修改矩阵中的两个变量即可：  

    ProjectionMatrix[0][2] += ( OffsetX * 2.0f – 1.0f ) / FrameBufferSize.Width;
    ProjectionMatrix[1][2] += ( OffsetY * 2.0f – 1.0f ) / FrameBufferSize.Height;

至于为什么要对 offset 乘 2 减 1 的原因是，Halton 序列即 offset 的范围是在 (0, 1)，我们希望采样点偏移的范围是在一个像素内，即在 (-0.5, 0.5) 之间，需要对 Halton 序列减去 0.5。又因为齐次除法后得到的 NDC 坐标的 x、y 分量都在 \[-1, 1\] 之间，而得到 uv 值在 (0, 1) 之间，故需要乘以 2 消除缩放影响。具体推导如下，假设 jitter 在 (-0.5, 0.5) 之间：  

$$ P'_{clip} = M_{persp}P_{view} = \begin{bmatrix} A & 0 & 2 \times jitter.x / width & 0 \\ 0 & B & 2 \times jitter.y / height & 0 \\ 0 & 0 & C & D \\ 0 & 0 & 1\,or\, -1 & 0 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix} = \begin{bmatrix} Ax + 2 \times jitter.x / width \times z \\ By + 2 \times jitter.y / height \times z \\ ... \\ ... \end{bmatrix} $$

$$ P'_{NDC} = \left[ \cfrac {A}{z}x + \cfrac {2 \times jitter.x}{width} , \cfrac {B}{z}y + \cfrac {2 \times jitter.y}{height} , ..., ... \right] $$

$$ P'_{ScreenUV} = \left[ \cfrac {A}{2z}x + \cfrac {jitter.x}{width} + 0.5 , \cfrac {B}{2z}y + \cfrac {jitter.y}{height} + 0.5 \right] $$

要注意，原 ScreenUV 为 $\, \left[ \cfrac {A}{2z}x + 0.5, \cfrac {B}{2z}y + 0.5 \right] \,$，故偏移了 $\, \left[ \cfrac {jitter.x}{width}, \cfrac {jitter.y}{height} \right] \,$，符合我们的要求。注意上面推导的是透视投影的情况，正交投影则需改变第一行第四位，以及第二行第四位，即 \[0\]\[3\] 和 \[1\]\[3\] ：

$$ P'_{clip} = M_{ortho}P_{view} = \begin{bmatrix} A & 0 & 0 & 2 \times jitter.x / width \\ 0 & B & 0 & 2 \times jitter.y / height \\ 0 & 0 & C & D \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix} = \begin{bmatrix} Ax + 2 \times jitter.x / width \\ By + 2 \times jitter.y / height \\ ... \\ ... \end{bmatrix} $$

$$ P'_{ScreenUV} = \left[ \cfrac {A}{2}x + \cfrac {jitter.x}{width} + 0.5 , \cfrac {B}{2}y + \cfrac {jitter.y}{height} + 0.5 \right] $$

为了方便控制偏移距离，可以给 jitter 乘以一个 jitterScale 参数，用于控制偏移的范围。得到修改的矩阵后，只需调用 `CommandBuffer.SetViewProjectionMatrices()` 即可实现抖动了，我用了一张金属度比较高的图片，方便观察闪烁现象：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/08/Hl3VvTNDFPCQh7n.gif" width = "512" height = "512" alt="图3 - Jitter"/>
</div>

## Exponential Blending
接下来就是将当前帧与历史帧进行混合了，直接混合所有历史帧肯定是不现实的，因为我们没法存储所有历史数据。绝大多数 TAA 的实现采用了类似递归的方式，将所有历史帧的累加结果存储到一张贴图当作，即 TAA History/Accumulation Texture，并采用了以下公式进行混合：  

$$ f_n(p) = \alpha \cdot s_n(p) + (1 - \alpha) \cdot f_{n-1} (\pi(p)) $$

其中 $\,f_n(p)\,$ 是第 n 帧的输出颜色，$\,\alpha\,$ 是混合系数，$\,s_n(p)\,$ 是当前帧颜色，$\,f_{n-1} (\pi(p))\,$ 是经过重投影后的累计历史帧。重投影后面再考虑，这里先考虑静态场景。在这个公式下，历史帧会被不断累计，当然随着时间的流逝，单一历史帧的影响会被无限缩小。

<div align="center">  
<img src="https://s2.loli.net/2025/07/08/ANXvRKOwUabtjDl.jpg" width = "45%" height = "45%" alt="图4 - 单一历史帧随着帧数增加所占的比例变化"/>
</div>

越老的历史帧所占的比例越来越小，在大部分情况下是很好的选择，因为场景肯定会变化，大概率老的历史帧的颜色已经不在屏幕上了，但是从最小化方差的角度来看，上述选择只能算次优解。下面的表格揭示了不同帧数下，不同 $\,\alpha\,$ 对应的有效累计样本数：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/08/YNvzT4HPkb29eK6.jpg" width = "45%" height = "45%" alt="图5 - 在 α = 0.1 的情况下，经过 5 帧相当于 1 个像素采样了 2 个样本；经过 10 帧，相当于 5 个样本；经过 15 帧，相当于 10 个样本；经过无限帧，相当于 19 个有效样本"/>
</div>

$\,\alpha\,$ 的值，通常的选择是 0.1。可以看到对于无限帧的情况，相当于 19x SSAA，效果还是相当不错的。在 Unity RenderGraph 创建持久化的 RT，即 TAA History，以及临时资源 TAA Target 并绘制的 C# 代码，这里就不展示了，就说一下大致流程，创建好了 TAA History 和 TAA Target 后，将 Color Attachment 作为当前帧的输入纹理，将 TAA History 作为历史帧的输入纹理，使用 Shader 或 Compute Shader 绘制出 TAA Target 后，作为 post processing 的输入纹理。与此同时，将 TAA Target 复制给 TAA History 以便下一帧使用。TAA Shader 目前的混合代码如下（我的 blend factor 是乘在 history 上的，故是 0.9）：  

    TEXTURE2D(_TAAHistory);
    float4 _TAAParams; // x: history blend factor

    float4 TAAFrag(Varyings IN) : SV_TARGET
    {
        float3 history = LOAD_TEXTURE2D_LOD(_TAAHistory, IN.positionHCS.xy, 0).xyz;
        float3 current = LOAD_TEXTURE2D_LOD(_BlitTexture, IN.positionHCS.xy, 0).xyz;
        float3 color = lerp(current, history, _TAAParams.x);
        return float4(color, 1.0);
    }

混合效果如下，为了方便观察高光闪烁问题，我又在机器人旁边加了盏红灯：  

<table><tr>
<td><img src='https://s2.loli.net/2025/07/08/JTbiFsuX9qG4Eeh.gif' width="512" alt="图6 - TAA (After Simple Exponential Blending)"></td>
<td><img src='https://s2.loli.net/2025/07/08/BonNJtUQaAZOPx9.gif' width="512" alt="图7 - NoAA"></td>
</tr></table>

可以很明显地感受到抗锯齿的效果，但也能明显地感受到闪烁问题。还有一点是，上图中可能会感觉到在 TAA 下会损失一些贴图细节，这是因为上图分辨率较小，只有 512 × 512，分辨率越高，这些现象越能得到缓解，对于现在普遍的 2k 与 4k 屏幕，这个问题不明显。

图 6 中的闪烁问题我们暂时先放着，后面再详细讨论，我们先来看看若将摄像机进行移动，图 6 会变为什么样子：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/09/AktR52mKGpsdqIQ.gif" width = "512" height = "512" alt="图8 - 鬼影 Ghost 现象"/>
</div>

上图只移动了摄像机，没移动物体，可以看到传说中的鬼影问题了。这下就集齐了 TAA 的两大问题：闪烁和鬼影。闪烁问题无论静态动态都存在，动态情况下会加剧闪烁问题，而鬼影只在动态场景存在，下面我们先来处理鬼影现象。

## 重投影 Reprojection
鬼影现象的产生原因很简单，相机或物体的移动导致了颜色信息的位置发生了变化，而我们还在采样原来的位置。所以我们很容易可以想到，通过计算屏幕像素每帧的坐标变化，即计算**运动向量 Motion Vector**，来找到像素移动前的屏幕坐标进行采样就可以解决鬼影问题。之前有提到过 Motion Vector 分为 **Camera Motion Vector** 和 **Object Motion Vector**，因为 Object Motion Vector 的实现在工程上相对麻烦一点（Unity 有一些历史遗留问题），我们先来解决只有摄像机移动的动态情形，即 Camera Motion Vector 的情形，至于物体的移动，等闪烁和鬼影解决得差不多了之后会专门讲解。

计算 Camera Motion Vector 的方法也不难，步骤如下：  
①通过 Camera Depth Texture 获取当前像素点的深度，通过深度还原出像素点的 clip space 坐标；  
②将 clip space 通过 view-projection 的逆矩阵，反向投影至世界坐标。因为只有相机在运动，物体的世界坐标是不会变的；  
③使用上一帧的 view-projection 矩阵，投影至上一帧的屏幕 uv 坐标，与当前帧的屏幕 uv 坐标相减得到 Camera Motion Vector。  
（注意上述方法只能计算 Camera Motion Vector，Object Motion Vector 还涉及到 MVP 矩阵的 M 的变化或者顶点在模型空间的变化。）

Camera Motion Vector 可以直接在 TAA 的 Shader 里计算，也可以存储在一张 RT（只需要 RG 通道就可以，因为精度很重要，常规的做法是两通道都 16 bit）里并在 TAA 里采样，我这里是开了一张 RT 存储的。如果后面没有使用 Camera Motion Vector 的后处理效果，比如动态模糊，并且 TAA 只打算使用 Camera Motion Vector，不使用 Object Motion Vector，那么就没必要多开张 RT。

这里顺便提一下 Unity 工程上的问题，以下计算 Camera Motion Vector 需要使用的矩阵 shader 参数，Unity 的 Built-in Engine 不会自动上传：  

    float4x4 unity_MatrixInvP;
    float4x4 unity_MatrixInvVP;
    float4x4 _PrevViewProjMatrix; // non-jittered.
    float4x4 _NonJitteredViewProjMatrix; // non-jittered.

因为我看 URP 将上述参数放在了 UnityInput.hlsl 里面，我还以为 Unity 的 Built-in Engine 会自动上传，但是使用这些参数，会发现它们都是单位矩阵。所以 UnityInput.hlsl 里的参数，哪些会被 Built-in Engine 自动上传，哪些不会，还得自己测试一下。最后上述这些参数，还得我们自己上传，首先找个地方保留住这一帧和上一帧的各种矩阵，然后根据它们计算相关参数，代码大致如下：  

``` C#
bool isProjectionMatrixFlipped = SystemInfo.graphicsUVStartsAtTop;

Matrix4x4 viewMatrix = yCamera.perCameraData.viewMatrix;
Matrix4x4 inverseViewMatrix = viewMatrix.inverse;
Matrix4x4 gpuProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.jitteredProjectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 inverseProjectionMatrix = gpuProjectionMatrix.inverse;
Matrix4x4 gpuNonJitterProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.projectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 nonJitterInverseProjectionMatrix = gpuNonJitterProjectionMatrix.inverse;

Matrix4x4 inverseViewProjectionMatrix = inverseViewMatrix * inverseProjectionMatrix;
Matrix4x4 nonJitterViewProjectionMatrix = gpuNonJitterProjectionMatrix * viewMatrix;
Matrix4x4 nonJitterInverseViewProjectionMatrix = inverseViewMatrix * nonJitterInverseProjectionMatrix;

Matrix4x4 previousViewMatrix = yCamera.perCameraData.previousViewMatrix;
Matrix4x4 previousInverseViewMatrix = previousViewMatrix.inverse;
Matrix4x4 previousGPUProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.previousJitteredProjectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 previousInverseProjectionMatrix = previousGPUProjectionMatrix.inverse;
Matrix4x4 previousGPUNonJitterProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.previousProjectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 previousNonJitterInverseProjectionMatrix = previousGPUNonJitterProjectionMatrix.inverse;

Matrix4x4 previousViewProjectionMatrix = previousGPUProjectionMatrix * previousViewMatrix;
Matrix4x4 previousInverseViewProjectionMatrix = previousInverseViewMatrix * previousInverseProjectionMatrix;
Matrix4x4 previousNonJitterViewProjectionMatrix = previousGPUNonJitterProjectionMatrix * previousViewMatrix;
Matrix4x4 previousNonJitterInverseViewProjectionMatrix = previousInverseViewMatrix * previousNonJitterInverseProjectionMatrix;

cmd.SetGlobalMatrix(YPipelineShaderIDs.k_InverseProjectionMatrixID, inverseProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_InverseViewProjectionMatrixID, inverseViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredViewProjectionMatrixID, nonJitterViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredInverseViewProjectionMatrixID, nonJitterInverseViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_PreviousViewProjectionMatrixID, previousViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_PreviousInverseViewProjectionMatrixID, previousInverseViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredPreviousViewProjectionMatrixID, previousNonJitterViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredPreviousInverseViewProjectionMatrixID, previousNonJitterInverseViewProjectionMatrix);
```

上面代码中要注意一下的是 `GL.GetGPUProjectionMatrix` 这个 API，从 `camera.projectionMatrix` 获取到投影矩阵是 OpenGL 习惯下的矩阵，我们需要根据不同平台转换成不同的习惯下的投影矩阵，所幸 `GL.GetGPUProjectionMatrix` 可以帮我们完成这件事情。另外一点要注意的是矩阵的乘法顺序，就不再赘述了。拿到矩阵后，就可以计算 Camera Motion Vector 了，Shader 代码如下：  

> 在 DirectX 平台下，只要将 `GL.GetGPUProjectionMatrix()` 设置为 true，Unity 会将 Projection Matrix 的 y 轴翻转，这样子经过视口变换（uv 的 v 再次翻转），就统一了 OpenGL 下和 DirectX 下的 uv 了（即原点在左下角），这也是普通的 Unity Shader 中我们不用关心 uv 原点的位置的原因。但是直接绘制 RT 的 Shader 就不同了，因为此时的 uv 和 positionHCS 是我们生成的，而不是通过 Projection Matrix 计算而得，所以之前在顶点着色器中，将 uv 的 v 轴手动翻转了，positionHCS 无需手动翻转是因为视口变换会翻转。所以下面计算 Camera Motion Vector 要特别注意 uv 的方向。

    float4 GetNDCFromUVAndDepth(float2 uv, float depth)
    {
        #if UNITY_UV_STARTS_AT_TOP
            uv.y = 1.0f - uv.y;
        #else
            depth = 2.0 * depth - 1.0;
        #endif
        
        return float4(2.0 * uv - 1.0, depth, 1.0);
    }

    float3 TransformNDCToWorld(float4 NDC, float4x4 invViewProjMatrix)
    {
        float4 positionHWS = mul(invViewProjMatrix, NDC);
        return positionHWS.xyz / positionHWS.w;
    }

    float4 CameraMotionVectorFrag(Varyings IN) : SV_TARGET
    {
        float depth = LOAD_TEXTURE2D_LOD(_CameraDepthTexture, IN.positionHCS.xy, 0).r;
        float4 NDC = GetNDCFromUVAndDepth(IN.uv, depth);
        float3 currentPositionWS = TransformNDCToWorld(NDC, UNITY_MATRIX_I_VP);

        float4 currentPositionCS = mul(UNITY_MATRIX_NONJITTERED_VP, float4(currentPositionWS.xyz, 1.0));
        float4 previousPositionCS = mul(UNITY_PREV_MATRIX_NONJITTERED_VP, float4(currentPositionWS.xyz, 1.0));
        
        float2 currentPositionNDC = currentPositionCS.xy * rcp(currentPositionCS.w);
        float2 previousPositionNDC = previousPositionCS.xy * rcp(previousPositionCS.w);
        
        float2 velocity = currentPositionNDC - previousPositionNDC;
        
        #if UNITY_UV_STARTS_AT_TOP
        velocity.y = -velocity.y;
        #endif
        
        velocity *= 0.5;

        return float4(velocity, 0,0);
    }

另外要注意的是，计算 Camera Motion Vector 的时候要去除掉 jitter 的影响，否则得到的 Motion Vector 是不对的，使用了会导致画面糊，所以上面计算时，使用了 `NONJITTERED_VP` 矩阵。然后在 TAA Shader 采样时，减去 Motion Vector：  

    float4 TAAFrag(Varyings IN) : SV_TARGET
    {
        float2 velocity = LOAD_TEXTURE2D_LOD(_CameraMotionVectorTexture, IN.positionHCS.xy, 0).rg;
        
        float3 history = SAMPLE_TEXTURE2D_LOD(_TAAHistory, sampler_LinearClamp, IN.uv - velocity, 0).xyz;
        float3 current = LOAD_TEXTURE2D_LOD(_BlitTexture, IN.positionHCS.xy, 0).xyz;

        float3 color = lerp(current, history, _TAAParams.x);
        return float4(color, 1.0);
    }

注意，采样 history 时，因为要减去 velocity，得到的新 uv 值不会正好在像素中心，此时若使用 load 或者 point 采样不合理，效果也不好，会出现涂抹 smear 感，建议使用 linear 采样。但是使用 linear 采样，又会造成模糊感，这也是一个较为重要的优化地方，在其他优化技术的 History Filter 小节中会详细介绍，目前为止的效果如下：  

> 在后处理中，因为我们是绘制全屏三角形，设置了 3 个顶点 uv 值为 (0, 0)、(2, 0)、(0, 2)，当 uv 通过光栅化插值时，uv 会天然满足精确对应到纹素中心，此时和 SV_POSITION 语义的 xy 分量是对齐的，都代表纹素中心，只不过 SV_POSITION 是像素坐标。如果采样在纹素中心，此时 linear 和 point 采样得到的结果会是一样的。

<div align="center">  
<img src="https://s2.loli.net/2025/07/10/Xgvw4oALqMxWShn.gif" width = "512" height = "512" alt="图9 - After Camera Motion Vector"/>
</div>

可以看到，虽然可以看清物体了，但是还是有鬼影现象，这是由于场景中的遮挡关系发生了变化。比如上图中，在这一帧中，机器人的位置可以拿到 Motion Vector 得到上一帧中机器人的颜色进行混合，但是机器人右上位置的像素，在上一帧中被机器人遮挡，在这一帧中没被机器人遮挡，同时 Motion Vector 也为 0，那么这些像素就会混合到上一帧中机器人的颜色，从而导致鬼影。所以 Camera Motion Vector 只能消除一部分的鬼影。当然鬼影现象还会因为其他因素引起，比如灯光的变化等等，这就需要我们去验证历史数据，拒绝不能使用的历史数据。

# 验证历史样本
那么如何验证历史样本呢？一般来说，有两类验证历史样本可信度的信息，即**几何信息 geometry data** 和**颜色信息 color data**。几何信息包括物体深度、速度以及 object ID 等等。使用几何信息拒绝历史样本相对于颜色信息来说没有那么 Robust，因为无法处理诸如灯光变化所产生的鬼影问题，所以这里主要详细介绍 Color Rejection，Color Rejection 可以说是 TAA 离开不了的一环。Geometry Rejection 的相关方法在下面只大致介绍一下思路。

## Color Rejection/Rectification
### Color Clamping
Color Clamping 假设采样点周围样本对于 TAA 累计过程是有效的，历史样本如果跟当前帧样本出现较大偏差，那么历史样本就应该被拒绝。但相较于直接拒绝历史样本，Color Clamping 选择将历史样本钳制到当前帧样本周围 5 个样本或 9 个样本组成的 AABB 包围盒中：  

    float4 TAAFrag(Varyings IN) : SV_TARGET
    {
        float2 velocity = LOAD_TEXTURE2D_LOD(_CameraMotionVectorTexture, IN.positionHCS.xy, 0).rg;
        float3 history = SAMPLE_TEXTURE2D_LOD(_TAAHistory, sampler_LinearClamp, IN.uv - velocity, 0).xyz;

        float3 current = LOAD_TEXTURE2D_LOD(_BlitTexture, IN.positionHCS.xy, 0).xyz;

        float3 N = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(0, 1)).xyz;
        float3 E = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(1, 0)).xyz;
        float3 S = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(0, -1)).xyz;
        float3 W = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(-1, 0)).xyz;
        #if _TAA_SAMPLE_3X3
        float3 NW = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(-1, 1)).xyz;
        float3 NE = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(1, 1)).xyz;
        float3 SW = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(-1, -1)).xyz;
        float3 SE = LoadOffset(_BlitTexture, IN.positionHCS.xy, int2(1, -1)).xyz;
        #endif

		float3 min = min(current, min(N, min(E, min(S, W))));
		float3 max = max(current, max(N, max(E, max(S, W))));
        #if _TAA_SAMPLE_3X3
        min = min(min, min(NW, min(NE, min(SW, SE))));
        max = max(max, max(NW, max(NE, max(SW, SE))));
        #endif
        
        history = clamp(history, min, max);
        float3 color = lerp(current, history, _TAAParams.x);
        return float4(color, 1.0);
    }

由于临近采样点的亮度变化可能会很大，这会导致 AABB 包围盒很大，从而重现鬼影现象。Epic Games 的 Karis 在 SIGGRAPH 2014 的演讲 [High Quality Temporal Supersampling](https://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf) 有提到使用 **YCoCg** 色彩空间可以使 AABB 包围盒更加紧致，因为它将颜色的色度 Chroma（即 Co, Cg 通道）从亮度（Y 通道）分离了出来，而亮度往往占据颜色差值的主导地位。

> YCoCg 色彩空间包含 Y、Co、Cg 三个通道，分别对应亮度、绿色色度（chrominance green）和橙色色度（chrominance orange）。  

YCoCg 到 RGB 的转换是线性变换，代码如下：  

    float3 RGB2YCoCg(float3 rgb) {
        return float3(
                rgb.x/4.0 + rgb.y/2.0 + rgb.z/4.0,
                rgb.x/2.0 - rgb.z/2.0,
                -rgb.x/4.0 + rgb.y/2.0 - rgb.z/4.0);
    }

    float3 YCoCg2RGB(float3 YCoCg) {
        return float3(
                YCoCg.x + YCoCg.y - YCoCg.z,
                YCoCg.x + YCoCg.z,
                YCoCg.x - YCoCg.y - YCoCg.z);
    }

最后 YCoCg 色彩空间下的 3X3 的 9 个样本的 Color Clamping 的结果如下：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/11/3IAFVrQbg6X4ZtE.gif" width = "512" height = "512" alt="图10 - After Color Clamping"/>
</div>

其实这样子 TAA 已经勉强成形了，后面的技术的目的都是进一步优化 TAA 以减少鬼影和闪烁问题。

### Color Clipping
Color Clipping 可以说是 Color Clamping 的进阶版本，用一张图可以概括它们的区别：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/11/Nq6mJQIWw2z3clg.jpg" width = "30%" height = "30%" alt="图11 - AABB clipping and clamping"/>
</div>

我在网上找到有两种 Color Clipping 的实现方法，一种是 UE4 中的实现，一种是 [Playdead](https://github.com/playdeadgames/temporal/blob/4795aa0007d464371abe60b7b28a1cf893a4e349/Assets/Shaders/TemporalReprojection.shader) 的实现（它在 GDC 2016 的演讲：[Temporal Reprojection Antialiasing in INSIDE](https://www.gdcvault.com/play/1022970/Temporal-Reprojection-Anti-Aliasing-in)）。UE4 的实现我没在网上找到具体的来源，虽然是开源的，但是登录 Github 还要 Epic 账号才能查看，比较麻烦，而且网上也有很多人已经将这些代码摘抄下来了。Unity 的 [HDRP](https://github.com/Unity-Technologies/Graphics/blob/master/Packages/com.unity.render-pipelines.high-definition/Runtime/PostProcessing/Shaders/TemporalAntialiasing.hlsl) 里面这两种实现也都有，可以直接查看。

#### Playdead 的 Clip to AABB Center
Playdead 分享的库中的源代码如下：  

    float4 clip_aabb(float3 aabb_min, float3 aabb_max, float4 p, float4 q)
    {
        // note: only clips towards aabb center (but fast!)
        float3 p_clip = 0.5 * (aabb_max + aabb_min);
        float3 e_clip = 0.5 * (aabb_max - aabb_min) + FLT_EPS;

        float4 v_clip = q - float4(p_clip, p.w);
        float3 v_unit = v_clip.xyz / e_clip;
        float3 a_unit = abs(v_unit);
        float ma_unit = max(a_unit.x, max(a_unit.y, a_unit.z));

        if (ma_unit > 1.0)
            return float4(p_clip, p.w) + v_clip / ma_unit;
        else
            return q;// point inside aabb
    }

其中 q 是 history，这个 p 不用管它，影响不到颜色。Playdead 的命名很难看懂，p_clip 就是 AABB center，e_clip 就是 AABB extent，FLT_EPS 是最小浮点数，防止除 0 的。v_clip 就是 history 到 AABB center 的距离（或向量），v_unit 就是 history 到 AABB center 的距离除以 AABB 边缘到 AABB center 的距离，它是一个距离倍数。只要这个距离倍数有一个分量大于 1 了，则认为需要 Clip，重新计算 history，计算方式为 AABB center 加上使用最大距离倍数缩放后的 v_clip。可以看到该方法是从 AABB center 出发的 Clip，因此我觉得从逻辑上来说，效果应该是不如 UE4 的实现的，我实际使用下来发现也确实如此，效果如下：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/11/1FzUWbOJwC7LRrd.gif" width = "512" height = "512" alt="图12 - Clip to AABB Center"/>
</div>

#### UE4 的 Clip to Filtered Color
我在网上找到不同人摘抄的这段 UE4 的代码，计算方式可能略有不同，但逻辑都是一样的，跟 Unity HDRP 里的也差不多：  

    float3 ClipToFiltered(float3 NeighborMin, float3 NeighborMax, float3 Filtered, float3 History)
    {
        float3 BoxMin = NeighborMin;
        float3 BoxMax = NeighborMax;

        float3 RayOrigin = History;
        float3 RayDir = Filtered - History;
        RayDir = abs(RayDir) < (1.0/65536.0) ? (1.0/65536.0) : RayDir;
        float3 InvDir = rcp(RayDir);

        float3 MaxIntersect = (BoxMax - RayOrigin) * InvDir;
        float3 MinIntersect = (BoxMin - RayOrigin) * InvDir;
        float3 EnterIntersect = min(MinIntersect, MaxIntersect);
        float ClipBlend = max3(EnterIntersect.x, EnterIntersect.y, EnterIntersect.z);
        ClipBlend = saturate(ClipBlend);
        return lerp(history, filtered, ClipBlend);
    }

上述代码和 playdead 的主要区别是使用了预过滤的当前帧中间颜色值，即 Filtered，而非 AABB 中心。它主要计算了三个方向，min 到 history 的方向、max 到 history 的方向、filtered color 到 history 的方向。MaxIntersect 就是 max 到 history 的方向所占 filtered color 到 history 的方向的比例，MinIntersect 同理，经过一次 min 一次 max 得到在 filtered color 和 history 的混合比例，混合后作为新的 history color。

理论上要使用 filtered color，我这里的展示是用了没过滤的中间颜色值替代的，预过滤在其他优化技术中详细说明，预过滤是一个抑制闪烁的有效方法，现在效果如下（展示的 gif 和上面的效果也感受不出太大区别，但是我在其他场景测试后能感受出这个方法的闪烁会更少一些，鬼影没怎么看出区别）：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/11/YgpwKOy23VFnm5i.gif" width = "512" height = "512" alt="图13 - Clip to Filtered Color"/>
</div>

### Variance Clipping
因为物体边缘的亮度变化往往较大，这会使 AABB 包围盒过大，导致物体边缘的抗锯齿效果不佳，特别是一些有较亮或较暗边缘的物体。为了解决这个问题，NVIDIA 在 GDC 2016 的演讲：[An Excursion in Temporal Supersampling](https://developer.download.nvidia.com/gameworks/events/GDC2016/msalvi_temporal_supersampling.pdf) ，提出了 **Variance Clipping** 的方法，它使用了平均数和标准差来定义 AABB 包围盒。（Variance Clipping 是建立 AABB 的一种方式，和 color clamp/clip 可以同时使用，只不过之前建立 AABB 使用的是 min max 方法。）

理论上最好的 clipping 应该是基于**凸包 Convex Hull** 的 clipping（凸包指做小能包含点集中所有的点的凸多边形），如下图所示：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/14/bDnzUpil6XxG8By.jpg" width = "40%" height = "40%" alt="图14 - Convex Hull"/>
</div>

因为凸包相交计算非常昂贵，所以退而求其次，普遍选择的是 AABB 包围盒，但是 AABB Clip 到的颜色可能会离凸包的结果相差较远，从而导致鬼影现象。于是 NVIDIA 开发出了使用统计学方法的 AABB 包围盒来替代之前 min/max 方法下的 AABB 包围盒，以确保更紧致的包围盒。它使用标准正太分布（z 分布）的区间估计来建立 AABB 包围盒，公式如下：  

$$ \mu \pm \gamma \sigma $$

$\,\mu\,$ (mu) 代表颜色的平均数，$\,\sigma\,$ (sigma) 代表颜色的标准差 standard deviation，$\,\gamma\,$ (gamma) 则是临界值 Critical Value 或者称为标准差乘数 Standard Deviation Multiplier，NVIDIA 建议 $\,\gamma\,$ 在 \[0.75, 1.25\] 之间。代码大致如下（标准差使用 $\,\sigma = \sqrt {\sum{X^2}/N - (\sum X / N)^2}\,$ 计算）：  

    void VarianceNeighbourhood(in Neighbourhoods samples, out float3 neighborMin, out float3 neighborMax, float gamma = 1.25)
    {
        float3 m1 = 0;
        float3 m2 = 0;
        for (int i = 0; i < NEIGHBOURHOOD_COUNT; i++)
        {
            float3 sampleColor = samples[i];
            m1 += sampleColor;
            m2 += sampleColor * sampleColor;
        }

        m1 *= rcp(NEIGHBOURHOOD_COUNT);
        m2 *= rcp(NEIGHBOURHOOD_COUNT);

        float3 sigma = sqrt(abs(m2 - m1 * m1)); // standard deviation
        neighborMin = m1 - gamma * sigma;
        neighborMax = m1 + gamma * sigma;
    }

上述代码还可以进一步修改，NVIDIA 的 PPT 中有提到说可以和 min/max 的 AABB 做 clamp，确保 variance 不会比 min/max 的更大，从而做到让 AABB 更小：  

    neighborMin = max(MinMaxAABB_Min, VarianceAABB_Min);
    neighborMax = min(MinMaxAABB_Max, VarianceAABB_Max);

而 UE4 则防止了 variance AABB 比 filter color 还紧致：  

    NeighborMin = min(VarianceAABB_Min, FilteredColor);
    NeighborMax = max(VarianceAABB_Max, FilteredColor);

Unity HDRP 还对 gamma 值（Standard Deviation Multiplier）进行了自适应动态处理，有兴趣可以去翻翻 HDRP 的 TAA，这里就不摘抄了。展示图片也不上传了，反正在劣质分辨率下也看不出来区别（我使用的图床免费情况最大只支持 5 M，/(ㄒoㄒ)/~~）。

## Geometry Rejection
说明一下，这里主要讲解如何消除鬼影，实际上也可以使用几何信息进行判断来控制闪烁现象，所以要和下面其他优化技术中的 Adaptive Blending Factor 进行一定程度的区分。但本质上来说，将 $\,\alpha\,$ 设置为 1 拒绝历史消除鬼影，从理论上来讲也属于自适应/动态 $\,\alpha\,$ 的范畴，闪烁和鬼影本就一体两面，这里为了方便理解将这二者进行区分，减少闪烁后面再讲。Geometry Rejection 的主要方法有如下几种：  

①**Depth Rejection**：这个方法假设每帧之间的深度不会发生显著变化，深度变化较大的不属于同一像素点。要实现该方法，需要存储上一帧的深度纹理。如果深度发生显著变化，则将 $\,\alpha\,$ 设置为 1，放弃历史数据，或者只增大 $\,\alpha\,$ 的值。这个方法可能比较适用于特殊的游戏，例如《这是我的战争》（This War of Mine）这样的 3d 横板游戏，因为镜头以平移为主，深度不会发生较大变化。但可想而知，这个方法对于一般的 3d 游戏相对来说没有这么好用。  

②**Stencil Rejection**：顽皮狗在 SIGGRAPH 2016 的演讲 [Temporal Antialiasing in Uncharted 4](https://advances.realtimerendering.com/s2016/) 有介绍这一方法的使用，大致意思就是对于一些主要渲染物体，比如人物，使用 Stencil Buffer 存储模板值，也要保留上一帧的 Stencil Buffer，进行比较，不同模板值的不是同一物体，此时只采样当前帧颜色，不混合历史帧。这个方法我觉得比较适用于屏幕长期占用着主要渲染物体的游戏，比如第三人称游戏或者赛车游戏等等。  

③**Velocity Rejection**：Velocity Rejection 可以有两种思路：一是类似于 Depth Rejection 的思路，就是比较前一帧和这一帧之间的速度差，速度差较大，则拒绝历史，也要存储上一帧的 motion vector。二是 motion vector 速度越大，越增加 $\,\alpha\,$ 的值。~~我觉得第二个思路肯定是更好的，它也可以一定程度上减弱闪烁现象~~ ，比如 motion vector 几乎为 0 时，将 $\,\alpha\,$ 设置得很小，具体在 Adaptive Blending Factor 讲解。

<font color='red'> TODO: Object Motion Vector 完成后再回来，上面都需要修改 </font>

# 其他优化技术
## 闪烁优化
Color Rejection/Rectification 会在一定程度下增加闪烁问题，这是因为历史的累计过程可以吸收颜色差异，但是 Color Rejection/Rectification 在抖动的过程中有可能会错误地打破历史的累计，特别是一些具有较大颜色变化的边缘。取消 Color Rejection/Rectification 可以减少闪烁问题，但是又会增加鬼影问题。所以当使用了 Color Rejection/Rectification，减少闪烁和减少鬼影的目标是相互矛盾的。上述内容可以概括为：**False Positives**（有效的历史被无效）会导致模糊以及闪烁，**False Negatives**（无效的历史被有效）会导致鬼影。

这也是 TAA 的一个巨大的坑，我也在这里浪费了不少时间，结果只能说是差强人意吧。同时，我觉得在 TAA 中彻底做到无闪烁应该是不太可能的。下面介绍的方法中，我会详细说明一些自己比较喜欢的方案，对于一些我不喜欢的方案会简单介绍思路（因为可能要多保留历史贴图，比如 depth 和 motion vector，但是效果可能确实会更好）。另外，这些方案需要根据项目需求和喜好选择使用，不推荐一股脑全加进 TAA 里面。

<font color='red'> TODO: 修改上面这段话 </font>  
  

闪烁问题又可以被分为**高光闪烁**（着色闪烁）和**几何闪烁**。高光闪烁可以理解为高光等在着色后由于抖动出现不连续的闪光点，产生的原因跟 bloom 闪烁类似，解决方法也一样；几何闪烁则是因为抖动后，光栅化后得到的像素点的几何内容是不确定的，特别是存在密集三角形的区域，比如树叶、栏杆等等，物体越远这个现象也会越明显。下面讲解时，我会专门说明该方法是处理哪一类闪烁问题的。

### Luma Weighted Exponential Blending
这个方法主要解决的是高光闪烁问题，这里顺便讨论一下 TAA 在 Pipeline 的位置问题，上面将 TAA 放置在了所有后处理之前，即是在 HDR 线性空间下进行的。在这样的情况下，物体或相机移动，甚至静止状态下的抖动，都会导致物体的高光计算的剧烈变化，从而导致高光的闪烁问题。可能有人会说为什么不把 TAA 放在 Tone Mapping 之后，这样子就能解决高光闪烁问题。因为 bloom 或者 lens flare 效果可能会增大因高亮度颜色产生的 alias 问题，TAA 放在 Tone Mapping 之后的话 bloom 闪烁问题也会比较严重。还有一点是在 Tone Mapping 之前，相对来说更物理正确，因为 Tone Mapping 之前是线性空间，之后是非线性空间。但是的确 TAA 在 LDR 下的效果会比 HDR 下要好，这就产生了一个妥协的办法，即类似于 Bloom 中的解决方案，使用 Karis Average 来混合当前帧与历史帧从而缓解高光闪烁现象，如下：

$$ w(c) = \cfrac {1} {1 + Luminance(c)} $$

代码如下：  

    float3 LumaExponentialAccumulation(float3 history, float3 current, float blendFactor)
    {
        float historyLuma = Luminance(history);
        float currentLuma = Luminance(current);
        float historyLumaWeight = rcp(historyLuma + 1.0);
        float currentLumaWeight = rcp(currentLuma + 1.0);
        float weightSum = lerp(currentLumaWeight, historyLumaWeight, blendFactor);
        float3 blendColor = lerp(current * currentLumaWeight, history * historyLumaWeight, blendFactor);
        return blendColor / weightSum;
    }

上述方法的缺点就是会在一定程度上压制高亮度物体的亮度。还有一点要注意的是，不要对 YCoCg 空间下的颜色使用 `Luminance()` 函数，因为 YCoCg 的 Y 就是亮度，直接使用即可，否则会出现颜色问题。

### Filter Current Color
这个方法就是过滤当前帧的颜色，主要解决高光闪烁问题，也可以略微减少点几何闪烁问题，缺点就是会模糊，最好能配合额外的后处理锐化重建使用（推荐 Mitchell-Netravali Filter，据说比 Catmull-Rom 要好）。可以使用 Box Kernel 或 Gaussian Kernel 来进行模糊，但是不建议使用 Box Kernel，因为 Box Kernel 对锐化重建相对来说不太友好，并且相对来说也更模糊。下面是 Gaussian Kernel 的代码：  

    float3 GaussianFilterMiddleColor(in Neighbourhoods samples)
    {
        const float weights[9] = { 4.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0 };
        float weightSum = 0;
        float3 filtered = 0;

        for (int i = 0; i < NEIGHBOURHOOD_COUNT; i++)
        {
            float lumaWeight = rcp(GetLuma(samples[i]) + 1.0) * weights[i];
            weightSum += lumaWeight;
            filtered += lumaWeight * samples[i];
        }
        filtered *= rcp(weightSum);
        return filtered;
    }

由于会模糊，这个方法应该根据项目的美术要求来选择是否使用。觉得太模糊也可以选择 $\,\sigma\,$ 更小的高斯核，我自己测试下来 $\,\sigma\,$ 大概在 0.6 左右，在 1080 p 的近距离时也基本上看不出来模糊的感觉，但是能略微抑制闪烁现象。下图为实现了 Luma Weighted Exponential Blending 和对当前帧颜色进行了 Filter 后的效果，可以看到高光闪烁得到了很好的抑制，缺点就是高光颜色感觉变暗了：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/25/sdOp7mSIloRf9qW.gif" width = "512" height = "512" alt="图15 - Luma Weighted Exponential Blending 和 $\,\sigma\,$ 为 0.5 的 Gaussian Filter 后的效果"/>
</div>

### Dynamic/Adaptive Blending Factor
自适应 $\,\alpha\,$ 应该是减少几何闪烁的最好的方式了，逻辑跟 Geometry/Color Rejection 是类似的，只不过 Geometry/Color Rejection 强调拒绝历史解决鬼影现象，自适应 $\,\alpha\,$ 强调在解决鬼影和解决闪烁之间寻求平衡。具体思路其实很简单：就是当物体和相机移动时，处理鬼影现象优先于处理闪烁现象（即增加 $\,\alpha\,$ 的值），特别高速移动的物体我们不太在乎是否闪烁，反正也看不出来；当物体和相机静止时，处理闪烁现象优先于处理鬼影现象（即减小 $\,\alpha\,$ 的值），因为静止状态下闪烁极其碍眼，影响体验。那么如何判断物体或相机是否在移动，还是老样子，就是使用颜色信息或几何信息进行判断。由于自适应 $\,\alpha\,$ 比较经验主义，里面会有很多魔法数字，所以对于下面的方法以讲逻辑为主，代码仅供参考，对于不同项目不同需求需要对代码做出适当调整。

①**Velocity Weighted Blending Factor**  
跟 Velocity Rejection 一样有两种思路：  
&emsp;&emsp; - 一是比较前一帧和这一帧之间的速度差，速度差越大，越增加 $\,\alpha\,$ 的值，具体可以看 CryEngine 3 在 SIGGRAPH 2011 的演讲 [Anti-Aliasing Methods in CryEngine 3](https://www.iryoku.com/aacourse/downloads/13-Anti-Aliasing-Methods-in-CryENGINE-3.pdf)，CryEngine 3 利用了 alpha 通道来存储了上一帧的速度，代码我就不摘录了。理论上来说这种额外存储上一帧的速度信息的效果应该比只使用一帧的速度判断（方法二）更好，因为 TAA 的混合本质上是一个多帧混合的状态，上上帧是否在运动也会影响到当前帧的结果（只要没有彻底抛弃历史，即 $\,\alpha\,$ 为 1 时）；  
&emsp;&emsp; - 二是直接使用当前帧的速度大小来调整 $\,\alpha\,$ 的值，代码大致如下：  

    float velocityLengthSqr = dot(velocity, velocity);
    blendFactor = lerp(blendFactor - 0.025, 1, saturate(velocityLengthSqr));

②**Depth Weighted Blending Factor**  
这个方法最好和 Velocity Weighted Blending Factor 同时使用，也有多种思路：  
&emsp;&emsp; - 一是比较这一帧与上一帧的深度差，若有深度差，且当速度为 0 时，则认为在几何闪烁，此时取消 history clamp 或者减小 $\,\alpha\,$ 的值。因为要保留额外的历史深度纹理，我没有使用该方法，但我觉得该方法的效果应该会很好；  
&emsp;&emsp; - 二是深度值越深，则 $\,\alpha\,$ 值越低。这么做的理由是，物体越远就越小，几何闪烁就越剧烈，而且物体很远时，即使移动产生鬼影也看不清楚（当然可以配合 Velocity Weighted 减少鬼影）。这个方法比较适合一些无需盯着远景的游戏，不太适合需要使用枪械或弓箭的游戏，代码大致如下（假设 reversed-z），就是要注意 skybox 的情况（深度为 0），此时最好将 $\,\alpha\,$ 设置为 1：  

    blendFactor = depth == 0 ? 1 : lerp(0.025, blendFactor, sqrt(depth));

③**Color Weighted Blending Factor**  

Karis [Kar14] reduces blend factor α when history is near clamping, in order to soften temporal change after clamping happens. 

https://research.activision.com/publications/archives/filmic-smaasharp-morphological-and-temporal-antialiasing 里也有

playdead 根据亮度变化确定混合系数

<font color='red'> TODO: Object Motion Vector 完成后再回来，上面都需要修改 </font>

由于几何闪烁的观察，需要特殊的场景，我这里就不放图片了

## 重投影优化
由于 motion vector 没有被抗锯齿处理过，使用 motion vector 进行重投影时会间接引入锯齿，特别是移动物体的边缘会因此出现锯齿。一种常见的解决方案就是采样 motion vector 时放大靠前物体的边缘，称为 **Depth Dilation**。具体来说就是，使用周围几个像素点的最近深度的像素点的 motion vector 进行重投影，以此达到更加平滑的效果，代码如下：  

    float2 GetClosestDepthPixelCoord(TEXTURE2D(depthTex), int2 pixelCoord, out float depth)
    {
        float M = LoadOffset(depthTex, pixelCoord, int2(0, 0)).x;
        float N = LoadOffset(depthTex, pixelCoord, int2(0, 1)).x;
        float E = LoadOffset(depthTex, pixelCoord, int2(1, 0)).x;
        float S = LoadOffset(depthTex, pixelCoord, int2(0, -1)).x;
        float W = LoadOffset(depthTex, pixelCoord, int2(-1, 0)).x;
        #if _TAA_SAMPLE_3X3
        float NW = LoadOffset(depthTex, pixelCoord, int2(-1, 1)).x;
        float NE = LoadOffset(depthTex, pixelCoord, int2(1, 1)).x;
        float SW = LoadOffset(depthTex, pixelCoord, int2(-1, -1)).x;
        float SE = LoadOffset(depthTex, pixelCoord, int2(1, -1)).x;
        #endif

        float3 offset = float3(0, 0, M);
        offset = lerp(offset, float3(0, 1, N), COMPARE_DEVICE_DEPTH_CLOSER(N, offset.z));
        offset = lerp(offset, float3(1, 0, E), COMPARE_DEVICE_DEPTH_CLOSER(E, offset.z));
        offset = lerp(offset, float3(0, -1, S), COMPARE_DEVICE_DEPTH_CLOSER(S, offset.z));
        offset = lerp(offset, float3(-1, 0, W), COMPARE_DEVICE_DEPTH_CLOSER(W, offset.z));
        #if _TAA_SAMPLE_3X3
        offset = lerp(offset, float3(-1, 1, NW), COMPARE_DEVICE_DEPTH_CLOSER(NW, offset.z));
        offset = lerp(offset, float3(1, 1, NE), COMPARE_DEVICE_DEPTH_CLOSER(NE, offset.z));
        offset = lerp(offset, float3(-1, -1, SW), COMPARE_DEVICE_DEPTH_CLOSER(SW, offset.z));
        offset = lerp(offset, float3(1, -1, SE), COMPARE_DEVICE_DEPTH_CLOSER(SE, offset.z));
        #endif

        depth = offset.z;
        return pixelCoord + offset.xy;
    }

上述代码就是采样当前像素点周围的 5 个或 9 个像素点，找到最近的像素点作为采样 motion vector 的坐标：  

    float closestDepth;
    float2 velocityPixelCoord = GetClosestDepthPixelCoord(_CameraDepthTexture, IN.positionHCS.xy, closestDepth);
    float2 velocity = LOAD_TEXTURE2D_LOD(_CameraMotionVectorTexture, velocityPixelCoord, 0).rg;

这个方法膨大了物体的边缘以此减少边缘锯齿。还有一种方法叫做 **Magnitude Dilation**，Depth Dilation 选取的是 neighborhood 中最近像素点的 motion vector，Magnitude Dilation 选取的是 neighborhood 中速度最大的 motion vector，具体我没有实现，不知道效果好不好，有兴趣可以自己尝试。Depth Dilation 的效果如下，仔细观察物体边缘，可以看出物体边缘跟上面比锯齿相对来说少了：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/25/sXKMAf4DVGL5Oe3.gif" width = "512" height = "512" alt="图16 - Closest Velocity（No Adaptive Blending Factor）"/>
</div>

## 模糊优化
TAA 的模糊大致上有 3 个来源：  
①为了抗闪烁，过滤了当前帧颜色引入的模糊；  
②在重投影小节中有提到过，采样 TAA History 贴图时，由于减去 Motion Vector 导致采样点不在像素点中心位置，所以必须使用 bilinear 采样 TAA History，同时这在一定程度上也引入了模糊。又由于 TAA 会在历史不断累计，导致这种模糊也会在历史中不断累计，更加强了模糊。而解决这种模糊的方式就是使用有锐化功能的过滤核采样 TAA History，比如 Catmull-Rom Bicubic Filter，在后面小节中详细说明；  
③历史验证以及混合导致的模糊。首先历史验证有个重要假设，即周围像素点包含了当前像素点所覆盖的表面颜色，但是对于高频颜色信息，由于抖动后光栅化的不确定性，这个假设会被打破，导致高频细节的丢失，这一问题经常出现在细节较多的场景。当然历史混合本身也会增加模糊。  

① 和 ③ 这两个来源没有什么特别好的解决方案，只能靠后处理锐化重建高频细节（在 TAA 完成后重建），可以使用 Laplacian Filter、Catmull-Rom Bicubic Filter 或者 Mitchell-Netravali Bicubic Filter，这一块我在本篇文章中就不详细说明了，详细搜索上述关键词（其实图像重建跟 Super-Resolution 技术有很强的关联性，我会在后面的 Super-Resolution 小节中大致说明，作为 TAA 技术的补充）。而 ② 由于 TAA 时就已经拿到 TAA History，所以可以直接在 TAA 时对 TAA History 进行过滤，即下面要讲的 History Filter。

### History Filter
常规的 Bicubic Filter 需要采样 16 次，可以被优化到只需采样 9 次，详见链接：https://gist.github.com/TheRealMJP/c83b8c0f46b63f3a88a5986f4fa982b1 。而动视 Activision 进行了进一步优化，在 SIGGRAPH 2016 的演讲 [Filmic SMAA: Sharp Morphological and Temporal Antialiasing](https://research.activision.com/publications/archives/filmic-smaasharp-morphological-and-temporal-antialiasing) 中提出了一个只需要使用 5 次采样的 Catmull-Rom Bicubic Filter，代码如下：  

    float3 SampleHistoryBicubic(TEXTURE2D(tex), float2 uv)
    {
        float2 samplePos = uv * _CameraBufferSize.zw;
        float2 tc1 = floor(samplePos - 0.5) + 0.5;
        float2 f = samplePos - tc1;
        float2 f2 = f * f;
        float2 f3 = f * f2;

        float c = 0.5; // sharpen factor (0, 1)
        
        float2 w0 = -c         * f3 +  2.0 * c         * f2 - c * f;
        float2 w1 =  (2.0 - c) * f3 - (3.0 - c)        * f2          + 1.0;
        float2 w2 = -(2.0 - c) * f3 + (3.0 - 2.0 * c)  * f2 + c * f;
        float2 w3 = c          * f3 - c                * f2;

        float2 w12 = w1 + w2;
        float2 tc0 = _CameraBufferSize.xy   * (tc1 - 1.0);
        float2 tc3 = _CameraBufferSize.xy   * (tc1 + 2.0);
        float2 tc12 = _CameraBufferSize.xy  * (tc1 + w2 / w12);

        float3 s0 = SampleHistoryLinear(tex, float2(tc12.x, tc0.y));
        float3 s1 = SampleHistoryLinear(tex, float2(tc0.x, tc12.y));
        float3 s2 = SampleHistoryLinear(tex, float2(tc12.x, tc12.y));
        float3 s3 = SampleHistoryLinear(tex, float2(tc3.x, tc12.y));
        float3 s4 = SampleHistoryLinear(tex, float2(tc12.x, tc3.y));

        float cw0 = (w12.x * w0.y);
        float cw1 = (w0.x * w12.y);
        float cw2 = (w12.x * w12.y);
        float cw3 = (w3.x * w12.y);
        float cw4 = (w12.x *  w3.y);

        s0 *= cw0;
        s1 *= cw1;
        s2 *= cw2;
        s3 *= cw3;
        s4 *= cw4;

        float3 historyFiltered = s0 + s1 + s2 + s3 + s4;
        float weightSum = cw0 + cw1 + cw2 + cw3 + cw4;

        float3 filteredVal = historyFiltered * rcp(weightSum);

        return filteredVal;
    }

在静止场景中，使用 Catmull-Rom Bicubic 过滤 TAA History 可能看不出区别。但是在动态场景中，是可以感受出 Catmull-Rom 过滤后会更加清晰，效果如下，确实比上面 Gaussian Filter 后的效果要清晰一点，虽然不多：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/25/ZcXodYyfsmvwuES.gif" width = "512" height = "512" alt="图17 - After Catmull-Rom History Filter（No Adaptive Blending Factor）"/>
</div>

# Object Motion Vector
Object Motion Vector 相对于 Camera Motion Vector 来说复杂了很多，需要考虑更多的情形。从计算方式来说，多了 MVP 矩阵的 M 的变化，以及顶点的 positionOS 的变化：需要使用这一帧的 positionOS 乘上这一帧的 M 矩阵和没有抖动过的 VP 矩阵，得到这一帧未抖动的屏幕空间坐标，再使用上一帧的 positionOS 乘上上一帧的 M 矩阵和没有抖动过的 VP 矩阵，得到上一帧未抖动的屏幕空间坐标，两个坐标相减即 Object Motion Vector（可以看出 Object Motion Vector 是包含 Camera Motion Vector 的）。

但是产生 MVP 矩阵的 M 的变化，以及顶点的 positionOS 的变化的来源有很多：  
①**刚体或变换运动 Rigid & Transform Motion**：物理引擎或者直接修改物体的 transform 本质上都是改变了物体的世界坐标，即 MVP 矩阵的 M。这类移动计算 Object Motion Vector 相对比较容易，只要我们能拿到上一帧的 MVP 矩阵即可计算；  
②**骨骼动画 Skeletal Animation**：骨骼动画改变的是物体的模型空间的顶点坐标，不改变物体的世界坐标，它通过骨骼变换矩阵将顶点变换到新的模型空间坐标，从而达到让物体运动的效果。而 **Root Motion** 技术，实际上是提取了骨骼动画中根骨骼（或其他指定骨骼）的模型空间位移或旋转信息，并将其应用于物体的世界坐标变换当中，所以 Root Motion 可以理解为 Transform Motion 和 Skeletal Animation 的结合体。骨骼动画计算 Object Motion Vector 除了上一帧的 MVP 矩阵，还需要额外的上一帧的 positionOS 信息，所幸的是 Unity 会帮我们保留这部分信息，只要勾选 Skinned Mesh Renderer -> Additional Settings -> Skinned Motion Vectors 即可（默认是勾选的）；  
③**混合形状动画 Blend Shape Animation**：Blend Shape 改变的也是模型空间的顶点坐标，只不过它存储的是每个顶点相对于基础形态在模型空间中的偏移量，在使用中可以根据权重值调节形态。所以跟骨骼动画一样，也需要额外的上一帧的 positionOS 信息，并且 Unity 的 Skinned Mesh Renderer 也帮我们保留这部分信息，只要开启了 Skinned Motion Vectors；  
④**Alembic 动画 Alembic Animation**：Alembic 动画存储了逐帧的几何体数据（包括顶点位置变化或数量变化），所以数据量是极度庞大的，故主要应用于影视级动画。它改变的也是模型空间的顶点坐标，但是 Unity 对 Alembic 动画的处理有点特殊，它额外存储了一个预计算好的模型空间 Velocity，需要在计算 Motion Vectors 额外减去以得到上一帧的模型空间坐标，后面会具体说明；  
⑤**顶点动画 Vertex Animation**：顶点动画就是在顶点着色器中直接修改顶点位置，所以无法使用上述通用的计算 Object Motion Vector 的方法。不同的顶点动画需要有不同的 Motion Vector Pass，我们需要根据自己实现的顶点动画额外计算出顶点在上一帧的位置，从而实现 Object Motion Vector。

一个通用的 Object Motion Vector Pass 只能处理上述的 ①、②、③、④ 情形，对于 ⑤ 则需要不同情况不同处理，下面主要讲解通用的 Object Motion Vector Pass，了解后根据需求处理情形 ⑤ 也不会很难。额外提一下，TAA 是无法处理**序列帧动画 Sequence Frame Animation** 的，因为没有 Motion Vector，序列帧动画主要用于 2D 游戏和一些特效。至于粒子动画，就得看粒子的实现方法了，看是 CPU 粒子还是 GPU 粒子，具体使用了哪些技术，具体情况具体分析，使用序列帧或透明的粒子在 TAA 下肯定会略微模糊的。透明物体的支持对 TAA 来说也是一个难题，我会在后面的 TAA 其他问题说明小节中说明。

## Unity SRP 中实现 Object Motion Vector
受限于 Unity C++ legacy 代码，在 Unity 中渲染 Object Motion Vector 是有不少的坑点的，而且坑点是一环扣一环的，让人感到非常智熄。具体有如下坑点：  

***【书签】***


material 层面禁止 Object Motion Vector Pass，

camera.depthTextureMode = DepthTextureMode.MotionVectors

Motion Vector 遮挡问题

接下来在实现 Object Motion Vector 的过程中，我们会一步接一步地遇到上述几个问题，遇到时我会详细说明。

### Motion Vector Pass in Pipeline
按照上面所说，理论上我们能拿到上一帧的 M 矩阵以及 positionOS 就可以渲染出 Object Motion Vector，而 Unity 也提供了对应的 PerObjectData，我们只需要将


在 SRP 调用 CommandBuffer.DrawRendererList 时，











在 Unity 渲染 Object Motion Vector 有不少的坑点，因为 Unity C++ legacy 代码的限制，它不会对 PerObjectData 的前一帧的

除了 Skinned Mesh Renderer 的 Skinned Motion Vectors 设置外，普通的 Mesh Renderer 的 Motion Vector 也有三个设置，如下图：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/25/f6AqDRbPiJ5IcGH.png" width = "45%" height = "45%" alt="图18 - Mesh Renderer Motion Vectors Option"/>
</div>

选择 Camera Motion Only，Unity 就不会绘制该 Mesh Renderer 的 object motion vector；选择 Per Object Motion，Unity 会绘制在运动物体的 object motion vector，忽略静止物体的 object motion vector；选择 Force No Motion，仍然会渲染 object motion vector，但是强行让运动的物体的 object motion vector 为 0。

### Motion Vector Shader
### 其他说明
还是有问题，motion vector 没被遮挡

试试输出 Motion Vector 顺带输出 Normal：先 depth prepass，再 Motion Vector 和 Normal

# TAA 其他问题说明
透明物体、粒子等等

# Upsampling / Super-Resolution