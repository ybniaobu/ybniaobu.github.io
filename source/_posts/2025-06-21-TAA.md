---
title: Temporal Anti-Aliasing (TAA)
date: 2025-06-21 19:43:03
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2025/06/23/kEPO3zg8IRXwUFC.gif
mathjax: true
description: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
---

> 本来是想先实现 SSGI 或者 SSR 的，但是研究着研究着，就发现 SSGI 或 SSR 的思想都有些许依赖于 SSAO 相关技术算法（SSAO、SSDO、HBAO、GTAO 等等），然后就想着要不先实现 SSAO，但是研究着研究着，又发现 SSAO 等相关技术跟 PCSS 一样较为依赖于降噪技术，降噪技术又可以分为**空间滤波 Spatial Filter** 以及**时间滤波 Temporal Filter**。所以为了打好基础，最终决定先实现 **Temporal Anti-Aliasing (TAA)**，对**抖动 jitter** 有个更深入的理解后，再进入全局光照的实现，顺便也能优化一下 PCSS。
>   
> 本篇文章具体实现是在 Unity 当中，主要参考了以下几篇文章或演讲或 PPT（加粗的文章更综合地介绍了 TAA 技术，建议优先观看）：  
> ① High Quality Temporal Supersampling（SIGGRAPH 2014）：https://advances.realtimerendering.com/s2014/#_HIGH-QUALITY_TEMPORAL_SUPERSAMPLING or https://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf ；  
> ② **A Survey of Temporal Antialiasing Techniques** ：https://research.nvidia.com/labs/rtr/publication/yang2020survey/ ；  
> ③ **Temporal AA and the Quest for the Holy Trail** ：https://www.elopezr.com/temporal-aa-and-the-quest-for-the-holy-trail/ ；  
> ④ Temporal Reprojection Anti-Aliasing in INSIDE（GDC 2016）：https://www.gdcvault.com/play/1022970/Temporal-Reprojection-Anti-Aliasing-in ；  
> ⑤ An Excursion in Temporal Supersampling（GDC 2016）：https://developer.download.nvidia.com/gameworks/events/GDC2016/msalvi_temporal_supersampling.pdf ；  
> ⑥ Temporal Antialiasing in Uncharted 4（SIGGRAPH 2016）：https://advances.realtimerendering.com/s2016/ ；  
> ⑦ Temporal Antialiasing Starter Pack ：https://alextardif.com/TAA.html ；  
> ⑧ Temporal Anti Aliasing – Step by Step ：https://ziyadbarakat.wordpress.com/2020/07/28/temporal-anti-aliasing-step-by-step/ ；  
> ⑨ Adaptive Temporal Antialiasing ：https://research.nvidia.com/publication/2018-08_adaptive-temporal-antialiasing ；  


# TAA 原理及流程简单介绍
**时域抗锯齿 Temporal Anti-Aliasing (TAA)** 用一句话概括可以说是使用了历史帧中的**子/次/亚像素 subpixel** 进行累计从而更有效地实现**超采样抗锯齿 Supersampling Anti-Aliasing (SSAA)**。相对于 SSAA 或 MSAA 来说，TAA 有效地将一个像素多次采样分摊到了连续的几帧当中，即以一个像素一次采样的代价实现了超采样抗锯齿。与此同时，单帧光栅化单像素采样，实际上会“损失”一些亚像素信息的，而 TAA 通过抖动在历史帧中解析出单帧光栅化以及单像素采样中丢失的亚像素信息，从而能够更好地解决**几何锯齿 Geometric Aliasing**（几何光栅化锯齿） 和**着色锯齿 Shading Aliasing**（渲染锯齿），也相对来说增加了基于时序的稳定性。特别是当物体或相机移动时，图元边缘相对于采样点的位置不断变化，导致边缘像素的覆盖率计算结果不断变化，使得边缘像素的颜色在帧与帧之间发生微妙的、高频的亮度变化，而历史帧的混合可以减少这种不稳定性。虽然由于每帧的抖动，会让我们误以为更加不稳定，但实际上可以通过些许 trick 来减少感知上的闪烁问题，从而达到其他后处理抗锯齿技术无法达到的基于时序的稳定性。这也引出了 TAA 的两大难题：**鬼影 ghost** 问题和**闪烁 flicker** 问题，我们在后面重点解决这两个问题。

TAA 大致流程如下所示：  
①首先对摄像机进行**抖动 jitter**，这样子每一帧采样的是一个基于中心位置的被抖动过后的随机位置采样点，将当前帧采样颜色和历史颜色进行混合得到 TAA Target 贴图，后处理后输出。与此同时，将 TAA Target 复制为 TAA History/Accumulation 贴图，将累计的历史颜色信息用于下一帧进行混合；  
②由于相机或物体会进行移动，所以采样历史帧（TAA History）时，不能使用当前的 uv 值，否则采样到的颜色会不是当前像素点或物体的历史颜色信息。这就需要使用上一帧的矩阵进行**重投影 Reprojection** 计算出**运动向量 Motion Vector**，从而还原出当前像素在上一帧的屏幕位置。Motion Vector 又分为 **Camera Motion Vector** 和 **Object Motion Vector**，后面会详细说明；  
③由于运动时遮挡的变化，或者超出屏幕范围，又或者是灯光的变化，会让我们采样到跟当前帧不匹配的历史颜色信息。因此我们需要**验证 Validate** 或**矫正 rectify** 历史信息，以防止鬼影以及闪烁现象的产生。

<div align="center">  
<img src="https://s2.loli.net/2025/07/06/HCp3vjaQ4tswEUq.png" width = "60%" height = "60%" alt="图1 - TAA 基本流程"/>
</div>

下面就开始介绍具体的实现方式，我们先处理静态场景的情形，即摄像机和物体都不运动：

# 累计历史样本
## 抖动 Jitter
由于我们要对一个像素内的多个**子像素 subpixel** 进行采样，故我们需要对采样点的位置进行偏移，即**抖动 Jitter**，通常情况下会使用低差异序列中的 Halton 序列，从而实现更好的抗锯齿效果。Playdead Studios 工作室（《INSIDE》、《地狱边境》的制作厂商）在 GDC 2016 的分享中有提到使用 Halton 序列的前 16 个样本可以产生更好的效果，我也采用了这个方式。Halton 序列等低差异序列的生成就不在这里赘述了，详见《Physically Based Rendering: From Theory To Implementation》中的第八章 Sampling and Reconstruction 的第六节 [Halton Sampler](https://www.pbr-book.org/4ed/Sampling_and_Reconstruction/Halton_Sampler) 。

<div align="center">  
<img src="https://s2.loli.net/2025/07/06/Fr3pSCQuLM2fJWl.jpg" width = "50%" height = "50%" alt="图2 - Halton Squence"/>
</div>

对采样点进行偏移的方式通常是修改相机的投影矩阵，只需修改矩阵中的两个变量即可：  

    ProjectionMatrix[0][2] += ( OffsetX * 2.0f – 1.0f ) / FrameBufferSize.Width;
    ProjectionMatrix[1][2] += ( OffsetY * 2.0f – 1.0f ) / FrameBufferSize.Height;

至于为什么要对 offset 乘 2 减 1 的原因是，Halton 序列即 offset 的范围是在 (0, 1)，我们希望采样点偏移的范围是在一个像素内，即在 (-0.5, 0.5) 之间，需要对 Halton 序列减去 0.5。又因为齐次除法后得到的 NDC 坐标的 x、y 分量都在 \[-1, 1\] 之间，而得到 uv 值在 (0, 1) 之间，故需要乘以 2 消除缩放影响。具体推导如下，假设 jitter 在 (-0.5, 0.5) 之间：  

$$ P'_{clip} = M_{persp}P_{view} = \begin{bmatrix} A & 0 & 2 \times jitter.x / width & 0 \\ 0 & B & 2 \times jitter.y / height & 0 \\ 0 & 0 & C & D \\ 0 & 0 & 1\,or\, -1 & 0 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix} = \begin{bmatrix} Ax + 2 \times jitter.x / width \times z \\ By + 2 \times jitter.y / height \times z \\ ... \\ ... \end{bmatrix} $$

$$ P'_{NDC} = \left[ \cfrac {A}{z}x + \cfrac {2 \times jitter.x}{width} , \cfrac {B}{z}y + \cfrac {2 \times jitter.y}{height} , ..., ... \right] $$

$$ P'_{ScreenUV} = \left[ \cfrac {A}{2z}x + \cfrac {jitter.x}{width} + 0.5 , \cfrac {B}{2z}y + \cfrac {jitter.y}{height} + 0.5 \right] $$

要注意，原 ScreenUV 为 $\, \left[ \cfrac {A}{2z}x + 0.5, \cfrac {B}{2z}y + 0.5 \right] \,$，故偏移了 $\, \left[ \cfrac {jitter.x}{width}, \cfrac {jitter.y}{height} \right] \,$，符合我们的要求。注意上面推导的是透视投影的情况，正交投影则需改变第一行第四位，以及第二行第四位，即 \[0\]\[3\] 和 \[1\]\[3\] ：

$$ P'_{clip} = M_{ortho}P_{view} = \begin{bmatrix} A & 0 & 0 & 2 \times jitter.x / width \\ 0 & B & 0 & 2 \times jitter.y / height \\ 0 & 0 & C & D \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix} = \begin{bmatrix} Ax + 2 \times jitter.x / width \\ By + 2 \times jitter.y / height \\ ... \\ ... \end{bmatrix} $$

$$ P'_{ScreenUV} = \left[ \cfrac {A}{2}x + \cfrac {jitter.x}{width} + 0.5 , \cfrac {B}{2}y + \cfrac {jitter.y}{height} + 0.5 \right] $$

为了方便控制偏移距离，可以给 jitter 乘以一个 jitterScale 参数，用于控制偏移的范围。得到修改的矩阵后，只需调用 `CommandBuffer.SetViewProjectionMatrices()` 即可实现抖动了，我用了一张金属度比较高的图片，方便观察闪烁现象：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/08/Hl3VvTNDFPCQh7n.gif" width = "512" height = "512" alt="图3 - Jitter"/>
</div>

## Exponential Blending
接下来就是将当前帧与历史帧进行混合了，直接混合所有历史帧肯定是不现实的，因为我们没法存储所有历史数据。绝大多数 TAA 的实现采用了类似递归的方式，将所有历史帧的累加结果存储到一张贴图当作，即 TAA History/Accumulation Texture，并采用了以下公式进行混合：  

$$ f_n(p) = \alpha \cdot s_n(p) + (1 - \alpha) \cdot f_{n-1} (\pi(p)) $$

其中 $\,f_n(p)\,$ 是第 n 帧的输出颜色，$\,\alpha\,$ 是混合系数，$\,s_n(p)\,$ 是当前帧颜色，$\,f_{n-1} (\pi(p))\,$ 是经过重投影后的累计历史帧。重投影后面再考虑，这里先考虑静态场景。在这个公式下，历史帧会被不断累计，当然随着时间的流逝，单一历史帧的影响会被无限缩小。

<div align="center">  
<img src="https://s2.loli.net/2025/07/08/ANXvRKOwUabtjDl.jpg" width = "45%" height = "45%" alt="图4 - 单一历史帧随着帧数增加所占的比例变化"/>
</div>

越老的历史帧所占的比例越来越小，在大部分情况下是很好的选择，因为场景肯定会变化，大概率老的历史帧的颜色已经不在屏幕上了，但是从最小化方差的角度来看，上述选择只能算次优解。下面的表格揭示了不同帧数下，不同 $\,\alpha\,$ 对应的有效累计样本数：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/08/YNvzT4HPkb29eK6.jpg" width = "45%" height = "45%" alt="图5 - 在 α = 0.1 的情况下，经过 5 帧相当于 1 个像素采样了 2 个样本；经过 10 帧，相当于 5 个样本；经过 15 帧，相当于 10 个样本；经过无限帧，相当于 19 个有效样本"/>
</div>

$\,\alpha\,$ 的值，通常的选择是 0.1。可以看到对于无限帧的情况，相当于 19x SSAA，效果还是相当不错的。在 Unity RenderGraph 创建持久化的 RT，即 TAA History，以及临时资源 TAA Target 并绘制的 C# 代码，这里就不展示了，就说一下大致流程，创建好了 TAA History 和 TAA Target 后，将 Color Attachment 作为当前帧的输入纹理，将 TAA History 作为历史帧的输入纹理，使用 Shader 或 Compute Shader 绘制出 TAA Target 后，作为 post processing 的输入纹理。与此同时，将 TAA Target 复制给 TAA History 以便下一帧使用。TAA Shader 目前的混合代码如下（我的 blend factor 是乘在 history 上的，故是 0.9）：  

    TEXTURE2D(_TAAHistory);
    float4 _TAAParams; // x: history blend factor

    float4 TAAFrag(Varyings IN) : SV_TARGET
    {
        float3 history = LOAD_TEXTURE2D_LOD(_TAAHistory, IN.positionHCS.xy, 0).xyz;
        float3 current = LOAD_TEXTURE2D_LOD(_BlitTexture, IN.positionHCS.xy, 0).xyz;
        float3 color = lerp(current, history, _TAAParams.x);
        return float4(color, 1.0);
    }

混合效果如下，为了方便观察高光闪烁问题，我又在机器人旁边加了盏红灯：  

<table><tr>
<td><img src='https://s2.loli.net/2025/07/08/JTbiFsuX9qG4Eeh.gif' width="512" alt="图6 - TAA (After Simple Exponential Blending)"></td>
<td><img src='https://s2.loli.net/2025/07/08/BonNJtUQaAZOPx9.gif' width="512" alt="图7 - NoAA"></td>
</tr></table>

可以很明显地感受到抗锯齿的效果，但也能明显地感受到闪烁问题。还有一点是，上图中可能会感觉到在 TAA 下会损失一些贴图细节，这是因为上图分辨率较小，只有 512 × 512，分辨率越高，这些现象越能得到缓解，对于现在普遍的 2k 与 4k 屏幕，这个问题不明显。

图 6 中的闪烁问题我们暂时先放着，后面再详细讨论，我们先来看看若将摄像机进行移动，图 6 会变为什么样子：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/09/AktR52mKGpsdqIQ.gif" width = "512" height = "512" alt="图8 - 鬼影 Ghost 现象"/>
</div>

上图只移动了摄像机，没移动物体，可以看到传说中的鬼影问题了。这下就集齐了 TAA 的两大问题：闪烁和鬼影。闪烁问题无论静态动态都存在，动态情况下会加剧闪烁问题，而鬼影只在动态场景存在，下面我们先来处理鬼影现象。

## 重投影
鬼影现象的产生原因很简单，相机或物体的移动导致了颜色信息的位置发生了变化，而我们还在采样原来的位置。所以我们很容易可以想到，通过计算屏幕像素每帧的坐标变化，即计算**运动向量 Motion Vector**，来找到像素移动前的屏幕坐标进行采样就可以解决鬼影问题。之前有提到过 Motion Vector 分为 **Camera Motion Vector** 和 **Object Motion Vector**，因为 Object Motion Vector 的实现在工程上（Unity 有一些历史遗留问题）具有一定的困难，我们先来解决只有摄像机移动的动态情形，即 Camera Motion Vector 的情形，至于物体的移动，等闪烁和鬼影解决得差不多了之后会专门讲解。

计算 Camera Motion Vector 的方法也不难，步骤如下：  
①通过 Camera Depth Texture 获取当前像素点的深度，通过深度还原出像素点的 clip space 坐标；  
②将 clip space 通过 view-projection 的逆矩阵，反向投影至世界坐标；  
③使用上一帧的 view-projection 矩阵，投影至上一帧的屏幕 uv 坐标，与当前帧的屏幕 uv 坐标相减得到 Camera Motion Vector。  
（注意上述方法只能计算 Camera Motion Vector，Object Motion Vector 还涉及到 MVP 矩阵的 M 的变化。）

Camera Motion Vector 可以直接在 TAA 的 Shader 里计算，也可以存储在一张 RT 里并在 TAA 里采样，我这里是开了一张 RT 存储的。如果后面没有使用 Camera Motion Vector 的后处理效果，比如动态模糊，并且 TAA 只打算使用 Camera Motion Vector，不使用 Object Motion Vector，那么就没必要多开张 RT。

这里顺便提一下 Unity 工程上的问题，以下的 Shader 参数，Unity 的 Built-in Engine 不会自动上传：  

    float4x4 unity_MatrixInvP;
    float4x4 unity_MatrixInvVP;
    float4x4 _PrevViewProjMatrix; // non-jittered.
    float4x4 _NonJitteredViewProjMatrix; // non-jittered.

因为我看 URP 将上述参数放在了 UnityInput.hlsl 里面，我还以为 Unity 的 Built-in Engine 会自动上传，但是使用这些参数，会发现它们都是单位矩阵。所以 UnityInput.hlsl 里的参数，哪些会被 Built-in Engine 自动上传，哪些不会，还得自己测试一下。最后上述这些参数，还得我们自己上传，首先找个地方保留住这一帧和上一帧的各种矩阵，然后根据它们计算相关参数，代码大致如下：  

``` C#
bool isProjectionMatrixFlipped = SystemInfo.graphicsUVStartsAtTop;

Matrix4x4 viewMatrix = yCamera.perCameraData.viewMatrix;
Matrix4x4 inverseViewMatrix = viewMatrix.inverse;
Matrix4x4 gpuProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.jitteredProjectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 inverseProjectionMatrix = gpuProjectionMatrix.inverse;
Matrix4x4 gpuNonJitterProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.projectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 nonJitterInverseProjectionMatrix = gpuNonJitterProjectionMatrix.inverse;

Matrix4x4 inverseViewProjectionMatrix = inverseViewMatrix * inverseProjectionMatrix;
Matrix4x4 nonJitterViewProjectionMatrix = gpuNonJitterProjectionMatrix * viewMatrix;
Matrix4x4 nonJitterInverseViewProjectionMatrix = inverseViewMatrix * nonJitterInverseProjectionMatrix;

Matrix4x4 previousViewMatrix = yCamera.perCameraData.previousViewMatrix;
Matrix4x4 previousInverseViewMatrix = previousViewMatrix.inverse;
Matrix4x4 previousGPUProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.previousJitteredProjectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 previousInverseProjectionMatrix = previousGPUProjectionMatrix.inverse;
Matrix4x4 previousGPUNonJitterProjectionMatrix = GL.GetGPUProjectionMatrix(yCamera.perCameraData.previousProjectionMatrix, isProjectionMatrixFlipped);
Matrix4x4 previousNonJitterInverseProjectionMatrix = previousGPUNonJitterProjectionMatrix.inverse;

Matrix4x4 previousViewProjectionMatrix = previousGPUProjectionMatrix * previousViewMatrix;
Matrix4x4 previousInverseViewProjectionMatrix = previousInverseViewMatrix * previousInverseProjectionMatrix;
Matrix4x4 previousNonJitterViewProjectionMatrix = previousGPUNonJitterProjectionMatrix * previousViewMatrix;
Matrix4x4 previousNonJitterInverseViewProjectionMatrix = previousInverseViewMatrix * previousNonJitterInverseProjectionMatrix;

cmd.SetGlobalMatrix(YPipelineShaderIDs.k_InverseProjectionMatrixID, inverseProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_InverseViewProjectionMatrixID, inverseViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredViewProjectionMatrixID, nonJitterViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredInverseViewProjectionMatrixID, nonJitterInverseViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_PreviousViewProjectionMatrixID, previousViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_PreviousInverseViewProjectionMatrixID, previousInverseViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredPreviousViewProjectionMatrixID, previousNonJitterViewProjectionMatrix);
cmd.SetGlobalMatrix(YPipelineShaderIDs.k_NonJitteredPreviousInverseViewProjectionMatrixID, previousNonJitterInverseViewProjectionMatrix);
```

上面代码中要注意一下的是 `GL.GetGPUProjectionMatrix` 这个 API，从 `camera.projectionMatrix` 获取到投影矩阵是 OpenGL 习惯下的矩阵，我们需要根据不同平台转换成不同的习惯下的投影矩阵，所幸 `GL.GetGPUProjectionMatrix` 可以帮我们完成这件事情。另外一点要注意的是矩阵的乘法顺序，就不再赘述了。拿到矩阵后，就可以计算 Camera Motion Vector 了，Shader 代码如下：  

> 在 DirectX 平台下，只要将 `GL.GetGPUProjectionMatrix()` 设置为 true，Unity 会将 Projection Matrix 的 y 轴翻转，这样子经过视口变换（uv 的 v 再次翻转），就统一了 OpenGL 下和 DirectX 下的 uv 了（即原点在左下角），这也是普通的 Unity Shader 中我们不用关心 uv 原点的位置的原因。但是直接绘制 RT 的 Shader 就不同了，因为此时的 uv 和 positionHCS 是我们生成的，而不是通过 Projection Matrix 计算而得，所以之前在顶点着色器中，将 uv 的 v 轴手动翻转了，positionHCS 无需手动翻转是因为视口变换会翻转。所以下面计算 Camera Motion Vector 要特别注意 uv 的方向。

    float4 GetNDCFromUVAndDepth(float2 uv, float depth)
    {
        #if UNITY_UV_STARTS_AT_TOP
            uv.y = 1.0f - uv.y;
        #else
            depth = 2.0 * depth - 1.0;
        #endif
        
        return float4(2.0 * uv - 1.0, depth, 1.0);
    }

    float3 TransformNDCToWorld(float4 NDC, float4x4 invViewProjMatrix)
    {
        float4 positionHWS = mul(invViewProjMatrix, NDC);
        return positionHWS.xyz / positionHWS.w;
    }

    float4 CameraMotionVectorFrag(Varyings IN) : SV_TARGET
    {
        float depth = LOAD_TEXTURE2D_LOD(_CameraDepthTexture, IN.positionHCS.xy, 0).r;
        float4 NDC = GetNDCFromUVAndDepth(IN.uv, depth);
        float3 currentPositionWS = TransformNDCToWorld(NDC, UNITY_MATRIX_I_VP);

        float4 currentPositionCS = mul(UNITY_MATRIX_NONJITTERED_VP, float4(currentPositionWS.xyz, 1.0));
        float4 previousPositionCS = mul(UNITY_PREV_MATRIX_NONJITTERED_VP, float4(currentPositionWS.xyz, 1.0));
        
        float2 currentPositionNDC = currentPositionCS.xy * rcp(currentPositionCS.w);
        float2 previousPositionNDC = previousPositionCS.xy * rcp(previousPositionCS.w);
        
        float2 velocity = currentPositionNDC - previousPositionNDC;
        
        #if UNITY_UV_STARTS_AT_TOP
        velocity.y = -velocity.y;
        #endif
        
        velocity *= 0.5;

        return float4(velocity, 0,0);
    }

另外要注意的是，计算 Camera Motion Vector 的时候要去除掉 jitter 的影响，否则得到的 Motion Vector 是不对的，使用了会导致画面糊，所以上面计算时，使用了 `NONJITTERED_VP` 矩阵。然后在 TAA Shader 采样时，减去 Motion Vector：  

    float4 TAAFrag(Varyings IN) : SV_TARGET
    {
        float2 velocity = LOAD_TEXTURE2D_LOD(_CameraMotionVectorTexture, IN.positionHCS.xy, 0).rg;
        
        float3 history = LOAD_TEXTURE2D_LOD(_TAAHistory, IN.positionHCS.xy - _CameraBufferSize.zw * velocity, 0).xyz;
        float3 current = LOAD_TEXTURE2D_LOD(_BlitTexture, IN.positionHCS.xy, 0).xyz;

        float3 color = lerp(current, history, _TAAParams.x);
        return float4(color, 1.0);
    }

此时效果如下：  

<div align="center">  
<img src="https://s2.loli.net/2025/07/09/KYZUX37coIqTjz6.gif" width = "512" height = "512" alt="图9 - After Camera Motion Vector"/>
</div>

可以看到，虽然画面比之前看起来更清晰了，但是还是有鬼影现象，这是由于场景中的遮挡关系发生了变化。比如上图中，在这一帧中，机器人的位置可以拿到 Motion Vector 得到上一帧中机器人的颜色进行混合，但是机器人右上位置的像素，在上一帧中被机器人遮挡，在这一帧中没被机器人遮挡，同时 Motion Vector 也为 0，那么这些像素就会混合到上一帧中机器人的颜色，从而导致鬼影。所以 Camera Motion Vector 只能消除一部分的鬼影。当然鬼影现象还会因为其他因素引起，比如灯光的变化等等，这就需要我们去验证历史数据，拒绝不能使用的历史数据。

# 验证历史样本
## Color Test
### Color Clamping
### Color Clipping

# 其他优化
## 闪烁优化

### Luma Weighted Exponential Blending
这里顺便讨论一下 TAA 在 Pipeline 的位置问题，上面将 TAA 放置在了所有后处理之前，即是在 HDR 线性空间下进行的。在这样的情况下，物体或相机移动，会导致物体的高光计算的剧烈变化，从而导致高光的闪烁问题。可能有人会说为什么不把 TAA 放在 Tone Mapping 之后，这样子就能解决闪烁问题。因为 bloom 或者 lens flare 效果可能会增大因高亮度颜色产生的 alias 问题，TAA 放在 Tone Mapping 之后的话 bloom 闪烁问题也会比较严重。还有一点是在 Tone Mapping 之前，相对来说更物理正确，因为 Tone Mapping 之前是线性空间，之后是非线性空间。但是的确 TAA 在 LDR 下的效果会比 HDR 下要好，这就产生了一个妥协的办法，即类似于 Bloom 中的解决方案，使用 Karis Average 来混合当前帧与历史帧从而缓解这个运动状态下的 bloom 闪烁现象，如下：

$$ w(c) = \cfrac {1} {1 + Luminance(c)} $$

代码如下：  

    float3 LumaExponentialAccumulation(float3 history, float3 current, float blendFactor)
    {
        float historyLuma = Luminance(history);
        float currentLuma = Luminance(current);
        float historyLumaWeight = rcp(historyLuma + 1.0);
        float currentLumaWeight = rcp(currentLuma + 1.0);
        float weightSum = lerp(currentLumaWeight, historyLumaWeight, blendFactor);
        float3 blendColor = lerp(current * currentLumaWeight, history * historyLumaWeight, blendFactor);
        return blendColor / weightSum;
    }

上述方法在静态场景中几乎看不出差别，就是能感觉到高光变暗了，并不能解决图 6 中的闪烁问题。但是在运动场景中，确实能减少一部分的高光闪烁问题，至于需不需要使用看项目需求吧。

# Object Motion Vector

