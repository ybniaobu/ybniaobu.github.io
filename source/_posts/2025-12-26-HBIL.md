---
title: Horizon-Based Indirect Lighting (HBIL)
date: 2025-12-26 15:44:00
categories: 
  - [图形学]
  - [unity, pipeline]
tags:
  - 图形学
  - 游戏开发
  - unity
top_img: /images/black.jpg
cover: https://s2.loli.net/2025/12/26/YpyBQSsUquzxeLP.gif
mathjax: true
description: 本文章主要内容为 HBIL (一种屏幕空间全局光照算法) 的基本原理与公式推导，以及具体实现和效果展示。
---

> 屏幕空间全局光照技术，诸如 SSDO、HBIL、SSGI 等，最好是作为低频全局光照技术的高频补充，从而做到双方优势互补，单独使用屏幕空间全局光照的结果只能说是差强人意，光照效果会随着场景的移动发生较大变化，感受下来较为突兀。我大致实现了一下使用上一帧场景颜色 (Radiance) 从而实现无限弹射的 SSDO，效果其实还是不错的，但因为 Horizon Based 的算法的物理正确性肯定要比基于 SSAO 的 SSDO 要高很多，所以 SSDO 算法我就不在 YPipeline 里支持了，打算好好实现 HBIL。至于 SSGI，它是通过步进的方式采样深度贴图进行求交，并通过 Hierarchical Z-Buffer 进行加速，我打算之后有空了再实现。  

**Horizon-Based Indirect Lighting (HBIL)** 简单来说是一个受到 HBAO 技术启发的屏幕空间全局光照技术，通过补充 near-field 光照显著增加了实时渲染质量。该算法出自这篇文章：Benoît Mayaux. 2018. Horizon Based Indirect Lighting (HBIL): https://github.com/Patapom/GodComplex/tree/master/Tests/TestHBIL ，下面的内容都是基于这篇文章书写的。之前在 [SSAO 的文章](https://ybniaobu.github.io/2025/08/01/2025-08-01-SSAO1/) 开头提到的这个库 https://github.com/cdrinmatane/SSRT3 的实现，本质上也是基于 HBIL 的。

# HBIL 基本原理
首先说明，文章中使用的球坐标是右手坐标系，且天顶角 $\,\theta\,$ 是基于 z 轴的，方位角 $\,\varphi\,$ 则是在 xy 平面基于 x 轴的角度。根据经典反射方程，在像素点 $\,x\,$ 的出射 radiance 为：  

$$ \begin{equation*} L_o(x, \omega_o) = \int_{\Omega^+}L_i(x,\omega_i)\rho(x, \omega_i, \omega_o)(n \cdot \omega_i) d\omega_i \end{equation*} $$

因为处理的是漫反射全局光照，可以将 Lambertian 项拆分出来：  

$$ \begin{equation*} L_o(x, \omega_o) = \cfrac {\rho(x)}{\pi} \int_{\Omega^+}L_i(x,\omega_i)(n \cdot \omega_i) d\omega_i = \cfrac {\rho(x)}{\pi} E(x, n) \end{equation*} $$

其中 $\,E(x, n)\,$ 是在像素点 $\,x\,$ 收集到的 irradiance。同时它又可以被分为 3 个部分：  

$$ E(x, n) = E_{direct}(x, n) + E_{far}(x, n) + E_{near}(x, n) $$

<div align="center">  
<img src="https://s2.loli.net/2025/12/29/QkLxHCtjvAWhMRS.png" width = "40%" height = "40%" alt="图1 - The 3 possible sources of irradiance: Direct Lighting, Far-Field Lighting, Near-Field Lighting"/>
</div>

直接光部分没什么好说的，HBIL 的本质就是补充 near-field 的间接光，而 far-field 后面会专门提到，往往通过采样 Cube Map 或球谐获取。$\, E_{near}(x, n) \,$ 可以被写为：  

$$ E_{near}(x, n) = \int_{\Omega^+} (1 - V(x, \omega_i)) L_i(x,\omega_i) (n \cdot \omega_i) d\omega_i $$

注意这里的可见性函数 $\,V(x, \omega_i)\,$ 是被遮挡返回 0，未遮挡返回 1。而 $\, E_{far}(x, n) \,$ 可以被写为：  

$$ E_{far}(x, n) = \int_{\Omega^+} V(x, \omega_i) L_i(x,\omega_i) (n \cdot \omega_i) d\omega_i $$

可以看到，near-field 项正好是 far-field 项的补充。

## 递归无限反射
当渲染第 N 帧画面时，我们就已经得到了第 N - 1 帧的 radiance 值，此时我们可以将反射方程改写为：  

$$ L_o^N(x, \omega_o) = \cfrac {\rho(x)}{\pi} \left[ E_{direct}^N(x, n) + E_{far}^N(x, n) + \int_{\Omega^+} L_d^{N - 1}(x') (n \cdot \omega_i) d\omega_i \right] $$

其中 $\,L_o^N(x, \omega_o)\,$ 就是第 N 帧的 radiance，会在第 N + 1 帧被再次作为输入值。$\,L_d^{N - 1}(x')\,$ 是在周围像素点 $\,x'\,$ 的第 N - 1 帧的 radiance 值。这样子计算 radiance 从理论上来说让我们得到一个无限光线弹射的效果。

## Near-Field 间接光
### 积分求解
跟 GTAO 一样，我们先来关注内积分，即每个 slice，将 slice 画出来如下图（其实这张图，我觉得是画错了的，因为文章开头就说 $\,\theta\,$ 是基于 z 轴的角度，而这里不是。虽然画错了，但不影响我们解积分）：  

<div align="center">  
<img src="https://s2.loli.net/2025/12/29/eMBWplcVR8mKw2q.png" width = "40%" height = "40%" alt="图2 - Sampling the gathered irradiance when the horizon is rising from θ0 到 θ1."/>
</div>

其中 $\,\theta_0\,$ 到 $\,\theta_1\,$ 部分的积分可以写为（由于上图画错，我们需将 $\,\theta\,$ 看作是基于 z 轴的角度，假设 $\,\theta_1\,$ 的这条虚线是 $\,\theta_0\,$， $\,\theta_0\,$ 的这条虚线是 $\,\theta_1\,$，*一定要注意顺序问题*，我们后面代码中步进得到的更小的角，即 cos 值更大的，应该写入 $\,\theta_0\,$）：  

$$ \Delta E = \int_{\theta_0}^{\theta_1} L_d(x') (n' \cdot w_i') sin \theta d\theta $$

其中 $\,n'\,$ 是投影到 slice 平面的法线，这点 GTAO 中也有提到过。文章中积分求解的方法跟 GTAO 有较大不同，其中 $\,n' \cdot w_i'\,$ 的化简是在 Slice 这个 2D 空间中化简的，如下图：  

<div align="center">  
<img src="https://s2.loli.net/2026/01/05/4QVsOIGijmhS3zk.png" width = "30%" height = "30%" alt="图3 - Slice Space"/>
</div>

每个 slice 的切线方向 $\,\widehat D(\varphi)\,$ 就是我们随机出来的方向（将半球切分）。对于 slice space 这个 2D 空间来说，$\,\widehat D(\varphi)\,$ 可以被定义为 $\,(1, 0)\,$，视角方向 $\,\widehat \omega_o\,$ 可以被定义为 $\,(0, 1)\,$，采样点 $\,x'\,$ 则是 slice space 原点。那么此时法线 $\,n\,$ 在 slice 的投影 $\,n'\,$ 的 slice space 坐标可以写为：  

$$ n' = \begin{cases} \widehat n \cdot \widehat D(\varphi) \\ \widehat n \cdot \widehat \omega_o \end{cases} = \begin{cases} n_x \\ n_y \end{cases} $$

其中 $\,\widehat n\,$、$\,\widehat D(\varphi)\,$、$\,\widehat \omega_o\,$ 都是世界空间（或相机空间）下的坐标，点乘计算出 cos 值，即是法线投影在 slice 上的 $\,n'\,$ 的 slice space 坐标，写作 $\,(n_x, n_y)\,$。与此同时，$\,w_i'\,$ 的 slice space 坐标可以写为 $\,\left(sin\theta, cos\theta\right)\,$，于是 $\,\Delta E\,$ 积分可以化简为：  

$$ \Delta E = L_d(x') \int_{\theta_0}^{\theta_1} (n_xsin \theta + n_ycos \theta)sin\theta d\theta $$

又因为 $\,sin^2 \theta\,$、$\,sin \theta cos \theta\,$ 的积分分别为：  

$$ \int sin^2 \theta d\theta = \cfrac {1}{2}\theta - \cfrac {1}{2} sin \theta cos \theta + C $$
$$ \int sin \theta cos \theta d\theta = \cfrac {1}{2} sin^2 \theta + C = -\cfrac {1}{2} cos^2 \theta + C $$

最终 $\,\Delta E\,$ 为：  

$$ \Delta E = L_d(x') [\cfrac {n_x} {2}(\theta_1 - \theta_0 + sin\theta_0 cos\theta_0 - sin\theta_1 cos\theta_1) + \cfrac {n_y} {2}(sin^2 \theta_1 - sin^2 \theta_0)] $$

### 计算 Horizon Angle
之前 GTAO 的 Horizon Angle 计算的想法是比较简单的，就是将步进的采样点减去中心采样点，得到的向量 h 和视角方向 v 点乘得到 cos 值，再反三角函数。因为向量 h 和视角方向 v 需要在同一空间下做点乘，所以需要一些矩阵运算，而 HBIL 文章中的方法进一步进行了简化，可以减少矩阵运算：  

<div align="center">  
<img src="https://s2.loli.net/2026/01/05/nUM5xcgrjIyz963.png" width = "25%" height = "25%" alt="图4 - Computing Horizon Angles"/>
</div>

上图中，$\,\widehat Z\,$ 和 $\,\widehat X\,$ 是相机空间（文章中叫做全局相机空间 Global Camera Space）的两个轴，$\,\widehat Z\,$ 指向屏幕内部，$\,\widehat x\,$ 就是中心采样点（步进的出发点），$\,\widehat x_1\,$ 为沿着 slice 步进的新的采样点，步进距离为 $\,r\,$，而 $\,z\,$ 就是采样得到的深度值。我们要计算的 Horizon Angle 是 $\,\theta\,$，这里要注意视角方向 $\,\widehat \omega_o\,$ 和相机空间的 $\,\widehat Z\,$ 轴只有当 $\,\widehat x\,$ 在屏幕中心时才是平行的（$\,\widehat \omega_o\,$ 和 $\,\widehat \omega_x\,$ 所在的空间，文章中称为局部相机空间 Local Camera Space）。我们可以观察到 $\,\theta = \beta - \alpha\,$，于是：  

$$ cos(\theta) = cos(\beta - \alpha) = cos(\alpha) cos(\beta) + sin(\alpha) sin(\beta) $$

同时：  

$$ cos(\beta) = \cfrac {z} {\sqrt{r^2 + z^2}} $$
$$ sin(\beta) = \cfrac {r} {\sqrt{r^2 + z^2}} $$

最终 Horizon Angle 公式为：  

$$ cos(\theta) = \cfrac {cos(\alpha)z + sin(\alpha)r} {\sqrt{r^2 + z^2}} $$

> 我感觉这里有点不太对，因为 $\,\omega_o\,$ 可能不在 rz 所在的平面上，我后面代码还是按 GTAO 里计算 Horizon Angle 的方式来写的。

### Normal 钳制
之前 GTAO 就提到过计算出来的 Horizon Angle 有可能不在基于 normal 的半球之内，需要进行钳制，GTAO 的方法是计算出法线与视角方向的角度 n，然后将 Horizon Angle 与 n 的差值钳制在 $\,\pi/2\,$ 以内。HBIL 文章中的方法我感觉更巧妙一点：  

<div align="center">  
<img src="https://s2.loli.net/2026/01/05/W7OCh1fHToYm5Ab.png" width = "40%" height = "40%" alt="图5 - Computing Horizon Angles"/>
</div>

图中 $\,t\,$ 是平行于 $\,\widehat \omega_o\,$ 的一个距离，那么：  

$$ cos \theta_{front} = \cfrac {t} {\sqrt{1 + t^2}} $$
$$ cos \theta_{back} = -\cfrac {t} {\sqrt{1 + t^2}} $$

而 $\,t\,$ 的计算的逻辑如下：  

$$ \widehat n \cdot (\widehat D(\varphi) + t \widehat \omega_o) = 0 \Rightarrow t = - \cfrac {\widehat n \cdot \widehat D(\varphi)}{\widehat n \cdot \omega_o} $$

由此可以计算出 Horizon Angle 的两个初始值。

## Far-Field 间接光
### 积分求解
上面写到过 far-field 的 irradiance 计算公式如下：  

$$ E_{far}(x, n) = \int_{\Omega^+} V(x, \omega_i) L_i(x,\omega_i) (n \cdot \omega_i) d\omega_i $$

这个公式非常眼熟，跟[讲 SSAO 的文章](https://ybniaobu.github.io/2025/08/01/2025-08-01-SSAO1/)中的介绍 Ambient Occlusion 在着色中的运用的公式是一模一样的，由此可以把它简化为：  

$$ E_{far}(x, n) = E_o(x, n) AO(x) $$

其中 $\,E_o(x, n)\,$ 是未被遮挡的 irradiance，可以从 cube map 或者 SH 采样获取。那么 AO 的计算，其实跟 Near-Field 的逻辑是几乎一样的，如下：  

<div align="center">  
<img src="https://s2.loli.net/2026/01/05/o53M7IFDG9rSRqs.png" width = "35%" height = "35%" alt="图6 - AO integral illustration"/>
</div>

注意一下上图的 $\,\theta_0\,$、$\,\theta_1\,$，别跟 Near-Field 的积分的这两个角度搞混了，这里的 $\,\theta_0\,$ 是在 $\,[-\pi, 0]\,$ 之间，$\,\theta_1\,$ 在 $\,[0, \pi]\,$ 之间。根据上图，$\,AO(x)\,$ 可以写为：  

$$ AO(x) \approx \cfrac {1}{N} \sum^N \int_{\theta_0}^{\theta_1} (n \cdot \omega_i(\theta)) |sin \theta| d\theta $$

跟 Near-Field 积分的处理类似，内积分可以写为：  

$$ AO_i = \int_{\theta_0}^{\theta_1} (n' \cdot \omega'_i(\theta)) |sin \theta| d\theta $$

$\,n' \cdot w_i'\,$ 的化简还是在 slice space 这个 2D 空间中进行。只是这里需要将积分拆分为两个部分（跟 GTAO 一样）：  

$$ AO_i = \int_{\theta_0}^{0} (n' \cdot \omega'_i(\theta)) (-sin \theta) d\theta + \int_{0}^{\theta_1} (n' \cdot \omega'_i(\theta)) sin \theta d\theta $$

于是内积分最终为：  

$$ AO_i = \cfrac {n_x} {2}(\theta_1 + \theta_0 - sin\theta_0 cos\theta_0 - sin\theta_1 cos\theta_1) + \cfrac {n_y} {2}(sin^2 \theta_1 + sin^2 \theta_0) $$

再次强调上述公式中的 $\,\theta_0\,$ 是负数，别搞错了！！！！！！！！！！！！

> ①我算出来的 AO 和 Bent Normal 公式跟 HBIL 论文中的公式有个负号上的小差异，上面写的是我算出来的公式。我不知道是不是论文里算错了，但我自己实践下来，从效果上来说，我计算出来的这个版本应该更合理一些。  
>   
> ②HBIL 文章中还提到了 $\, E_{far}(x, n) = E_o(x, n) AO(x) \,$ 这样的简化处理，会导致能量上的损失，可以通过计算 bent cone 的立体角来弥补这部分能量损失。我觉得这么做的必要性不大，所以我后面就不打算这么处理了。这里就记录一下公式，bent cone 的 aperture angle $\,\alpha\,$ 的计算在之前 GTAO 文章的 GTSO 章节有提到过，对于 Uniform Weighting：  
>   
> $$ \alpha = cos^{-1} (1 - AO(x)) $$
> 
> 对于 Cosine Weighting： 
> 
> $$ \alpha = cos^{-1} (\sqrt{1 - AO(x)}) $$
> 
> $\, E_{far}(x, n) \,$ 可以通过应用一个因子 $\,\mathcal{F}_0(\alpha)\,$ 来弥补能量损失：  
> 
> $$ E_{far}(x, n) = E_o(x, n) \mathcal{F}_0(AO(x)) $$
> $$ \mathcal{F}_0(\alpha) = \alpha \cdot \left( 1 + \cfrac {(1 - \alpha)^{0.75}}{2} \right) = \alpha \cdot \left( 1 + \cfrac {1 - \alpha} {2 \sqrt[4] {1 - \alpha}} \right)$$
>   
> 上述公式具体怎么来的，详见 HBIL 文章作者的另外一篇文章 [Improved Ambient Occlusion](https://drive.google.com/file/d/1SyagcEVplIm2KkRD3WQYSO9O0Iyi1hfy/edit) 。

### Bent Normal
HBIL 文章还提到了 far-field irradiance 可以通过 Bent Normal 采样以获取更好的效果，Bent Normal 跟上面 AO 的计算一样，是 Near-Field 积分计算过程中的一个副产品。还是根据图 6，Bent Normal $\,\widetilde{n}\,$ 可以写为：  

$$ \widetilde{n} \approx \cfrac {\pi}{N} \sum^N \int_{\theta_0}^{\theta_1} \omega_i(\theta) |sin \theta| d \theta $$

公式中的 $\,\pi\,$ 影响的是向量长度，不用关心。还是老样子，在 slice space 2D 空间中处理，那么：  

$$ \omega_i'(\theta) = \begin{cases} sin\theta \\ cos\theta \end{cases} $$

x、y 分量分开处理，$\,\widetilde{n}\,$ 在 slice space 2D 空间中的坐标可以写为：  

$$ \widetilde{n} = \begin{cases} \widetilde{n}_x \\ \widetilde{n}_y \end{cases} $$
$$ \widetilde{n}_x = \cfrac {1} {2}(\theta_1 + \theta_0 - sin\theta_0 cos\theta_0 - sin\theta_1 cos\theta_1) $$
$$ \widetilde{n}_y = \cfrac {1} {2}(sin^2 \theta_1 + sin^2 \theta_0) $$

再再次提醒，公式中的 $\,\theta_0\,$ 是负数，别搞错了！！！！！！！！！！！！然后我们需要其转换到相机空间：  

$$ \widetilde{n} = \cfrac {\sum_i^S D(\varphi_i) \cdot \widetilde{n}_x(i) + \omega_o \cdot \widetilde{n}_y(i) } { || \sum_i^S D(\varphi_i) \cdot \widetilde{n}_x(i) + \omega_o \cdot \widetilde{n}_y(i) || } $$

其中 $\,D(\varphi_i)\,$ 是 Slice $\,i\,$ 的切线方向。由此就得到了 Bent Normal，用 Bent Normal 采样 far-field irradiance 可以得到比 Normal 采样更柔和的效果。

# 具体实现
因为代码结构跟 HBAO/GTAO 基本一致，很多细节我就不再赘述了，下面我主要阐述一些需要注意的地方和不一样的地方，整体代码我放在文章最后面。

## 影响范围
虽然算法跟 HBAO/GTAO 极为相似，但是实现上有个比较重要的抉择点需要考虑，就是是否跟 HBAO/GTAO 一样使用 Radius 限制影响范围，还是尽可能更多地使用屏幕空间的信息。HBIL 作者的原意应该是使用 Radius 限制影响范围，Radius 范围内的即 Near-Field Lighting，范围外的 Far-Field Lighting 由其他 Fallback 方案补充，但是我实践了一下，觉得这样做的优缺点都是比较明显的，优点就是可以在输出 Irradiance 的同时输出 AO 值，缺点就是此时间接光照的范围也是被严重限制的，有时想要更好的间接光照效果，需要将 Radius 设置得很大，但这样做又会导致 AO 效果看起来不舒适（一般 HBAO/GTAO 的范围不会设置得很大）。那既然无法同时维系间接光照和 AO 的效果，那为什么不将屏幕分辨率的最大值作为 Radius 从而充分利用屏幕空间信息，这样间接光照的效果会更加优秀，我最终选择了这个方案，只是这样做 AO 就需要额外计算了。

    int maxRayTexel = max(_TextureSize.z, _TextureSize.w);
    ...
    for (int d = 0; d < GetDirectionCount(); d++)
    {
        ...
        float2 maxTexelDelta = sliceDir.xy * maxRayTexel;
        ...
    }

然而因为算法的性能消耗是非常大的，采样数量非常有限，将整个屏幕作为采样的范围，样本与样本之间的距离太大，此时可以增加一个指数参数（下面代码中的 `GetConvergeDegree()`），用于控制样本的聚集程度，代码大致如下：  

    for (int s = 0; s < GetStepCount(); s++)
    {
        float2 offset = PositivePow((randomOffset + s) / (GetStepCount() + 1.0), GetConvergeDegree()) * maxTexelDelta;
        ...
    }

## Initialize Horizon Angle
获取像素点的观察空间坐标以及法线的观察空间坐标，还是不赘述了，直接进入方向循环当中。首先我们要计算每个 Slice 的初始 Horizon Angle，讲原理时说过（在 Normal 钳制小节），代码如下：  

    for (int d = 0; d < GetDirectionCount(); d++)
    {
        ...
        float3 sliceNormal = normalize(cross(sliceDir, viewDir));
        float3 tangentVS = cross(viewDir, sliceNormal); // D(φ)
        float2 projectedNormal2D = FetchProjectedNormal2D(normalVS, tangentVS, viewDir); // n'
        float2 maxCosH = InitializeHorizonAngle(projectedNormal2D);
        ...
    }

上述代码先计算了 2D Slice 平面的切线 $\,\widehat D(\varphi)\,$ 在观察空间下的坐标。拿到 $\,\widehat D(\varphi)\,$ 就可以跟视角方向 $\,\widehat \omega_o\,$ 和法线方向 $\,\widehat n\,$ 一起计算出初始 Horizon Angle 值了，计算方式见原理部分。然而因为，积分求解需要用到的投影法线 $\,n'\,$ 在 2D Slice 平面的坐标，其在计算上跟 Initialize Horizon Angle 是部分重复的，所以在代码中可以先计算出来：  

    inline float2 FetchProjectedNormal2D(float3 normalVS, float3 tangentVS, float3 viewDir)
    {
        return float2(dot(normalVS, tangentVS), dot(normalVS, viewDir));
    }

    inline float2 InitializeHorizonAngle(float2 projectedNormal2D)
    {
        float t = -projectedNormal2D.x / projectedNormal2D.y;
        float cosFront = t * rsqrt(1.0 + t * t);
        return float2(-cosFront, cosFront);
    }

## Slice Integral
### 积分运算
然后就是每个 Slice 的积分了，跟 GTAO 一样需要注意 $\,\theta\,$ 的符号，向右步进的 $\,\theta_{front}\,$ 是正的，向左步进的 $\,\theta_{back}\,$ 是负的，否则计算出来会出错，上面讲原理时的公式是 $\,\theta_{front}\,$ 的情况，$\,\theta_{back}\,$ 没有展示出来，可以看下面代码：  

    // cosTheta 更大的是 θ0，即 cosTheta.x
    float3 ComputeNearFieldIrradiance_Front(float2 projectedNormal2D, float2 cosTheta, float3 radiance)
    {
        float2 theta = GTAOFastAcos(cosTheta);
        float2 sinTheta2 = 1 - cosTheta * cosTheta;
        float2 sinCosTheta = sqrt(sinTheta2) * cosTheta;
        
        float part0 = projectedNormal2D.x * 0.5 * (theta.y - theta.x + sinCosTheta.x - sinCosTheta.y);
        float part1 = projectedNormal2D.y * 0.5 * (sinTheta2.y - sinTheta2.x);
        
        return radiance * (part0 + part1);
    }

    float3 ComputeNearFieldIrradiance_Back(float2 projectedNormal2D, float2 cosTheta, float3 radiance)
    {
        float2 theta = -GTAOFastAcos(cosTheta);
        float2 sinTheta2 = 1 - cosTheta * cosTheta;
        float2 sinCosTheta = -sqrt(sinTheta2) * cosTheta;
        
        float part0 = projectedNormal2D.x * 0.5 * (theta.y - theta.x + sinCosTheta.x - sinCosTheta.y);
        float part1 = projectedNormal2D.y * 0.5 * (sinTheta2.y - sinTheta2.x);
        
        return radiance * (part0 + part1);
    }

`ComputeNearFieldIrradiance_Back` 传递进去的 cosTheta 的顺序跟 `ComputeNearFieldIrradiance_Front` 是一致的，因为当 $\,\theta\,$ 为负数时，积分中的 $\,sin\theta\,$ 要变为 $\,-sin\theta\,$，刚好负负得正，顺序就一样了，这点 GTAO 文章中也提到过。  

### 验证采样 Radiance
因为要实现无限弹射的间接光照效果，我们采样的是上一帧的屏幕颜色，但是相机或物体是会移动的，所以需要通过 Motion Vector 来重投影上一帧的屏幕颜色，得到在这一帧可以使用的颜色信息。于此同时，也要参考 TAA 对历史信息进行验证，重投影后的颜色信息仍然受到遮挡的影响，部分内容是无法使用的。因为计算 HBIL 的时候还没有当前帧的颜色信息，只能通过当前帧的几何信息和历史几何信息进行验证，最好的方式就是 TAA History 贴图的 alpha 通道存储上一帧的深度信息，这样就可以在不额外存储历史深度贴图的情况下解决这个问题。验证历史颜色信息这一步骤，最好在 HBIL 之前额外使用一个 pass 完成，这样可以减少采样次数，否则就需要在 HBIL 的循环中额外多次采样 Motion Vector 贴图，对 HBIL 的性能问题雪上加霜。这一步骤我就不展示代码了，但较为重要，最好花时间处理。处理好后，每个 Slice 的循环中的采样就比较简单了，如下：  

    float3 SliceIntegration_Front(uint2 pixelCoord, float2 maxTexelDelta, float randomOffset, float3 positionVS, float3 viewDir, float2 projectedNormal2D, inout float2 maxCosH)
    {
        float3 irradiance = 0;
        for (int s = 0; s < GetStepCount(); s++)
        {
            float2 offset = PositivePow((randomOffset + s) / GetStepCount(), GetConvergeDegree()) * maxTexelDelta;
            int2 sampleCoord = pixelCoord + offset;
            float2 sampleUV = (float2(sampleCoord) + 0.5) * _TextureSize.xy;
            if (any(abs(sampleUV - 0.5) > 0.5)) break;
            
            // ------------------------- Calculate CosH -------------------------
            
            float sampleDepth = LoadDepth(sampleCoord);
            float4 sampleNDC = GetNDCFromUVAndDepth(sampleUV, sampleDepth);
            float3 sampleVS = TransformNDCToView(sampleNDC, UNITY_MATRIX_I_P);
            sampleVS.z = -sampleVS.z;
            float3 H = normalize(sampleVS - positionVS);
            float cosH = dot(H, viewDir);
            
            // ------------------------- Irradiance Integral -------------------------
            
            if (cosH > maxCosH.y)
            {
                // 理论上采样上一帧 Scene Color 需要 Motion Vector 重投影。为了节省采样，在本 pass 前提前将 Scene Color 进行了重投影，故这里无需考虑
                float3 nearRadiance = LoadRadiance(sampleCoord);
                float2 cosTheta = float2(cosH, maxCosH.y);
                irradiance += ComputeNearFieldIrradiance_Front(projectedNormal2D, cosTheta, nearRadiance);
                maxCosH.y = cosH;
            }
        }
        return irradiance;
    }

## AO & Bent Normal
AO 和 Bent Normal 的计算公式几乎是一样的，可以放在一起计算：  

    float ComputeAOAndBentNormal(float2 projectedNormal2D, float2 maxCosH, out float2 bentNormal)
    {
        float2 theta = GTAOFastAcos(maxCosH);
        float2 sinTheta2 = 1 - maxCosH * maxCosH;
        float2 sinCosTheta = sqrt(sinTheta2) * maxCosH;
        
        float part0 = 0.5 * (theta.y - theta.x + sinCosTheta.x - sinCosTheta.y);
        float part1 = 0.5 * (sinTheta2.y + sinTheta2.x);
        bentNormal = float2(part0, part1);
        
        return projectedNormal2D.x * part0 + projectedNormal2D.y * part1;
    }

注意一下代码里的符号，理论上传递进去的 maxCosH.x 应该是向左步进的 $\,\theta_{back}\,$，它应该是负值，所以算出来的 theta 和 sinCosTheta 的 x 分量应该都加个负号，但我在代码中直接将 part0 的计算的符号改了，本质是一样的。同时，上面代码计算出来的 bentNormal 是在 2D slice 平面内的坐标，需要计算至观察空间再转换至世界坐标使用：  

    for (int d = 0; d < GetDirectionCount(); d++)
    {
        ...
        float2 bentNormal2D;
        ao += ComputeAOAndBentNormal(projectedNormal2D, maxCosH, bentNormal2D);
        bentNormalVS += bentNormal2D.x * tangentVS + bentNormal2D.y * viewDir;
    }

    bentNormalVS = normalize(bentNormalVS);
    bentNormalVS.z = -bentNormalVS.z;
    float3 bentNormal = TransformViewToWorldDir(bentNormalVS, true);
    ao = pow(saturate(ao / GetDirectionCount()), GetFarFieldAOIntensity());

计算出 bent normal 是为了采样 Ambient Probe 或 APV 时使用，以获取更好的远距离光照效果。实际上计算出来的 bent normal 不太能单独输出使用，原因跟 AO 一样，因为我们 Radius 的范围实在太大了，若想要输出使用，还是在 GTAO 环节输出比较好。但作为 HBIL 的 far-field 间接光照的采样依据是可以的，我目前主要采样 APV 或 Ambient Probe，其实 Fallback Reflection Probe 也是可以的，只是我没实现。

## 最终效果
HBIL 的噪点还是很大的，降噪部分目前还是使用了 Temporal Filter + Bilateral Filter 组合，只能说大部分情况下是够用的，但是在光源较弱时，噪点还是会比较明显的。我不知道 ReSTIR 的思想能不能使用在 HBAO/GTAO/HBIL 当中，想要进一步降噪，我日后还是得好好学习 ReSTIR 的。

优化这部分还是老样子使用 Half Resolution，最好将 Scene Color、Depth、Normal 贴图都下采样，否则性能消耗是比较大的。我简单测试了一下，在 Cornell Box 这么简单的场景下，在 4k 不使用 Half Resolution 的情况下，4080 显卡，Direction Count 是 3， Step Count 是 6，总采样数为 3 × 6 × 2 = 36 次（实际上应该没这么多次采样，因为超出屏幕的样本会跳过，我们 radius 是屏幕最长的边，很容易超出屏幕，所以我感觉综合采样数应该是 18 次左右，可能更少），每帧需要 6.8 ms，使用 Half Resolution 直接回到 2.8 ms。Cryteck’s Sponza 场景下，还是 4k，不使用 HBIL 每帧 3.4 ms，使用 HBIL + Half Resolution 每帧 4.8 ms 左右，不使用 Half Resolution 直接飙到 11.2 ms。

<div align="center">  
<img src="https://s2.loli.net/2026/01/20/Ry61o4hJC2GbKWY.png" width = "100%" height = "100%" alt="图7 - Direction Count 是 3， Step Count 是 6，使用了 Temporal Filter + Bilateral Filter，无额外 AO，左：4k，Full Resolution，右：4k，Half Resolution。"/>
</div>

当发光物体较小时，需要的采样数量也要额外增加，否则计算出来的效果不佳，噪点也会很重，如下图所示，当 Direction Count 是 3， Step Count 是 6 时，采样数量明显不足以计算出正常的效果（我 Temporal Filter 使用了 Luminance Weight，这部分对亮度影响是很大的，若不使用 Luminance Weight，左图会相对正常一点，但取消 Luminance Weight 会加重一点噪声闪烁现象，怎么取舍看项目需求）：  

<div align="center">  
<img src="https://s2.loli.net/2026/01/20/YkI5o1MbnfG74qD.png" width = "80%" height = "80%" alt="图8 - 4k，Full Resolution，仅 Temporal Filter，左：Direction Count 是 3， Step Count 是 6，右：Direction Count 是 6， Step Count 是 12。"/>
</div>

# 整体代码

``` C
inline float GTAOFastAcos(float x)
{
    float res = -0.156583 * abs(x) + HALF_PI;
    res *= sqrt(1.0 - abs(x));
    return x >= 0 ? res : PI - res;
}

inline float2 GTAOFastAcos(float2 x)
{
    float2 res = -0.156583 * abs(x) + HALF_PI;
    res *= sqrt(1.0 - abs(x));
    return x >= 0 ? res : PI - res;
}

inline float2 FetchProjectedNormal2D(float3 normalVS, float3 tangentVS, float3 viewDir)
{
    return float2(dot(normalVS, tangentVS), dot(normalVS, viewDir));
}

inline float2 InitializeHorizonAngle(float2 projectedNormal2D)
{
    float t = -projectedNormal2D.x / projectedNormal2D.y;
    float cosFront = t * rsqrt(1.0 + t * t);
    return float2(-cosFront, cosFront);
}

// cosTheta 更大的是 θ0，即 cosTheta.x
float3 ComputeNearFieldIrradiance_Front(float2 projectedNormal2D, float2 cosTheta, float3 radiance)
{
    float2 theta = GTAOFastAcos(cosTheta);
    float2 sinTheta2 = 1 - cosTheta * cosTheta;
    float2 sinCosTheta = sqrt(sinTheta2) * cosTheta;
    
    float part0 = projectedNormal2D.x * 0.5 * (theta.y - theta.x + sinCosTheta.x - sinCosTheta.y);
    float part1 = projectedNormal2D.y * 0.5 * (sinTheta2.y - sinTheta2.x);
    
    return radiance * (part0 + part1);
}

float3 ComputeNearFieldIrradiance_Back(float2 projectedNormal2D, float2 cosTheta, float3 radiance)
{
    float2 theta = -GTAOFastAcos(cosTheta);
    float2 sinTheta2 = 1 - cosTheta * cosTheta;
    float2 sinCosTheta = -sqrt(sinTheta2) * cosTheta;
    
    float part0 = projectedNormal2D.x * 0.5 * (theta.y - theta.x + sinCosTheta.x - sinCosTheta.y);
    float part1 = projectedNormal2D.y * 0.5 * (sinTheta2.y - sinTheta2.x);
    
    return radiance * (part0 + part1);
}

float3 SliceIntegration_Front(uint2 pixelCoord, float2 maxTexelDelta, float randomOffset, float3 positionVS, float3 viewDir, float2 projectedNormal2D, inout float2 maxCosH)
{
    float3 irradiance = 0;
    for (int s = 0; s < GetStepCount(); s++)
    {
        float2 offset = PositivePow((randomOffset + s) / GetStepCount(), GetConvergeDegree()) * maxTexelDelta;
        int2 sampleCoord = pixelCoord + offset;
        float2 sampleUV = (float2(sampleCoord) + 0.5) * _TextureSize.xy;
        if (any(abs(sampleUV - 0.5) > 0.5)) break;
        
        // ------------------------- Calculate CosH -------------------------
        
        float sampleDepth = LoadDepth(sampleCoord);
        float4 sampleNDC = GetNDCFromUVAndDepth(sampleUV, sampleDepth);
        float3 sampleVS = TransformNDCToView(sampleNDC, UNITY_MATRIX_I_P);
        sampleVS.z = -sampleVS.z;
        float3 H = normalize(sampleVS - positionVS);
        float cosH = dot(H, viewDir);
        
        // ------------------------- Irradiance Integral -------------------------
        
        if (cosH > maxCosH.y)
        {
            // 理论上采样上一帧 Scene Color 需要 Motion Vector 重投影。为了节省采样，在本 pass 前提前将 Scene Color 进行了重投影，故这里无需考虑
            float3 nearRadiance = LoadRadiance(sampleCoord);
            float2 cosTheta = float2(cosH, maxCosH.y);
            irradiance += ComputeNearFieldIrradiance_Front(projectedNormal2D, cosTheta, nearRadiance);
            maxCosH.y = cosH;
        }
    }
    return irradiance;
}

float3 SliceIntegration_Back(uint2 pixelCoord, float2 maxTexelDelta, float randomOffset, float3 positionVS, float3 viewDir, float2 projectedNormal2D, inout float2 maxCosH)
{
    float3 irradiance = 0;
    for (int s = 0; s < GetStepCount(); s++)
    {
        float2 offset = PositivePow((randomOffset + s) / GetStepCount(), GetConvergeDegree()) * maxTexelDelta;
        int2 sampleCoord = pixelCoord - offset;
        float2 sampleUV = (float2(sampleCoord) + 0.5) * _TextureSize.xy;
        if (any(abs(sampleUV - 0.5) > 0.5)) break;
        
        // ------------------------- Calculate CosH -------------------------
        
        float sampleDepth = LoadDepth(sampleCoord);
        float4 sampleNDC = GetNDCFromUVAndDepth(sampleUV, sampleDepth);
        float3 sampleVS = TransformNDCToView(sampleNDC, UNITY_MATRIX_I_P);
        sampleVS.z = -sampleVS.z;
        float3 H = normalize(sampleVS - positionVS);
        float cosH = dot(H, viewDir);
        
        // ------------------------- Irradiance Integral -------------------------
        
        if (cosH > maxCosH.x)
        {
            // 理论上采样上一帧 Scene Color 需要 Motion Vector 重投影。为了节省采样，在本 pass 前提前将 Scene Color 进行了重投影，故这里无需考虑
            float3 nearRadiance = LoadRadiance(sampleCoord);
            float2 cosTheta = float2(cosH, maxCosH.x);
            irradiance += ComputeNearFieldIrradiance_Back(projectedNormal2D, cosTheta, nearRadiance);
            maxCosH.x = cosH;
        }
    }
    return irradiance;
}

float ComputeAOAndBentNormal(float2 projectedNormal2D, float2 maxCosH, out float2 bentNormal)
{
    float2 theta = GTAOFastAcos(maxCosH);
    float2 sinTheta2 = 1 - maxCosH * maxCosH;
    float2 sinCosTheta = sqrt(sinTheta2) * maxCosH;
    
    float part0 = 0.5 * (theta.y - theta.x + sinCosTheta.x - sinCosTheta.y);
    float part1 = 0.5 * (sinTheta2.y + sinTheta2.x);
    bentNormal = float2(part0, part1);
    
    return projectedNormal2D.x * part0 + projectedNormal2D.y * part1;
}

[numthreads(8, 8, 1)]
void HBILKernel(uint3 id : SV_DispatchThreadID)
{
    int2 pixelCoord = clamp(id.xy, 0, _TextureSize.zw - 1);
    float2 screenUV = (float2(pixelCoord) + float2(0.5, 0.5)) * _TextureSize.xy;
    float rawDepth = LoadDepth(pixelCoord);
    
    if (rawDepth < 1e-7)
    {
        float linearDepth = GetViewDepthFromDepthTexture(rawDepth);
        _OutputTexture[id.xy] = float4(0.0, 0.0, 0.0, linearDepth);
        return;
    }
    
    // ------------------------- Fetch Position & Normal -------------------------
    
    float4 positionNDC = GetNDCFromUVAndDepth(screenUV, rawDepth);
    float3 positionWS = TransformNDCToWorld(positionNDC, UNITY_MATRIX_I_VP);
    float3 positionVS = TransformNDCToView(positionNDC, UNITY_MATRIX_I_P);
    positionVS.z = -positionVS.z;
    float3 viewDir = normalize(-positionVS);
    // float3 viewDirWS = GetWorldSpaceNormalizedViewDir(positionWS);

    float3 normalWS = LoadAndDecodeNormal(pixelCoord);
    float3 normalVS = FetchNormalVS(normalWS);
    
    // ------------------------- Noise -------------------------
    
    int2 noiseCoord = (pixelCoord + (_Jitter.zw + 0.5) * _STBN128Scalar3_TexelSize.zw * IsTemporalDenoiseEnabled()) % _STBN128Scalar3_TexelSize.zw;
    float3 noise = LOAD_TEXTURE2D_LOD(_STBN128Scalar3, noiseCoord, 0).rgb;
    float randomRadian = noise.r * TWO_PI;
    float randomOffset = noise.g;
    
    // ------------------------- Loop -------------------------
    
    int maxRayTexel = max(_TextureSize.z, _TextureSize.w);
    float dirAngle = PI / GetDirectionCount();
    float3 irradiance = 0;
    float ao = 0;
    float3 bentNormalVS = 0;
    
    for (int d = 0; d < GetDirectionCount(); d++)
    {
        // ------------------------- Randomize Direction -------------------------
        
        float angle = dirAngle * d + randomRadian;
        float3 sliceDir = float3(cos(angle), sin(angle), 0);
        float2 maxTexelDelta = sliceDir.xy * maxRayTexel;
        
        // ------------------------- Normal Projection -------------------------
        
        float3 sliceNormal = normalize(cross(sliceDir, viewDir));
        float3 tangentVS = cross(viewDir, sliceNormal); // D(φ)
        // float3 realTangentVS = cross(normalVS, sliceNormal);
        float2 projectedNormal2D = FetchProjectedNormal2D(normalVS, tangentVS, viewDir); // n'
        float2 maxCosH = InitializeHorizonAngle(projectedNormal2D);
        
        // ------------------------- Marching -------------------------
        
        irradiance += SliceIntegration_Back(id.xy, maxTexelDelta, randomOffset, positionVS, viewDir, projectedNormal2D, maxCosH);
        irradiance += SliceIntegration_Front(id.xy, maxTexelDelta, randomOffset, positionVS, viewDir, projectedNormal2D, maxCosH);
        
        // ------------------------- Bent Normal & AO -------------------------
        
        float2 bentNormal2D;
        ao += ComputeAOAndBentNormal(projectedNormal2D, maxCosH, bentNormal2D);
        bentNormalVS += bentNormal2D.x * tangentVS + bentNormal2D.y * viewDir;
    }
    
    bentNormalVS = normalize(bentNormalVS);
    bentNormalVS.z = -bentNormalVS.z;
    float3 bentNormal = TransformViewToWorldDir(bentNormalVS, true);
    
    ao = pow(saturate(ao / GetDirectionCount()), GetFarFieldAOIntensity());
    irradiance = irradiance / GetDirectionCount() * GetIntensity();
    
    UNITY_BRANCH
    if (GetFallbackMode() < 0.5)
    {
        irradiance += FallbackAPV(positionWS, normalWS, bentNormal, noise) * ao * GetFallbackIntensity();
    }
    else
    {
        irradiance += FallbackAmbientProbe(bentNormal) * ao * GetFallbackIntensity();
    }
    
    // 物体边缘会出现 NaN，但不知道原因，增加 randomOffset 的值，使其离 0 越远，NaN 越少，但不能完全消除，而且会严重影响效果
    irradiance = AnyIsNaN(irradiance) ? 0.0 : irradiance;
    _OutputTexture[id.xy] = float4(max(irradiance, 0), positionVS.z);
}
```